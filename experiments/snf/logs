/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch 000 | [    2/60000 ( 0%)] | Loss:  292.964264 | rec: 292.953796 | kl:    1.047701
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch 000 | [    2/60000 ( 0%)] | Loss:  292.096771 | rec: 292.088226 | kl:    0.853375
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch 000 | [    2/60000 ( 0%)] | Loss:  282.733398 | rec: 282.722565 | kl:    1.082556
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch 000 | [    2/60000 ( 0%)] | Loss:  251.636734 | rec: 251.626831 | kl:    0.989536
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch 000 | [    2/60000 ( 0%)] | Loss:  283.951660 | rec: 283.941956 | kl:    0.970002
Epoch 000 | [   22/60000 ( 2%)] | Loss:  250.233994 | rec: 250.223785 | kl:    1.020995
Epoch 000 | [   42/60000 ( 3%)] | Loss:  233.662949 | rec: 233.650070 | kl:    1.286669
Epoch 000 | [   62/60000 ( 5%)] | Loss:  181.893723 | rec: 181.763397 | kl:   13.033112
Epoch 000 | [   82/60000 ( 7%)] | Loss:  171.409637 | rec: 171.372528 | kl:    3.710455
Epoch 000 | [  102/60000 ( 8%)] | Loss:  170.494446 | rec: 170.484375 | kl:    1.007176
Epoch 000 | [  122/60000 (10%)] | Loss:  163.853165 | rec: 163.844040 | kl:    0.911772
Epoch 000 | [  142/60000 (12%)] | Loss:  159.874893 | rec: 159.858704 | kl:    1.619034
Epoch 000 | [  162/60000 (13%)] | Loss:  143.550186 | rec: 143.416626 | kl:   13.356720
Epoch 000 | [  182/60000 (15%)] | Loss:  169.900940 | rec: 169.870453 | kl:    3.048874
Epoch 000 | [  202/60000 (17%)] | Loss:  146.774811 | rec: 146.739883 | kl:    3.493037
Epoch 000 | [  222/60000 (18%)] | Loss:  130.109329 | rec: 127.788223 | kl:  232.109955
Epoch 000 | [  242/60000 (20%)] | Loss:  114.213005 | rec: 112.584656 | kl:  162.835114
Epoch 000 | [  262/60000 (22%)] | Loss:  107.427521 | rec: 107.119102 | kl:   30.842070
Epoch 000 | [  282/60000 (23%)] | Loss:  101.909966 | rec: 101.579948 | kl:   33.002266
Epoch 000 | [  302/60000 (25%)] | Loss:  104.034821 | rec: 103.596367 | kl:   43.846127
Epoch 000 | [  322/60000 (27%)] | Loss:   99.922646 | rec:  99.607193 | kl:   31.544863
Epoch 000 | [  342/60000 (28%)] | Loss:  103.135582 | rec: 102.754234 | kl:   38.134575
Epoch 000 | [  362/60000 (30%)] | Loss:  104.659927 | rec: 104.287468 | kl:   37.245922
Epoch 000 | [  382/60000 (32%)] | Loss:  102.269676 | rec: 101.935661 | kl:   33.401077
Epoch 000 | [  402/60000 (33%)] | Loss:   99.361084 | rec:  98.970131 | kl:   39.094528
Epoch 000 | [  422/60000 (35%)] | Loss:  100.317833 | rec:  99.952202 | kl:   36.562866
Epoch 000 | [  442/60000 (37%)] | Loss:   99.627411 | rec:  99.331192 | kl:   29.621656
Epoch 000 | [  462/60000 (38%)] | Loss:   96.931671 | rec:  96.632309 | kl:   29.935997
Epoch 000 | [  482/60000 (40%)] | Loss:  101.202751 | rec: 100.841003 | kl:   36.174812
Epoch 000 | [  502/60000 (42%)] | Loss:  102.526024 | rec: 102.112251 | kl:   41.376610
Epoch 000 | [  522/60000 (43%)] | Loss:   99.262642 | rec:  98.870895 | kl:   39.174400
Epoch 000 | [  542/60000 (45%)] | Loss:   91.505447 | rec:  91.079811 | kl:   42.563160
Epoch 000 | [  562/60000 (47%)] | Loss:   93.086128 | rec:  92.691109 | kl:   39.502190
Epoch 000 | [  582/60000 (48%)] | Loss:   92.027290 | rec:  91.505440 | kl:   52.185116
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3803(0.3803) | [    2/60000 ( 0%)] | Loss:  264.312958 |rec: 264.302063 | kl:    1.089183
Epoch 000 | Time 0.0565(0.2954) | [   22/60000 ( 2%)] | Loss:  196.974533 |rec: 196.962143 | kl:    1.239203
Epoch 000 | Time 0.0566(0.2327) | [   42/60000 ( 3%)] | Loss:  163.343124 |rec: 163.323212 | kl:    1.990565
Epoch 000 | Time 0.0582(0.1866) | [   62/60000 ( 5%)] | Loss:  156.177460 |rec: 156.027100 | kl:   15.036138
Epoch 000 | Time 0.0565(0.1525) | [   82/60000 ( 7%)] | Loss:  126.074333 |rec: 125.513580 | kl:   56.074833
Epoch 000 | Time 0.0565(0.1273) | [  102/60000 ( 8%)] | Loss:  116.097168 |rec: 115.369682 | kl:   72.748337
Epoch 000 | Time 0.0581(0.1088) | [  122/60000 (10%)] | Loss:  110.190018 |rec: 109.774956 | kl:   41.506340
Epoch 000 | Time 0.0565(0.0952) | [  142/60000 (12%)] | Loss:  106.004829 |rec: 105.402817 | kl:   60.201584
Epoch 000 | Time 0.0566(0.0851) | [  162/60000 (13%)] | Loss:  104.292686 |rec: 103.760788 | kl:   53.189472
Epoch 000 | Time 0.0579(0.0777) | [  182/60000 (15%)] | Loss:  106.967979 |rec: 106.500359 | kl:   46.761703
Epoch 000 | Time 0.0565(0.0722) | [  202/60000 (17%)] | Loss:   99.052567 |rec:  98.661171 | kl:   39.140045
Epoch 000 | Time 0.0566(0.0681) | [  222/60000 (18%)] | Loss:   98.698135 |rec:  98.360756 | kl:   33.737671
Epoch 000 | Time 0.0581(0.0652) | [  242/60000 (20%)] | Loss:   97.153084 |rec:  96.771065 | kl:   38.202530
Epoch 000 | Time 0.0565(0.0630) | [  262/60000 (22%)] | Loss:   96.004082 |rec:  95.521111 | kl:   48.297153
Epoch 000 | Time 0.0565(0.0614) | [  282/60000 (23%)] | Loss:   98.444344 |rec:  97.989861 | kl:   45.448723
Epoch 000 | Time 0.0583(0.0602) | [  302/60000 (25%)] | Loss:   98.240509 |rec:  97.744713 | kl:   49.579021
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3286(0.3286) | [    2/60000 ( 0%)] | Loss:  276.607178 |rec: 276.597961 | kl:    0.921042
Epoch 000 | Time 0.0566(0.2575) | [   22/60000 ( 2%)] | Loss:  223.355438 |rec: 223.336868 | kl:    1.857882
Epoch 000 | Time 0.0566(0.2048) | [   42/60000 ( 3%)] | Loss:  176.351318 |rec: 175.968704 | kl:   38.262352
Epoch 000 | Time 0.0567(0.1660) | [   62/60000 ( 5%)] | Loss:  165.540344 |rec: 165.524689 | kl:    1.566504
Epoch 000 | Time 0.0567(0.1374) | [   82/60000 ( 7%)] | Loss:  168.951630 |rec: 168.941956 | kl:    0.969518
Epoch 000 | Time 0.0566(0.1163) | [  102/60000 ( 8%)] | Loss:  165.458649 |rec: 164.633041 | kl:   82.561394
Epoch 000 | Time 0.0565(0.1007) | [  122/60000 (10%)] | Loss:  126.678017 |rec: 126.583473 | kl:    9.454576
Epoch 000 | Time 0.0585(0.0892) | [  142/60000 (12%)] | Loss:  116.676376 |rec: 116.479027 | kl:   19.734186
Epoch 000 | Time 0.0566(0.0807) | [  162/60000 (13%)] | Loss:  106.319389 |rec: 106.082321 | kl:   23.707048
Epoch 000 | Time 0.0566(0.0745) | [  182/60000 (15%)] | Loss:   99.091530 |rec:  98.816933 | kl:   27.460287
Epoch 000 | Time 0.0572(0.0699) | [  202/60000 (17%)] | Loss:  103.585167 |rec: 103.374138 | kl:   21.102673
Epoch 000 | Time 0.0567(0.0665) | [  222/60000 (18%)] | Loss:  100.864189 |rec: 100.658531 | kl:   20.565332
Epoch 000 | Time 0.0570(0.0640) | [  242/60000 (20%)] | Loss:   95.000099 |rec:  94.740173 | kl:   25.992001
Epoch 000 | Time 0.0569(0.0622) | [  262/60000 (22%)] | Loss:   99.343788 |rec:  99.080719 | kl:   26.306925
Epoch 000 | Time 0.0570(0.0610) | [  282/60000 (23%)] | Loss:   96.247185 |rec:  95.913345 | kl:   33.383961
Epoch 000 | Time 0.0570(0.0600) | [  302/60000 (25%)] | Loss:   95.426056 |rec:  95.051346 | kl:   37.470936
Epoch 000 | Time 0.0570(0.0593) | [  322/60000 (27%)] | Loss:   88.494736 |rec:  88.034363 | kl:   46.037239
Epoch 000 | Time 0.0569(0.0588) | [  342/60000 (28%)] | Loss:   87.707634 |rec:  87.099960 | kl:   60.767998
Epoch 000 | Time 0.0570(0.0584) | [  362/60000 (30%)] | Loss:   85.795929 |rec:  85.231651 | kl:   56.427498
Epoch 000 | Time 0.0569(0.0581) | [  382/60000 (32%)] | Loss:   83.314781 |rec:  82.440857 | kl:   87.392838
Epoch 000 | Time 0.0570(0.0578) | [  402/60000 (33%)] | Loss:   79.685699 |rec:  79.040520 | kl:   64.517426
Epoch 000 | Time 0.0570(0.0576) | [  422/60000 (35%)] | Loss:   75.252724 |rec:  74.230125 | kl:  102.259865
Epoch 000 | Time 0.0569(0.0575) | [  442/60000 (37%)] | Loss:   77.067192 |rec:  76.329834 | kl:   73.736473
Epoch 000 | Time 0.0570(0.0574) | [  462/60000 (38%)] | Loss:   69.357002 |rec:  68.253487 | kl:  110.351532
Epoch 000 | Time 0.0570(0.0573) | [  482/60000 (40%)] | Loss:   66.075745 |rec:  65.193512 | kl:   88.223183
Epoch 000 | Time 0.0570(0.0572) | [  502/60000 (42%)] | Loss:   68.192505 |rec:  67.044624 | kl:  114.788231
Epoch 000 | Time 0.0569(0.0572) | [  522/60000 (43%)] | Loss:   62.658134 |rec:  61.405224 | kl:  125.290993
Epoch 000 | Time 0.0570(0.0571) | [  542/60000 (45%)] | Loss:   65.921227 |rec:  64.879082 | kl:  104.214439
Epoch 000 | Time 0.0570(0.0571) | [  562/60000 (47%)] | Loss:   64.226585 |rec:  63.223503 | kl:  100.308289
Epoch 000 | Time 0.0570(0.0571) | [  582/60000 (48%)] | Loss:   59.722229 |rec:  58.373150 | kl:  134.908157
Epoch 000 | Time 0.0570(0.0571) | [  602/60000 (50%)] | Loss:   62.033577 |rec:  60.957031 | kl:  107.654610
Epoch 000 | Time 0.0568(0.0571) | [  622/60000 (52%)] | Loss:   56.635117 |rec:  55.509628 | kl:  112.548737
Epoch 000 | Time 0.0568(0.0570) | [  642/60000 (53%)] | Loss:   57.886642 |rec:  56.632172 | kl:  125.447357
Epoch 000 | Time 0.0568(0.0570) | [  662/60000 (55%)] | Loss:   59.730122 |rec:  58.808472 | kl:   92.165085
Epoch 000 | Time 0.0568(0.0570) | [  682/60000 (57%)] | Loss:   54.002720 |rec:  52.969959 | kl:  103.275932
Epoch 000 | Time 0.0569(0.0569) | [  702/60000 (58%)] | Loss:   55.367207 |rec:  54.169968 | kl:  119.723648
Epoch 000 | Time 0.0568(0.0570) | [  722/60000 (60%)] | Loss:   52.217144 |rec:  50.947422 | kl:  126.972397
Epoch 000 | Time 0.0568(0.0570) | [  742/60000 (62%)] | Loss:   51.281662 |rec:  50.137489 | kl:  114.417458
Epoch 000 | Time 0.0567(0.0569) | [  762/60000 (63%)] | Loss:   50.883369 |rec:  49.728661 | kl:  115.470787
Epoch 000 | Time 0.0568(0.0569) | [  782/60000 (65%)] | Loss:   51.454773 |rec:  50.305508 | kl:  114.926743
Epoch 000 | Time 0.0568(0.0569) | [  802/60000 (67%)] | Loss:   49.083954 |rec:  47.883823 | kl:  120.013367
Epoch 000 | Time 0.0569(0.0569) | [  822/60000 (68%)] | Loss:   53.809242 |rec:  52.748116 | kl:  106.112885
Epoch 000 | Time 0.0570(0.0569) | [  842/60000 (70%)] | Loss:   54.237991 |rec:  53.213562 | kl:  102.443016
Epoch 000 | Time 0.0568(0.0569) | [  862/60000 (72%)] | Loss:   50.114128 |rec:  48.968269 | kl:  114.585770
Epoch 000 | Time 0.0568(0.0570) | [  882/60000 (73%)] | Loss:   50.968281 |rec:  49.822281 | kl:  114.599968
Epoch 000 | Time 0.0568(0.0570) | [  902/60000 (75%)] | Loss:   46.405293 |rec:  45.314594 | kl:  109.069809
Epoch 000 | Time 0.0568(0.0569) | [  922/60000 (77%)] | Loss:   47.585064 |rec:  46.461323 | kl:  112.373886
Epoch 000 | Time 0.0569(0.0569) | [  942/60000 (78%)] | Loss:   47.094746 |rec:  45.962128 | kl:  113.261513
Epoch 000 | Time 0.0568(0.0569) | [  962/60000 (80%)] | Loss:   47.083820 |rec:  45.970097 | kl:  111.372543
Epoch 000 | Time 0.0568(0.0569) | [  982/60000 (82%)] | Loss:   44.327251 |rec:  43.272797 | kl:  105.445290
Epoch 000 | Time 0.0571(0.0570) | [ 1002/60000 (83%)] | Loss:   45.453018 |rec:  44.431885 | kl:  102.113083
Epoch 000 | Time 0.0572(0.0571) | [ 1022/60000 (85%)] | Loss:   43.106472 |rec:  42.050018 | kl:  105.645737
Epoch 000 | Time 0.0572(0.0571) | [ 1042/60000 (87%)] | Loss:   44.620068 |rec:  43.597588 | kl:  102.247818
Epoch 000 | Time 0.0572(0.0571) | [ 1062/60000 (88%)] | Loss:   42.548210 |rec:  41.494202 | kl:  105.400810
Epoch 000 | Time 0.0571(0.0572) | [ 1082/60000 (90%)] | Loss:   42.688641 |rec:  41.600307 | kl:  108.833572
Epoch 000 | Time 0.0572(0.0572) | [ 1102/60000 (92%)] | Loss:   43.985077 |rec:  42.961399 | kl:  102.367920
Epoch 000 | Time 0.0571(0.0572) | [ 1122/60000 (93%)] | Loss:   45.618671 |rec:  44.615791 | kl:  100.288315
Epoch 000 | Time 0.0570(0.0572) | [ 1142/60000 (95%)] | Loss:   43.476376 |rec:  42.419121 | kl:  105.725670
Epoch 000 | Time 0.0571(0.0572) | [ 1162/60000 (97%)] | Loss:   45.754280 |rec:  44.743744 | kl:  101.053856
Epoch 000 | Time 0.0571(0.0572) | [ 1182/60000 (98%)] | Loss:   42.230682 |rec:  41.236240 | kl:   99.444351
Epoch: 1 	Beta: 0.02
Epoch 001 | Time 0.0581(0.0573) | [    2/60000 ( 0%)] | Loss:   42.606697 |rec:  40.561478 | kl:  102.260933
Epoch 001 | Time 0.0572(0.0573) | [   22/60000 ( 2%)] | Loss:   41.743202 |rec:  40.182430 | kl:   78.038696
Epoch 001 | Time 0.0571(0.0573) | [   42/60000 ( 3%)] | Loss:   40.702713 |rec:  39.218018 | kl:   74.234741
Epoch 001 | Time 0.0572(0.0573) | [   62/60000 ( 5%)] | Loss:   40.434994 |rec:  39.052837 | kl:   69.107742
Epoch 001 | Time 0.0572(0.0573) | [   82/60000 ( 7%)] | Loss:   42.677666 |rec:  41.185703 | kl:   74.598038
Epoch 001 | Time 0.0572(0.0573) | [  102/60000 ( 8%)] | Loss:   42.761093 |rec:  41.322651 | kl:   71.922173
Epoch 001 | Time 0.0572(0.0573) | [  122/60000 (10%)] | Loss:   42.665489 |rec:  41.230404 | kl:   71.754204
Epoch 001 | Time 0.0571(0.0573) | [  142/60000 (12%)] | Loss:   40.278538 |rec:  38.921307 | kl:   67.861443
Epoch 001 | Time 0.0572(0.0573) | [  162/60000 (13%)] | Loss:   41.563740 |rec:  40.117123 | kl:   72.330734
Epoch 001 | Time 0.0572(0.0573) | [  182/60000 (15%)] | Loss:   42.372032 |rec:  40.992256 | kl:   68.988762
Epoch 001 | Time 0.0571(0.0573) | [  202/60000 (17%)] | Loss:   39.947357 |rec:  38.542343 | kl:   70.250748
Epoch 001 | Time 0.0571(0.0573) | [  222/60000 (18%)] | Loss:   41.012321 |rec:  39.626694 | kl:   69.281418
Epoch 001 | Time 0.0572(0.0573) | [  242/60000 (20%)] | Loss:   40.619671 |rec:  39.356995 | kl:   63.133701
Epoch 001 | Time 0.0572(0.0573) | [  262/60000 (22%)] | Loss:   41.785770 |rec:  40.338806 | kl:   72.348183
Epoch 001 | Time 0.0571(0.0573) | [  282/60000 (23%)] | Loss:   41.140820 |rec:  39.841789 | kl:   64.951393
Epoch 001 | Time 0.0571(0.0573) | [  302/60000 (25%)] | Loss:   40.488274 |rec:  39.161583 | kl:   66.334656
Epoch 001 | Time 0.0571(0.0573) | [  322/60000 (27%)] | Loss:   41.720268 |rec:  40.449764 | kl:   63.525101
Epoch 001 | Time 0.0571(0.0573) | [  342/60000 (28%)] | Loss:   40.083260 |rec:  38.802883 | kl:   64.018745
Epoch 001 | Time 0.0572(0.0573) | [  362/60000 (30%)] | Loss:   38.501781 |rec:  37.164684 | kl:   66.854866
Epoch 001 | Time 0.0572(0.0573) | [  382/60000 (32%)] | Loss:   40.417824 |rec:  39.116669 | kl:   65.057724
Epoch 001 | Time 0.0572(0.0573) | [  402/60000 (33%)] | Loss:   39.730206 |rec:  38.469410 | kl:   63.039852
Epoch 001 | Time 0.0571(0.0573) | [  422/60000 (35%)] | Loss:   41.005657 |rec:  39.775707 | kl:   61.497509
Epoch 001 | Time 0.0572(0.0573) | [  442/60000 (37%)] | Loss:   39.493477 |rec:  38.249149 | kl:   62.216476
Epoch 001 | Time 0.0572(0.0573) | [  462/60000 (38%)] | Loss:   37.841846 |rec:  36.627438 | kl:   60.720604
Epoch 001 | Time 0.0572(0.0573) | [  482/60000 (40%)] | Loss:   40.475773 |rec:  39.256115 | kl:   60.982857
Epoch 001 | Time 0.0572(0.0573) | [  502/60000 (42%)] | Loss:   38.642769 |rec:  37.424644 | kl:   60.906273
Epoch 001 | Time 0.0571(0.0573) | [  522/60000 (43%)] | Loss:   41.489834 |rec:  40.263504 | kl:   61.316357
Epoch 001 | Time 0.0572(0.0573) | [  542/60000 (45%)] | Loss:   39.146912 |rec:  37.946526 | kl:   60.019154
Epoch 001 | Time 0.0572(0.0573) | [  562/60000 (47%)] | Loss:   40.197403 |rec:  38.981079 | kl:   60.816113
Epoch 001 | Time 0.0572(0.0573) | [  582/60000 (48%)] | Loss:   40.765415 |rec:  39.581810 | kl:   59.180428
Epoch 001 | Time 0.0572(0.0573) | [  602/60000 (50%)] | Loss:   40.701244 |rec:  39.455822 | kl:   62.271084
Epoch 001 | Time 0.0572(0.0573) | [  622/60000 (52%)] | Loss:   38.153301 |rec:  37.012630 | kl:   57.033672
Epoch 001 | Time 0.0572(0.0573) | [  642/60000 (53%)] | Loss:   39.798882 |rec:  38.562481 | kl:   61.820210
Epoch 001 | Time 0.0572(0.0573) | [  662/60000 (55%)] | Loss:   37.898853 |rec:  36.730747 | kl:   58.405224
Epoch 001 | Time 0.0572(0.0573) | [  682/60000 (57%)] | Loss:   37.842319 |rec:  36.626202 | kl:   60.805859
Epoch 001 | Time 0.0572(0.0573) | [  702/60000 (58%)] | Loss:   39.771820 |rec:  38.600052 | kl:   58.588425
Epoch 001 | Time 0.0572(0.0573) | [  722/60000 (60%)] | Loss:   39.786823 |rec:  38.601555 | kl:   59.263325
Epoch 001 | Time 0.0572(0.0573) | [  742/60000 (62%)] | Loss:   39.256268 |rec:  38.038216 | kl:   60.902630
Epoch 001 | Time 0.0572(0.0573) | [  762/60000 (63%)] | Loss:   39.193348 |rec:  37.983238 | kl:   60.505547
Epoch 001 | Time 0.0572(0.0573) | [  782/60000 (65%)] | Loss:   38.377644 |rec:  37.240467 | kl:   56.858944
Epoch 001 | Time 0.0573(0.0573) | [  802/60000 (67%)] | Loss:   38.140518 |rec:  36.941544 | kl:   59.948895
Epoch 001 | Time 0.0572(0.0573) | [  822/60000 (68%)] | Loss:   37.261944 |rec:  36.031834 | kl:   61.505486
Epoch 001 | Time 0.0572(0.0573) | [  842/60000 (70%)] | Loss:   38.678959 |rec:  37.481785 | kl:   59.858582
Epoch 001 | Time 0.0574(0.0573) | [  862/60000 (72%)] | Loss:   36.722157 |rec:  35.599121 | kl:   56.151844
Epoch 001 | Time 0.0575(0.0574) | [  882/60000 (73%)] | Loss:   35.120117 |rec:  33.981937 | kl:   56.908916
Epoch 001 | Time 0.0575(0.0575) | [  902/60000 (75%)] | Loss:   36.199295 |rec:  35.047424 | kl:   57.593548
Epoch 001 | Time 0.0574(0.0575) | [  922/60000 (77%)] | Loss:   37.700081 |rec:  36.527809 | kl:   58.613472
Epoch 001 | Time 0.0575(0.0575) | [  942/60000 (78%)] | Loss:   38.274162 |rec:  37.100395 | kl:   58.688416
Epoch 001 | Time 0.0575(0.0576) | [  962/60000 (80%)] | Loss:   37.703323 |rec:  36.518124 | kl:   59.259922
Epoch 001 | Time 0.0574(0.0576) | [  982/60000 (82%)] | Loss:   36.740593 |rec:  35.592861 | kl:   57.386551
Epoch 001 | Time 0.0575(0.0576) | [ 1002/60000 (83%)] | Loss:   37.216320 |rec:  36.088783 | kl:   56.376812
Epoch 001 | Time 0.0574(0.0576) | [ 1022/60000 (85%)] | Loss:   39.341663 |rec:  38.167740 | kl:   58.696083
Epoch 001 | Time 0.0575(0.0576) | [ 1042/60000 (87%)] | Loss:   37.525021 |rec:  36.402241 | kl:   56.139008
Epoch 001 | Time 0.0575(0.0576) | [ 1062/60000 (88%)] | Loss:   35.394344 |rec:  34.269135 | kl:   56.260555
Epoch 001 | Time 0.0575(0.0576) | [ 1082/60000 (90%)] | Loss:   36.313057 |rec:  35.161739 | kl:   57.566067
Epoch 001 | Time 0.0574(0.0576) | [ 1102/60000 (92%)] | Loss:   38.234303 |rec:  37.027195 | kl:   60.355358
Epoch 001 | Time 0.0575(0.0576) | [ 1122/60000 (93%)] | Loss:   36.883541 |rec:  35.781090 | kl:   55.122597
Epoch 001 | Time 0.0575(0.0576) | [ 1142/60000 (95%)] | Loss:   36.303516 |rec:  35.112446 | kl:   59.553596
Epoch 001 | Time 0.0575(0.0576) | [ 1162/60000 (97%)] | Loss:   36.237331 |rec:  35.105808 | kl:   56.576195
Epoch 001 | Time 0.0575(0.0576) | [ 1182/60000 (98%)] | Loss:   36.617844 |rec:  35.473717 | kl:   57.206440
Epoch: 2 	Beta: 0.03
Epoch 002 | Time 0.0589(0.0577) | [    2/60000 ( 0%)] | Loss:   35.831921 |rec:  34.220959 | kl:   53.698757
Epoch 002 | Time 0.0574(0.0576) | [   22/60000 ( 2%)] | Loss:   39.051041 |rec:  37.493992 | kl:   51.901669
Epoch 002 | Time 0.0575(0.0576) | [   42/60000 ( 3%)] | Loss:   37.697002 |rec:  36.127422 | kl:   52.319324
Epoch 002 | Time 0.0575(0.0576) | [   62/60000 ( 5%)] | Loss:   36.422947 |rec:  34.917183 | kl:   50.192139
Epoch 002 | Time 0.0575(0.0576) | [   82/60000 ( 7%)] | Loss:   36.519630 |rec:  34.971752 | kl:   51.595966
Epoch 002 | Time 0.0575(0.0576) | [  102/60000 ( 8%)] | Loss:   36.690868 |rec:  35.120930 | kl:   52.331337
Epoch 002 | Time 0.0575(0.0577) | [  122/60000 (10%)] | Loss:   37.833611 |rec:  36.324982 | kl:   50.287712
Epoch 002 | Time 0.0575(0.0576) | [  142/60000 (12%)] | Loss:   37.431019 |rec:  35.917202 | kl:   50.460423
Epoch 002 | Time 0.0574(0.0576) | [  162/60000 (13%)] | Loss:   36.373875 |rec:  34.927036 | kl:   48.228027
Epoch 002 | Time 0.0575(0.0576) | [  182/60000 (15%)] | Loss:   36.225914 |rec:  34.756588 | kl:   48.977509
Epoch 002 | Time 0.0575(0.0576) | [  202/60000 (17%)] | Loss:   37.731045 |rec:  36.191444 | kl:   51.320019
Epoch 002 | Time 0.0576(0.0576) | [  222/60000 (18%)] | Loss:   36.824207 |rec:  35.336426 | kl:   49.592632
Epoch 002 | Time 0.0576(0.0576) | [  242/60000 (20%)] | Loss:   38.093811 |rec:  36.561737 | kl:   51.069107
Epoch 002 | Time 0.0575(0.0576) | [  262/60000 (22%)] | Loss:   37.552505 |rec:  36.048782 | kl:   50.124039
Epoch 002 | Time 0.0575(0.0577) | [  282/60000 (23%)] | Loss:   36.412128 |rec:  34.919209 | kl:   49.764038
Epoch 002 | Time 0.0575(0.0577) | [  302/60000 (25%)] | Loss:   37.219391 |rec:  35.687000 | kl:   51.079716
Epoch 002 | Time 0.0575(0.0576) | [  322/60000 (27%)] | Loss:   36.964809 |rec:  35.486595 | kl:   49.273819
Epoch 002 | Time 0.0575(0.0576) | [  342/60000 (28%)] | Loss:   36.501656 |rec:  34.992386 | kl:   50.309097
Epoch 002 | Time 0.0574(0.0576) | [  362/60000 (30%)] | Loss:   38.245911 |rec:  36.722042 | kl:   50.795723
Epoch 002 | Time 0.0575(0.0576) | [  382/60000 (32%)] | Loss:   38.071693 |rec:  36.535450 | kl:   51.208157
Epoch 002 | Time 0.0574(0.0576) | [  402/60000 (33%)] | Loss:   37.045841 |rec:  35.591091 | kl:   48.491718
Epoch 002 | Time 0.0575(0.0576) | [  422/60000 (35%)] | Loss:   37.798161 |rec:  36.310848 | kl:   49.577122
Epoch 002 | Time 0.0574(0.0576) | [  442/60000 (37%)] | Loss:   35.922649 |rec:  34.470989 | kl:   48.388641
Epoch 002 | Time 0.0574(0.0576) | [  462/60000 (38%)] | Loss:   36.218098 |rec:  34.714268 | kl:   50.127621
Epoch 002 | Time 0.0575(0.0576) | [  482/60000 (40%)] | Loss:   36.509251 |rec:  35.056606 | kl:   48.421577
Epoch 002 | Time 0.0574(0.0576) | [  502/60000 (42%)] | Loss:   35.163948 |rec:  33.728821 | kl:   47.837540
Epoch 002 | Time 0.0575(0.0576) | [  522/60000 (43%)] | Loss:   35.584366 |rec:  34.146702 | kl:   47.922089
Epoch 002 | Time 0.0575(0.0576) | [  542/60000 (45%)] | Loss:   36.376194 |rec:  34.899082 | kl:   49.236965
Epoch 002 | Time 0.0575(0.0576) | [  562/60000 (47%)] | Loss:   35.868614 |rec:  34.400539 | kl:   48.935780
Epoch 002 | Time 0.0575(0.0576) | [  582/60000 (48%)] | Loss:   38.339504 |rec:  36.847233 | kl:   49.742386
Epoch 002 | Time 0.0575(0.0576) | [  602/60000 (50%)] | Loss:   34.692329 |rec:  33.240776 | kl:   48.385185
Epoch 002 | Time 0.0576(0.0575) | [  622/60000 (52%)] | Loss:   36.411026 |rec:  34.921944 | kl:   49.635979
Epoch 002 | Time 0.0575(0.0575) | [  642/60000 (53%)] | Loss:   37.046314 |rec:  35.534050 | kl:   50.408871
Epoch 002 | Time 0.0575(0.0576) | [  662/60000 (55%)] | Loss:   37.002510 |rec:  35.549046 | kl:   48.448833
Epoch 002 | Time 0.0575(0.0576) | [  682/60000 (57%)] | Loss:   38.404522 |rec:  36.888489 | kl:   50.534534
Epoch 002 | Time 0.0575(0.0576) | [  702/60000 (58%)] | Loss:   37.593113 |rec:  36.047253 | kl:   51.528694
Epoch 002 | Time 0.0575(0.0576) | [  722/60000 (60%)] | Loss:   36.812943 |rec:  35.380703 | kl:   47.741333
Epoch 002 | Time 0.0575(0.0576) | [  742/60000 (62%)] | Loss:   35.227711 |rec:  33.720947 | kl:   50.225445
Epoch 002 | Time 0.0574(0.0576) | [  762/60000 (63%)] | Loss:   35.542843 |rec:  34.064251 | kl:   49.286327
Epoch 002 | Time 0.0574(0.0576) | [  782/60000 (65%)] | Loss:   36.829220 |rec:  35.356583 | kl:   49.087982
Epoch 002 | Time 0.0575(0.0576) | [  802/60000 (67%)] | Loss:   36.547092 |rec:  35.093609 | kl:   48.449512
Epoch 002 | Time 0.0575(0.0576) | [  822/60000 (68%)] | Loss:   36.741505 |rec:  35.258144 | kl:   49.445301
Epoch 002 | Time 0.0575(0.0576) | [  842/60000 (70%)] | Loss:   34.737198 |rec:  33.313690 | kl:   47.450214
Epoch 002 | Time 0.0575(0.0576) | [  862/60000 (72%)] | Loss:   36.249050 |rec:  34.822140 | kl:   47.563606
Epoch 002 | Time 0.0575(0.0576) | [  882/60000 (73%)] | Loss:   35.323284 |rec:  33.864700 | kl:   48.619453
Epoch 002 | Time 0.0575(0.0576) | [  902/60000 (75%)] | Loss:   35.115635 |rec:  33.705776 | kl:   46.995380
Epoch 002 | Time 0.0575(0.0576) | [  922/60000 (77%)] | Loss:   36.693413 |rec:  35.223621 | kl:   48.993027
Epoch 002 | Time 0.0575(0.0576) | [  942/60000 (78%)] | Loss:   35.414951 |rec:  33.963062 | kl:   48.396381
Epoch 002 | Time 0.0575(0.0576) | [  962/60000 (80%)] | Loss:   36.717587 |rec:  35.286358 | kl:   47.707554
Epoch 002 | Time 0.0575(0.0576) | [  982/60000 (82%)] | Loss:   35.602032 |rec:  34.177418 | kl:   47.487175
Epoch 002 | Time 0.0575(0.0576) | [ 1002/60000 (83%)] | Loss:   35.393467 |rec:  33.922302 | kl:   49.038822
Epoch 002 | Time 0.0575(0.0576) | [ 1022/60000 (85%)] | Loss:   35.144493 |rec:  33.669609 | kl:   49.162724
Epoch 002 | Time 0.0575(0.0576) | [ 1042/60000 (87%)] | Loss:   35.106281 |rec:  33.661839 | kl:   48.148003
Epoch 002 | Time 0.0575(0.0576) | [ 1062/60000 (88%)] | Loss:   37.033108 |rec:  35.529392 | kl:   50.123894
Epoch 002 | Time 0.0575(0.0576) | [ 1082/60000 (90%)] | Loss:   34.511726 |rec:  33.089859 | kl:   47.395557
Epoch 002 | Time 0.0575(0.0576) | [ 1102/60000 (92%)] | Loss:   36.952480 |rec:  35.438137 | kl:   50.478100
Epoch 002 | Time 0.0576(0.0576) | [ 1122/60000 (93%)] | Loss:   34.625851 |rec:  33.184063 | kl:   48.059685
Epoch 002 | Time 0.0574(0.0576) | [ 1142/60000 (95%)] | Loss:   34.609131 |rec:  33.151123 | kl:   48.600273
Epoch 002 | Time 0.0574(0.0576) | [ 1162/60000 (97%)] | Loss:   33.634823 |rec:  32.257973 | kl:   45.895039
Epoch 002 | Time 0.0575(0.0576) | [ 1182/60000 (98%)] | Loss:   36.201878 |rec:  34.746040 | kl:   48.527939
Epoch: 3 	Beta: 0.04
Epoch 003 | Time 0.0591(0.0576) | [    2/60000 ( 0%)] | Loss:   36.516117 |rec:  34.534462 | kl:   49.541382
Epoch 003 | Time 0.0575(0.0576) | [   22/60000 ( 2%)] | Loss:   36.435974 |rec:  34.637478 | kl:   44.962376
Epoch 003 | Time 0.0575(0.0576) | [   42/60000 ( 3%)] | Loss:   36.158020 |rec:  34.370522 | kl:   44.687450
Epoch 003 | Time 0.0575(0.0576) | [   62/60000 ( 5%)] | Loss:   37.577198 |rec:  35.732182 | kl:   46.125374
Epoch 003 | Time 0.0575(0.0576) | [   82/60000 ( 7%)] | Loss:   34.989071 |rec:  33.167332 | kl:   45.543514
Epoch 003 | Time 0.0575(0.0576) | [  102/60000 ( 8%)] | Loss:   34.989132 |rec:  33.266476 | kl:   43.066441
Epoch 003 | Time 0.0576(0.0576) | [  122/60000 (10%)] | Loss:   34.596577 |rec:  32.815666 | kl:   44.522758
Epoch 003 | Time 0.0575(0.0576) | [  142/60000 (12%)] | Loss:   36.460258 |rec:  34.680088 | kl:   44.504288
Epoch 003 | Time 0.0575(0.0576) | [  162/60000 (13%)] | Loss:   36.015869 |rec:  34.205307 | kl:   45.264042
Epoch 003 | Time 0.0575(0.0576) | [  182/60000 (15%)] | Loss:   35.954433 |rec:  34.205658 | kl:   43.719410
Epoch 003 | Time 0.0575(0.0576) | [  202/60000 (17%)] | Loss:   37.627308 |rec:  35.822041 | kl:   45.131664
Epoch 003 | Time 0.0575(0.0576) | [  222/60000 (18%)] | Loss:   35.861820 |rec:  34.027222 | kl:   45.865013
Epoch 003 | Time 0.0574(0.0576) | [  242/60000 (20%)] | Loss:   35.904060 |rec:  34.125816 | kl:   44.456181
Epoch 003 | Time 0.0575(0.0576) | [  262/60000 (22%)] | Loss:   35.177700 |rec:  33.340881 | kl:   45.920437
Epoch 003 | Time 0.0575(0.0576) | [  282/60000 (23%)] | Loss:   35.700054 |rec:  33.900406 | kl:   44.991291
Epoch 003 | Time 0.0575(0.0576) | [  302/60000 (25%)] | Loss:   37.243610 |rec:  35.438423 | kl:   45.129688
Epoch 003 | Time 0.0575(0.0576) | [  322/60000 (27%)] | Loss:   35.828979 |rec:  34.035995 | kl:   44.824608
Epoch 003 | Time 0.0574(0.0576) | [  342/60000 (28%)] | Loss:   35.697536 |rec:  33.922607 | kl:   44.373314
Epoch 003 | Time 0.0575(0.0576) | [  362/60000 (30%)] | Loss:   36.476250 |rec:  34.617924 | kl:   46.458149
Epoch 003 | Time 0.0575(0.0576) | [  382/60000 (32%)] | Loss:   36.488384 |rec:  34.745213 | kl:   43.579277
Epoch 003 | Time 0.0575(0.0576) | [  402/60000 (33%)] | Loss:   34.129536 |rec:  32.385662 | kl:   43.596863
Epoch 003 | Time 0.0575(0.0576) | [  422/60000 (35%)] | Loss:   35.860302 |rec:  34.100552 | kl:   43.993759
Epoch 003 | Time 0.0575(0.0576) | [  442/60000 (37%)] | Loss:   35.305504 |rec:  33.510517 | kl:   44.874615
Epoch 003 | Time 0.0575(0.0576) | [  462/60000 (38%)] | Loss:   35.303352 |rec:  33.593468 | kl:   42.747154
Epoch 003 | Time 0.0575(0.0576) | [  482/60000 (40%)] | Loss:   34.776474 |rec:  33.016361 | kl:   44.002811
Epoch 003 | Time 0.0577(0.0576) | [  502/60000 (42%)] | Loss:   35.367580 |rec:  33.583435 | kl:   44.603603
Epoch 003 | Time 0.0575(0.0577) | [  522/60000 (43%)] | Loss:   35.612667 |rec:  33.862366 | kl:   43.757450
Epoch 003 | Time 0.0576(0.0580) | [  542/60000 (45%)] | Loss:   36.342178 |rec:  34.529007 | kl:   45.329266
Epoch 003 | Time 0.0575(0.0579) | [  562/60000 (47%)] | Loss:   35.327217 |rec:  33.557690 | kl:   44.238243
Epoch 003 | Time 0.0582(0.0583) | [  582/60000 (48%)] | Loss:   35.945461 |rec:  34.189175 | kl:   43.907234
Epoch 003 | Time 0.0604(0.0591) | [  602/60000 (50%)] | Loss:   37.543919 |rec:  35.774109 | kl:   44.245209
Epoch 003 | Time 0.0587(0.0590) | [  622/60000 (52%)] | Loss:   35.499805 |rec:  33.712791 | kl:   44.675308
Epoch 003 | Time 0.0603(0.0591) | [  642/60000 (53%)] | Loss:   34.875435 |rec:  33.063828 | kl:   45.290142
Epoch 003 | Time 0.0574(0.0587) | [  662/60000 (55%)] | Loss:   37.012238 |rec:  35.160610 | kl:   46.290668
Epoch 003 | Time 0.0575(0.0585) | [  682/60000 (57%)] | Loss:   35.079693 |rec:  33.309467 | kl:   44.255650
Epoch 003 | Time 0.0601(0.0583) | [  702/60000 (58%)] | Loss:   36.987022 |rec:  35.162445 | kl:   45.614441
Epoch 003 | Time 0.0616(0.0590) | [  722/60000 (60%)] | Loss:   36.343910 |rec:  34.590000 | kl:   43.847794
Epoch 003 | Time 0.0575(0.0590) | [  742/60000 (62%)] | Loss:   35.827827 |rec:  34.022011 | kl:   45.145378
Epoch 003 | Time 0.0615(0.0594) | [  762/60000 (63%)] | Loss:   35.425953 |rec:  33.625916 | kl:   45.000916
Epoch 003 | Time 0.0575(0.0590) | [  782/60000 (65%)] | Loss:   34.692234 |rec:  32.926014 | kl:   44.155521
Epoch 003 | Time 0.0576(0.0586) | [  802/60000 (67%)] | Loss:   35.001991 |rec:  33.241879 | kl:   44.002792
Epoch 003 | Time 0.0575(0.0584) | [  822/60000 (68%)] | Loss:   36.412262 |rec:  34.649776 | kl:   44.062187
Epoch 003 | Time 0.0575(0.0582) | [  842/60000 (70%)] | Loss:   33.506409 |rec:  31.764170 | kl:   43.555958
Epoch 003 | Time 0.0608(0.0585) | [  862/60000 (72%)] | Loss:   35.863663 |rec:  34.097851 | kl:   44.145306
Epoch 003 | Time 0.0614(0.0592) | [  882/60000 (73%)] | Loss:   36.258884 |rec:  34.431274 | kl:   45.690186
Epoch 003 | Time 0.0575(0.0589) | [  902/60000 (75%)] | Loss:   34.588081 |rec:  32.809170 | kl:   44.472820
Epoch 003 | Time 0.0575(0.0587) | [  922/60000 (77%)] | Loss:   34.133121 |rec:  32.394871 | kl:   43.456326
Epoch 003 | Time 0.0575(0.0584) | [  942/60000 (78%)] | Loss:   36.184864 |rec:  34.449570 | kl:   43.382347
Epoch 003 | Time 0.0575(0.0582) | [  962/60000 (80%)] | Loss:   35.196190 |rec:  33.456139 | kl:   43.501331
Epoch 003 | Time 0.0618(0.0587) | [  982/60000 (82%)] | Loss:   34.795616 |rec:  33.041065 | kl:   43.863792
Epoch 003 | Time 0.0574(0.0585) | [ 1002/60000 (83%)] | Loss:   35.182419 |rec:  33.480469 | kl:   42.548759
Epoch 003 | Time 0.0575(0.0583) | [ 1022/60000 (85%)] | Loss:   36.619095 |rec:  34.825180 | kl:   44.847923
Epoch 003 | Time 0.0575(0.0581) | [ 1042/60000 (87%)] | Loss:   34.410751 |rec:  32.685547 | kl:   43.130154
Epoch 003 | Time 0.0576(0.0580) | [ 1062/60000 (88%)] | Loss:   34.820515 |rec:  33.064846 | kl:   43.891747
Epoch 003 | Time 0.0575(0.0579) | [ 1082/60000 (90%)] | Loss:   34.438114 |rec:  32.727646 | kl:   42.761703
Epoch 003 | Time 0.0576(0.0578) | [ 1102/60000 (92%)] | Loss:   35.114353 |rec:  33.306538 | kl:   45.195435
Epoch 003 | Time 0.0574(0.0578) | [ 1122/60000 (93%)] | Loss:   35.795914 |rec:  33.975044 | kl:   45.521797
Epoch 003 | Time 0.0574(0.0577) | [ 1142/60000 (95%)] | Loss:   34.254913 |rec:  32.488853 | kl:   44.151524
Epoch 003 | Time 0.0575(0.0577) | [ 1162/60000 (97%)] | Loss:   34.374043 |rec:  32.554829 | kl:   45.480331
Epoch 003 | Time 0.0575(0.0577) | [ 1182/60000 (98%)] | Loss:   35.920902 |rec:  34.146519 | kl:   44.359638
Epoch: 4 	Beta: 0.05
Epoch 004 | Time 0.0590(0.0577) | [    2/60000 ( 0%)] | Loss:   34.218460 |rec:  32.096874 | kl:   42.431683
Epoch 004 | Time 0.0575(0.0578) | [   22/60000 ( 2%)] | Loss:   35.758930 |rec:  33.619774 | kl:   42.783138
Epoch 004 | Time 0.0575(0.0577) | [   42/60000 ( 3%)] | Loss:   34.892551 |rec:  32.783596 | kl:   42.179077
Epoch 004 | Time 0.0575(0.0577) | [   62/60000 ( 5%)] | Loss:   34.734592 |rec:  32.630440 | kl:   42.083065
Epoch 004 | Time 0.0576(0.0576) | [   82/60000 ( 7%)] | Loss:   36.716309 |rec:  34.629856 | kl:   41.728977
Epoch 004 | Time 0.0576(0.0578) | [  102/60000 ( 8%)] | Loss:   34.665359 |rec:  32.610386 | kl:   41.099461
Epoch 004 | Time 0.0577(0.0579) | [  122/60000 (10%)] | Loss:   34.958538 |rec:  32.760181 | kl:   43.967155
Epoch 004 | Time 0.0578(0.0579) | [  142/60000 (12%)] | Loss:   35.780941 |rec:  33.674133 | kl:   42.136158
Epoch 004 | Time 0.0576(0.0578) | [  162/60000 (13%)] | Loss:   35.146557 |rec:  33.061604 | kl:   41.699062
Epoch 004 | Time 0.0576(0.0578) | [  182/60000 (15%)] | Loss:   34.459358 |rec:  32.333988 | kl:   42.507381
Epoch 004 | Time 0.0577(0.0578) | [  202/60000 (17%)] | Loss:   35.031372 |rec:  32.962681 | kl:   41.373833
Epoch 004 | Time 0.0577(0.0578) | [  222/60000 (18%)] | Loss:   36.838890 |rec:  34.658699 | kl:   43.603794
Epoch 004 | Time 0.0576(0.0578) | [  242/60000 (20%)] | Loss:   34.425014 |rec:  32.351589 | kl:   41.468575
Epoch 004 | Time 0.0577(0.0578) | [  262/60000 (22%)] | Loss:   33.673122 |rec:  31.618847 | kl:   41.085545
Epoch 004 | Time 0.0577(0.0577) | [  282/60000 (23%)] | Loss:   35.061729 |rec:  33.067856 | kl:   39.877468
Epoch 004 | Time 0.0576(0.0578) | [  302/60000 (25%)] | Loss:   34.912128 |rec:  32.932022 | kl:   39.602112
Epoch 004 | Time 0.0576(0.0577) | [  322/60000 (27%)] | Loss:   34.847672 |rec:  32.834595 | kl:   40.261551
Epoch 004 | Time 0.0577(0.0578) | [  342/60000 (28%)] | Loss:   35.910160 |rec:  33.818344 | kl:   41.836311
Epoch 004 | Time 0.0577(0.0577) | [  362/60000 (30%)] | Loss:   34.879379 |rec:  32.779823 | kl:   41.991108
Epoch 004 | Time 0.0577(0.0578) | [  382/60000 (32%)] | Loss:   35.352425 |rec:  33.206097 | kl:   42.926552
Epoch 004 | Time 0.0576(0.0577) | [  402/60000 (33%)] | Loss:   35.993538 |rec:  33.848747 | kl:   42.895847
Epoch 004 | Time 0.0575(0.0577) | [  422/60000 (35%)] | Loss:   35.641205 |rec:  33.489960 | kl:   43.024902
Epoch 004 | Time 0.0575(0.0577) | [  442/60000 (37%)] | Loss:   34.723640 |rec:  32.657906 | kl:   41.314636
Epoch 004 | Time 0.0575(0.0576) | [  462/60000 (38%)] | Loss:   35.772976 |rec:  33.697598 | kl:   41.507561
Epoch 004 | Time 0.0574(0.0576) | [  482/60000 (40%)] | Loss:   34.677094 |rec:  32.536350 | kl:   42.814869
Epoch 004 | Time 0.0575(0.0576) | [  502/60000 (42%)] | Loss:   36.935143 |rec:  34.819077 | kl:   42.321281
Epoch 004 | Time 0.0574(0.0576) | [  522/60000 (43%)] | Loss:   35.241295 |rec:  33.203716 | kl:   40.751492
Epoch 004 | Time 0.0575(0.0576) | [  542/60000 (45%)] | Loss:   34.201145 |rec:  32.143623 | kl:   41.150433
Epoch 004 | Time 0.0574(0.0576) | [  562/60000 (47%)] | Loss:   33.904819 |rec:  31.951250 | kl:   39.071396
Epoch 004 | Time 0.0575(0.0576) | [  582/60000 (48%)] | Loss:   36.987350 |rec:  34.799454 | kl:   43.757904
Epoch 004 | Time 0.0576(0.0576) | [  602/60000 (50%)] | Loss:   35.617348 |rec:  33.514294 | kl:   42.061104
Epoch 004 | Time 0.0575(0.0576) | [  622/60000 (52%)] | Loss:   36.271580 |rec:  34.147945 | kl:   42.472672
Epoch 004 | Time 0.0575(0.0576) | [  642/60000 (53%)] | Loss:   34.694824 |rec:  32.498917 | kl:   43.918213
Epoch 004 | Time 0.0575(0.0576) | [  662/60000 (55%)] | Loss:   35.360657 |rec:  33.327538 | kl:   40.662350
Epoch 004 | Time 0.0588(0.0576) | [  682/60000 (57%)] | Loss:   35.613178 |rec:  33.538342 | kl:   41.496746
Epoch 004 | Time 0.0607(0.0578) | [  702/60000 (58%)] | Loss:   35.600361 |rec:  33.415619 | kl:   43.694817
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3422(0.3422) | [    2/60000 ( 0%)] | Loss:  250.417984 |rec: 250.407806 | kl:    1.017402
Epoch 000 | Time 0.0567(0.2675) | [   22/60000 ( 2%)] | Loss:  193.321228 |rec: 193.311111 | kl:    1.011884
Epoch 000 | Time 0.0568(0.2122) | [   42/60000 ( 3%)] | Loss:  177.939743 |rec: 177.923096 | kl:    1.665033
Epoch 000 | Time 0.0568(0.1714) | [   62/60000 ( 5%)] | Loss:  174.275192 |rec: 174.265076 | kl:    1.011217
Epoch 000 | Time 0.0568(0.1415) | [   82/60000 ( 7%)] | Loss:  169.568039 |rec: 169.229553 | kl:   33.849812
Epoch 000 | Time 0.0570(0.1194) | [  102/60000 ( 8%)] | Loss:  161.372696 |rec: 161.361038 | kl:    1.166285
Epoch 000 | Time 0.0570(0.1030) | [  122/60000 (10%)] | Loss:  149.542847 |rec: 149.521133 | kl:    2.171012
Epoch 000 | Time 0.0570(0.0910) | [  142/60000 (12%)] | Loss:  119.445961 |rec: 119.077950 | kl:   36.801456
Epoch 000 | Time 0.0569(0.0821) | [  162/60000 (13%)] | Loss:  110.510742 |rec: 110.132301 | kl:   37.843479
Epoch 000 | Time 0.0569(0.0756) | [  182/60000 (15%)] | Loss:  105.189140 |rec: 104.905800 | kl:   28.334335
Epoch 000 | Time 0.0570(0.0707) | [  202/60000 (17%)] | Loss:   99.878677 |rec:  99.623169 | kl:   25.550425
Epoch 000 | Time 0.0570(0.0672) | [  222/60000 (18%)] | Loss:  102.223297 |rec: 101.976219 | kl:   24.708374
Epoch 000 | Time 0.0569(0.0646) | [  242/60000 (20%)] | Loss:  101.688797 |rec: 101.415489 | kl:   27.330864
Epoch 000 | Time 0.0575(0.0626) | [  262/60000 (22%)] | Loss:   96.954063 |rec:  96.673515 | kl:   28.054630
Epoch 000 | Time 0.0569(0.0615) | [  282/60000 (23%)] | Loss:   98.986794 |rec:  98.691216 | kl:   29.557430
Epoch 000 | Time 0.0597(0.0607) | [  302/60000 (25%)] | Loss:  100.337685 |rec:  99.976913 | kl:   36.077568
Epoch 000 | Time 0.0583(0.0602) | [  322/60000 (27%)] | Loss:   93.493973 |rec:  93.184578 | kl:   30.939682
Epoch 000 | Time 0.0568(0.0597) | [  342/60000 (28%)] | Loss:   95.490486 |rec:  95.059677 | kl:   43.080894
Epoch 000 | Time 0.0568(0.0591) | [  362/60000 (30%)] | Loss:   88.829277 |rec:  88.413330 | kl:   41.594818
Epoch 000 | Time 0.0567(0.0586) | [  382/60000 (32%)] | Loss:   88.164116 |rec:  87.685722 | kl:   47.839569
Epoch 000 | Time 0.0569(0.0581) | [  402/60000 (33%)] | Loss:   93.076324 |rec:  92.701607 | kl:   37.471638
Epoch 000 | Time 0.0568(0.0578) | [  422/60000 (35%)] | Loss:   85.939766 |rec:  85.375916 | kl:   56.384434
Epoch 000 | Time 0.0569(0.0577) | [  442/60000 (37%)] | Loss:   80.915443 |rec:  80.163979 | kl:   75.146278
Epoch 000 | Time 0.0579(0.0575) | [  462/60000 (38%)] | Loss:   78.741158 |rec:  77.990356 | kl:   75.079803
Epoch 000 | Time 0.0568(0.0574) | [  482/60000 (40%)] | Loss:   76.887604 |rec:  76.153419 | kl:   73.418228
Epoch 000 | Time 0.0586(0.0573) | [  502/60000 (42%)] | Loss:   79.146255 |rec:  78.341202 | kl:   80.504845
Epoch 000 | Time 0.0572(0.0574) | [  522/60000 (43%)] | Loss:   72.790359 |rec:  71.964417 | kl:   82.594223
Epoch 000 | Time 0.0571(0.0574) | [  542/60000 (45%)] | Loss:   70.671471 |rec:  69.897667 | kl:   77.380264
Epoch 000 | Time 0.0571(0.0574) | [  562/60000 (47%)] | Loss:   68.342674 |rec:  67.425583 | kl:   91.709663
Epoch 000 | Time 0.0571(0.0575) | [  582/60000 (48%)] | Loss:   69.662155 |rec:  68.728058 | kl:   93.409653
Epoch 000 | Time 0.0570(0.0574) | [  602/60000 (50%)] | Loss:   67.629303 |rec:  66.567131 | kl:  106.217400
Epoch 000 | Time 0.0570(0.0576) | [  622/60000 (52%)] | Loss:   65.954918 |rec:  64.943367 | kl:  101.155045
Epoch 000 | Time 0.0601(0.0575) | [  642/60000 (53%)] | Loss:   65.681595 |rec:  64.548126 | kl:  113.346664
Epoch 000 | Time 0.0570(0.0575) | [  662/60000 (55%)] | Loss:   61.188168 |rec:  60.013172 | kl:  117.499420
Epoch 000 | Time 0.0588(0.0575) | [  682/60000 (57%)] | Loss:   60.825687 |rec:  59.610092 | kl:  121.559341
Epoch 000 | Time 0.0613(0.0577) | [  702/60000 (58%)] | Loss:   55.912548 |rec:  54.748981 | kl:  116.356529
Epoch 000 | Time 0.0570(0.0576) | [  722/60000 (60%)] | Loss:   57.947239 |rec:  56.670555 | kl:  127.668610
Epoch 000 | Time 0.0610(0.0580) | [  742/60000 (62%)] | Loss:   53.112938 |rec:  51.888271 | kl:  122.466949
Epoch 000 | Time 0.0573(0.0584) | [  762/60000 (63%)] | Loss:   54.581982 |rec:  53.472893 | kl:  110.908897
Epoch 000 | Time 0.0570(0.0581) | [  782/60000 (65%)] | Loss:   53.018681 |rec:  51.906052 | kl:  111.262833
Epoch 000 | Time 0.0588(0.0579) | [  802/60000 (67%)] | Loss:   53.699036 |rec:  52.486187 | kl:  121.285271
Epoch 000 | Time 0.0570(0.0579) | [  822/60000 (68%)] | Loss:   50.848125 |rec:  49.681732 | kl:  116.639130
Epoch 000 | Time 0.0571(0.0578) | [  842/60000 (70%)] | Loss:   50.287621 |rec:  49.085083 | kl:  120.253944
Epoch 000 | Time 0.0570(0.0577) | [  862/60000 (72%)] | Loss:   49.822159 |rec:  48.753124 | kl:  106.903107
Epoch 000 | Time 0.0570(0.0575) | [  882/60000 (73%)] | Loss:   49.535843 |rec:  48.407429 | kl:  112.841400
Epoch 000 | Time 0.0577(0.0575) | [  902/60000 (75%)] | Loss:   48.921654 |rec:  47.845047 | kl:  107.660675
Epoch 000 | Time 0.0570(0.0575) | [  922/60000 (77%)] | Loss:   48.816544 |rec:  47.670620 | kl:  114.592407
Epoch 000 | Time 0.0571(0.0574) | [  942/60000 (78%)] | Loss:   51.813301 |rec:  50.705566 | kl:  110.773575
Epoch 000 | Time 0.0576(0.0574) | [  962/60000 (80%)] | Loss:   47.757504 |rec:  46.610760 | kl:  114.674187
Epoch 000 | Time 0.0571(0.0574) | [  982/60000 (82%)] | Loss:   46.852154 |rec:  45.777481 | kl:  107.467377
Epoch 000 | Time 0.0570(0.0573) | [ 1002/60000 (83%)] | Loss:   45.023094 |rec:  43.922623 | kl:  110.047554
Epoch 000 | Time 0.0570(0.0574) | [ 1022/60000 (85%)] | Loss:   43.768711 |rec:  42.721794 | kl:  104.691635
Epoch 000 | Time 0.0571(0.0573) | [ 1042/60000 (87%)] | Loss:   46.997974 |rec:  45.945206 | kl:  105.277016
Epoch 000 | Time 0.0570(0.0573) | [ 1062/60000 (88%)] | Loss:   44.451580 |rec:  43.396080 | kl:  105.550079
Epoch 000 | Time 0.0570(0.0573) | [ 1082/60000 (90%)] | Loss:   43.949955 |rec:  42.933235 | kl:  101.672058
Epoch 000 | Time 0.0571(0.0573) | [ 1102/60000 (92%)] | Loss:   44.513744 |rec:  43.469761 | kl:  104.398521
Epoch 000 | Time 0.0571(0.0573) | [ 1122/60000 (93%)] | Loss:   44.410530 |rec:  43.430489 | kl:   98.004295
Epoch 000 | Time 0.0571(0.0573) | [ 1142/60000 (95%)] | Loss:   45.247734 |rec:  44.214905 | kl:  103.282524
Epoch 000 | Time 0.0571(0.0573) | [ 1162/60000 (97%)] | Loss:   41.929951 |rec:  40.918575 | kl:  101.137413
Epoch 000 | Time 0.0571(0.0573) | [ 1182/60000 (98%)] | Loss:   44.682552 |rec:  43.665989 | kl:  101.656281
validating...
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3709(0.3709) | [    2/60000 ( 0%)] | Loss:  257.113312 |rec: 257.103821 | kl:    0.949032
Epoch 000 | Time 0.0566(0.2886) | [   22/60000 ( 2%)] | Loss:  233.417267 |rec: 233.408371 | kl:    0.887867
Epoch 000 | Time 0.0566(0.2277) | [   42/60000 ( 3%)] | Loss:  230.039719 |rec: 229.601807 | kl:   43.790161
Epoch 000 | Time 0.0566(0.1829) | [   62/60000 ( 5%)] | Loss:  190.515869 |rec: 186.615509 | kl:  390.036896
Epoch 000 | Time 0.0566(0.1498) | [   82/60000 ( 7%)] | Loss:  148.224716 |rec: 146.815384 | kl:  140.934860
Epoch 000 | Time 0.0567(0.1255) | [  102/60000 ( 8%)] | Loss:  123.183464 |rec: 119.386940 | kl:  379.652557
Epoch 000 | Time 0.0567(0.1075) | [  122/60000 (10%)] | Loss:  112.032074 |rec: 110.132080 | kl:  189.999969
Epoch 000 | Time 0.0567(0.0942) | [  142/60000 (12%)] | Loss:  111.184937 |rec: 110.005951 | kl:  117.898056
Epoch 000 | Time 0.0567(0.0844) | [  162/60000 (13%)] | Loss:  114.083656 |rec: 113.158562 | kl:   92.509483
Epoch 000 | Time 0.0566(0.0772) | [  182/60000 (15%)] | Loss:  106.696671 |rec: 105.373398 | kl:  132.326981
Epoch 000 | Time 0.0567(0.0719) | [  202/60000 (17%)] | Loss:  107.972443 |rec: 106.750381 | kl:  122.206505
Epoch 000 | Time 0.0567(0.0679) | [  222/60000 (18%)] | Loss:  101.591492 |rec: 100.568657 | kl:  102.283302
Epoch 000 | Time 0.0567(0.0650) | [  242/60000 (20%)] | Loss:  102.030968 |rec: 101.370163 | kl:   66.080162
Epoch 000 | Time 0.0567(0.0630) | [  262/60000 (22%)] | Loss:  103.522301 |rec: 102.644188 | kl:   87.811989
Epoch 000 | Time 0.0567(0.0614) | [  282/60000 (23%)] | Loss:  107.670761 |rec: 107.021072 | kl:   64.969223
Epoch 000 | Time 0.0568(0.0603) | [  302/60000 (25%)] | Loss:  100.079361 |rec:  99.188911 | kl:   89.044868
Epoch 000 | Time 0.0567(0.0594) | [  322/60000 (27%)] | Loss:  102.995850 |rec: 102.388855 | kl:   60.699608
Epoch 000 | Time 0.0567(0.0587) | [  342/60000 (28%)] | Loss:   97.057831 |rec:  96.282684 | kl:   77.514824
Epoch 000 | Time 0.0567(0.0583) | [  362/60000 (30%)] | Loss:   99.259186 |rec:  98.499565 | kl:   75.961533
Epoch 000 | Time 0.0582(0.0579) | [  382/60000 (32%)] | Loss:   99.983955 |rec:  99.069473 | kl:   91.447769
Epoch 000 | Time 0.0567(0.0576) | [  402/60000 (33%)] | Loss:   95.006508 |rec:  94.184975 | kl:   82.153465
Epoch 000 | Time 0.0567(0.0574) | [  422/60000 (35%)] | Loss:   93.306679 |rec:  92.438332 | kl:   86.834862
Epoch 000 | Time 0.0567(0.0572) | [  442/60000 (37%)] | Loss:   92.795815 |rec:  91.931755 | kl:   86.406258
Epoch 000 | Time 0.0570(0.0572) | [  462/60000 (38%)] | Loss:   89.129997 |rec:  88.296638 | kl:   83.336197
Epoch 000 | Time 0.0569(0.0571) | [  482/60000 (40%)] | Loss:   89.374336 |rec:  88.548515 | kl:   82.581696
Epoch 000 | Time 0.0570(0.0571) | [  502/60000 (42%)] | Loss:   86.159424 |rec:  85.299736 | kl:   85.968536
Epoch 000 | Time 0.0570(0.0571) | [  522/60000 (43%)] | Loss:   80.990929 |rec:  79.897163 | kl:  109.376816
Epoch 000 | Time 0.0570(0.0571) | [  542/60000 (45%)] | Loss:   83.351349 |rec:  82.369194 | kl:   98.214981
Epoch 000 | Time 0.0570(0.0571) | [  562/60000 (47%)] | Loss:   82.482109 |rec:  81.585884 | kl:   89.622147
Epoch 000 | Time 0.0570(0.0571) | [  582/60000 (48%)] | Loss:   89.244545 |rec:  88.255402 | kl:   98.915359
Epoch 000 | Time 0.0570(0.0571) | [  602/60000 (50%)] | Loss:   80.044197 |rec:  78.723953 | kl:  132.024506
Epoch 000 | Time 0.0570(0.0571) | [  622/60000 (52%)] | Loss:   78.608582 |rec:  77.373581 | kl:  123.500153
Epoch 000 | Time 0.0570(0.0570) | [  642/60000 (53%)] | Loss:   73.630394 |rec:  72.460457 | kl:  116.993690
Epoch 000 | Time 0.0570(0.0571) | [  662/60000 (55%)] | Loss:   75.884537 |rec:  74.824509 | kl:  106.002945
Epoch 000 | Time 0.0570(0.0570) | [  682/60000 (57%)] | Loss:   69.130745 |rec:  67.860962 | kl:  126.977821
Epoch 000 | Time 0.0570(0.0571) | [  702/60000 (58%)] | Loss:   68.804161 |rec:  67.788528 | kl:  101.562843
Epoch 000 | Time 0.0569(0.0570) | [  722/60000 (60%)] | Loss:   75.384636 |rec:  74.304390 | kl:  108.024620
Epoch 000 | Time 0.0570(0.0571) | [  742/60000 (62%)] | Loss:   66.702789 |rec:  65.511330 | kl:  119.146019
Epoch 000 | Time 0.0569(0.0570) | [  762/60000 (63%)] | Loss:   68.302223 |rec:  67.115959 | kl:  118.626678
Epoch 000 | Time 0.0569(0.0571) | [  782/60000 (65%)] | Loss:   69.781593 |rec:  68.864342 | kl:   91.724380
Epoch 000 | Time 0.0570(0.0570) | [  802/60000 (67%)] | Loss:   69.054199 |rec:  67.820587 | kl:  123.361748
Epoch 000 | Time 0.0570(0.0571) | [  822/60000 (68%)] | Loss:   66.760193 |rec:  65.530861 | kl:  122.932983
Epoch 000 | Time 0.0570(0.0570) | [  842/60000 (70%)] | Loss:   60.247669 |rec:  59.045788 | kl:  120.187851
Epoch 000 | Time 0.0569(0.0571) | [  862/60000 (72%)] | Loss:   60.015278 |rec:  58.919956 | kl:  109.532295
Epoch 000 | Time 0.0570(0.0570) | [  882/60000 (73%)] | Loss:   63.477383 |rec:  62.344051 | kl:  113.332764
Epoch 000 | Time 0.0569(0.0571) | [  902/60000 (75%)] | Loss:   57.521465 |rec:  56.386875 | kl:  113.459213
Epoch 000 | Time 0.0570(0.0571) | [  922/60000 (77%)] | Loss:   60.733002 |rec:  59.422852 | kl:  131.015091
Epoch 000 | Time 0.0570(0.0570) | [  942/60000 (78%)] | Loss:   55.066654 |rec:  53.809841 | kl:  125.681290
Epoch 000 | Time 0.0570(0.0571) | [  962/60000 (80%)] | Loss:   55.832424 |rec:  54.711636 | kl:  112.079018
Epoch 000 | Time 0.0570(0.0571) | [  982/60000 (82%)] | Loss:   54.198013 |rec:  52.903179 | kl:  129.483566
Epoch 000 | Time 0.0570(0.0571) | [ 1002/60000 (83%)] | Loss:   56.172817 |rec:  54.961529 | kl:  121.128700
Epoch 000 | Time 0.0570(0.0571) | [ 1022/60000 (85%)] | Loss:   57.113754 |rec:  55.922695 | kl:  119.105858
Epoch 000 | Time 0.0570(0.0571) | [ 1042/60000 (87%)] | Loss:   56.750980 |rec:  55.559189 | kl:  119.179199
Epoch 000 | Time 0.0570(0.0571) | [ 1062/60000 (88%)] | Loss:   58.123772 |rec:  57.023018 | kl:  110.075714
Epoch 000 | Time 0.0570(0.0571) | [ 1082/60000 (90%)] | Loss:   53.632183 |rec:  52.506401 | kl:  112.578354
Epoch 000 | Time 0.0570(0.0571) | [ 1102/60000 (92%)] | Loss:   53.152405 |rec:  51.863827 | kl:  128.858093
Epoch 000 | Time 0.0570(0.0571) | [ 1122/60000 (93%)] | Loss:   52.702850 |rec:  51.487114 | kl:  121.573837
Epoch 000 | Time 0.0570(0.0571) | [ 1142/60000 (95%)] | Loss:   51.183918 |rec:  49.975891 | kl:  120.802895
Epoch 000 | Time 0.0570(0.0571) | [ 1162/60000 (97%)] | Loss:   50.510925 |rec:  49.191185 | kl:  131.974075
Epoch 000 | Time 0.0570(0.0571) | [ 1182/60000 (98%)] | Loss:   51.847324 |rec:  50.638817 | kl:  120.850418
validating...
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3679(0.3679) | [    2/60000 ( 0%)] | Loss:  272.346252 |rec: 272.336060 | kl:    1.020911
Epoch 000 | Time 0.0570(0.2864) | [   22/60000 ( 2%)] | Loss:  245.890625 |rec: 245.879608 | kl:    1.102246
Epoch 000 | Time 0.0570(0.2262) | [   42/60000 ( 3%)] | Loss:  235.210876 |rec: 235.195908 | kl:    1.496213
Epoch 000 | Time 0.0575(0.1819) | [   62/60000 ( 5%)] | Loss:  184.616104 |rec: 183.910858 | kl:   70.525612
Epoch 000 | Time 0.0571(0.1492) | [   82/60000 ( 7%)] | Loss:  149.715866 |rec: 134.616608 | kl: 1509.926025
Epoch 000 | Time 0.0570(0.1250) | [  102/60000 ( 8%)] | Loss:  133.810760 |rec: 131.247665 | kl:  256.309601
Epoch 000 | Time 0.0568(0.1072) | [  122/60000 (10%)] | Loss:  119.487907 |rec: 117.315536 | kl:  217.237259
Epoch 000 | Time 0.0570(0.0941) | [  142/60000 (12%)] | Loss:  111.242233 |rec: 109.637192 | kl:  160.503983
Epoch 000 | Time 0.0572(0.0844) | [  162/60000 (13%)] | Loss:  102.558800 |rec: 101.196892 | kl:  136.190186
Epoch 000 | Time 0.0571(0.0774) | [  182/60000 (15%)] | Loss:  103.543739 |rec: 102.626465 | kl:   91.727615
Epoch 000 | Time 0.0572(0.0722) | [  202/60000 (17%)] | Loss:  106.391937 |rec: 105.496689 | kl:   89.525543
Epoch 000 | Time 0.0571(0.0683) | [  222/60000 (18%)] | Loss:  104.614326 |rec: 103.961685 | kl:   65.264030
Epoch 000 | Time 0.0572(0.0655) | [  242/60000 (20%)] | Loss:  102.805145 |rec: 102.214531 | kl:   59.061413
Epoch 000 | Time 0.0572(0.0635) | [  262/60000 (22%)] | Loss:   98.652733 |rec:  97.999794 | kl:   65.294189
Epoch 000 | Time 0.0573(0.0619) | [  282/60000 (23%)] | Loss:  100.570747 |rec:  99.784119 | kl:   78.662949
Epoch 000 | Time 0.0574(0.0608) | [  302/60000 (25%)] | Loss:  100.511002 |rec:  99.910172 | kl:   60.083305
Epoch 000 | Time 0.0574(0.0600) | [  322/60000 (27%)] | Loss:   91.303940 |rec:  90.418488 | kl:   88.546051
Epoch 000 | Time 0.0573(0.0593) | [  342/60000 (28%)] | Loss:   90.998344 |rec:  90.117676 | kl:   88.067139
Epoch 000 | Time 0.0573(0.0589) | [  362/60000 (30%)] | Loss:   90.556023 |rec:  89.835205 | kl:   72.081673
Epoch 000 | Time 0.0573(0.0586) | [  382/60000 (32%)] | Loss:   84.100685 |rec:  83.143654 | kl:   95.703613
Epoch 000 | Time 0.0573(0.0583) | [  402/60000 (33%)] | Loss:   86.219048 |rec:  85.196503 | kl:  102.255310
Epoch 000 | Time 0.0573(0.0581) | [  422/60000 (35%)] | Loss:   76.175789 |rec:  74.720566 | kl:  145.522491
Epoch 000 | Time 0.0572(0.0579) | [  442/60000 (37%)] | Loss:   79.705322 |rec:  78.618561 | kl:  108.675919
Epoch 000 | Time 0.0572(0.0578) | [  462/60000 (38%)] | Loss:   78.873787 |rec:  77.665756 | kl:  120.803307
Epoch 000 | Time 0.0572(0.0577) | [  482/60000 (40%)] | Loss:   76.440582 |rec:  75.113159 | kl:  132.742188
Epoch 000 | Time 0.0572(0.0576) | [  502/60000 (42%)] | Loss:   75.426895 |rec:  74.163696 | kl:  126.319633
Epoch 000 | Time 0.0571(0.0575) | [  522/60000 (43%)] | Loss:   74.921700 |rec:  73.765335 | kl:  115.636635
Epoch 000 | Time 0.0572(0.0575) | [  542/60000 (45%)] | Loss:   72.409210 |rec:  71.308716 | kl:  110.049362
Epoch 000 | Time 0.0572(0.0574) | [  562/60000 (47%)] | Loss:   69.556213 |rec:  68.242653 | kl:  131.356155
Epoch 000 | Time 0.0572(0.0574) | [  582/60000 (48%)] | Loss:   65.754768 |rec:  64.629822 | kl:  112.494606
Epoch 000 | Time 0.0572(0.0574) | [  602/60000 (50%)] | Loss:   69.022141 |rec:  67.880135 | kl:  114.200523
Epoch 000 | Time 0.0572(0.0574) | [  622/60000 (52%)] | Loss:   66.685883 |rec:  65.379326 | kl:  130.655930
Epoch 000 | Time 0.0572(0.0574) | [  642/60000 (53%)] | Loss:   64.561714 |rec:  63.237919 | kl:  132.379715
Epoch 000 | Time 0.0572(0.0573) | [  662/60000 (55%)] | Loss:   57.961662 |rec:  56.629910 | kl:  133.175079
Epoch 000 | Time 0.0572(0.0573) | [  682/60000 (57%)] | Loss:   60.756054 |rec:  59.578163 | kl:  117.789200
Epoch 000 | Time 0.0572(0.0573) | [  702/60000 (58%)] | Loss:   59.362846 |rec:  57.977131 | kl:  138.571396
Epoch 000 | Time 0.0572(0.0573) | [  722/60000 (60%)] | Loss:   61.027714 |rec:  59.748798 | kl:  127.891533
Epoch 000 | Time 0.0573(0.0573) | [  742/60000 (62%)] | Loss:   56.928879 |rec:  55.688114 | kl:  124.076469
Epoch 000 | Time 0.0572(0.0573) | [  762/60000 (63%)] | Loss:   53.228798 |rec:  51.854252 | kl:  137.454346
Epoch 000 | Time 0.0572(0.0573) | [  782/60000 (65%)] | Loss:   54.140560 |rec:  52.858181 | kl:  128.237701
Epoch 000 | Time 0.0573(0.0573) | [  802/60000 (67%)] | Loss:   50.908012 |rec:  49.683437 | kl:  122.457359
Epoch 000 | Time 0.0572(0.0573) | [  822/60000 (68%)] | Loss:   55.361523 |rec:  54.125355 | kl:  123.616661
Epoch 000 | Time 0.0571(0.0573) | [  842/60000 (70%)] | Loss:   53.155293 |rec:  51.834965 | kl:  132.032715
Epoch 000 | Time 0.0571(0.0573) | [  862/60000 (72%)] | Loss:   49.053082 |rec:  47.769180 | kl:  128.390228
Epoch 000 | Time 0.0572(0.0573) | [  882/60000 (73%)] | Loss:   53.317600 |rec:  52.031109 | kl:  128.649643
Epoch 000 | Time 0.0572(0.0573) | [  902/60000 (75%)] | Loss:   50.973362 |rec:  49.763466 | kl:  120.989647
Epoch 000 | Time 0.0572(0.0573) | [  922/60000 (77%)] | Loss:   50.417210 |rec:  48.964447 | kl:  145.276443
Epoch 000 | Time 0.0572(0.0573) | [  942/60000 (78%)] | Loss:   50.078663 |rec:  48.746494 | kl:  133.216721
Epoch 000 | Time 0.0572(0.0573) | [  962/60000 (80%)] | Loss:   49.358681 |rec:  47.994255 | kl:  136.442444
Epoch 000 | Time 0.0573(0.0573) | [  982/60000 (82%)] | Loss:   48.750416 |rec:  47.459377 | kl:  129.103745
Epoch 000 | Time 0.0572(0.0573) | [ 1002/60000 (83%)] | Loss:   45.831528 |rec:  44.641960 | kl:  118.956436
Epoch 000 | Time 0.0573(0.0574) | [ 1022/60000 (85%)] | Loss:   46.841259 |rec:  45.606293 | kl:  123.496513
Epoch 000 | Time 0.0571(0.0573) | [ 1042/60000 (87%)] | Loss:   47.410824 |rec:  46.248161 | kl:  116.265877
Epoch 000 | Time 0.0575(0.0574) | [ 1062/60000 (88%)] | Loss:   45.913090 |rec:  44.633007 | kl:  128.008530
Epoch 000 | Time 0.0575(0.0575) | [ 1082/60000 (90%)] | Loss:   47.406754 |rec:  46.179070 | kl:  122.768082
Epoch 000 | Time 0.0575(0.0575) | [ 1102/60000 (92%)] | Loss:   43.137417 |rec:  41.987656 | kl:  114.976196
Epoch 000 | Time 0.0574(0.0575) | [ 1122/60000 (93%)] | Loss:   49.268768 |rec:  48.010593 | kl:  125.817505
Epoch 000 | Time 0.0575(0.0576) | [ 1142/60000 (95%)] | Loss:   47.556103 |rec:  46.268490 | kl:  128.761383
Epoch 000 | Time 0.0575(0.0576) | [ 1162/60000 (97%)] | Loss:   43.331993 |rec:  42.123699 | kl:  120.829346
Epoch 000 | Time 0.0575(0.0576) | [ 1182/60000 (98%)] | Loss:   43.855606 |rec:  42.678715 | kl:  117.688805
validating...
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3707(0.3707) | [    2/60000 ( 0%)] | Loss:  292.172058 |rec: 292.162048 | kl:    0.999959
Epoch 000 | Time 0.0573(0.2885) | [   22/60000 ( 2%)] | Loss:  259.739471 |rec: 259.730194 | kl:    0.927836
Epoch 000 | Time 0.0573(0.2279) | [   42/60000 ( 3%)] | Loss:  209.794235 |rec: 207.611206 | kl:  218.302261
Epoch 000 | Time 0.0573(0.1831) | [   62/60000 ( 5%)] | Loss:  185.996323 |rec: 185.972382 | kl:    2.394090
Epoch 000 | Time 0.0575(0.1502) | [   82/60000 ( 7%)] | Loss:  165.154007 |rec: 165.117691 | kl:    3.630736
Epoch 000 | Time 0.0574(0.1259) | [  102/60000 ( 8%)] | Loss:  128.745834 |rec: 127.409561 | kl:  133.628342
Epoch 000 | Time 0.0573(0.1079) | [  122/60000 (10%)] | Loss:  113.184166 |rec: 112.314453 | kl:   86.971710
Epoch 000 | Time 0.0573(0.0948) | [  142/60000 (12%)] | Loss:  107.172447 |rec: 106.763008 | kl:   40.944527
Epoch 000 | Time 0.0573(0.0850) | [  162/60000 (13%)] | Loss:  107.772797 |rec: 107.399658 | kl:   37.314121
Epoch 000 | Time 0.0573(0.0778) | [  182/60000 (15%)] | Loss:  100.364609 |rec:  99.997849 | kl:   36.675842
Epoch 000 | Time 0.0573(0.0726) | [  202/60000 (17%)] | Loss:  103.853951 |rec: 103.537262 | kl:   31.669138
Epoch 000 | Time 0.0574(0.0687) | [  222/60000 (18%)] | Loss:   96.286552 |rec:  96.004852 | kl:   28.169556
Epoch 000 | Time 0.0573(0.0658) | [  242/60000 (20%)] | Loss:  101.106361 |rec: 100.835945 | kl:   27.041674
Epoch 000 | Time 0.0580(0.0637) | [  262/60000 (22%)] | Loss:  105.434616 |rec: 105.058533 | kl:   37.608143
Epoch 000 | Time 0.0576(0.0622) | [  282/60000 (23%)] | Loss:  100.549950 |rec: 100.218117 | kl:   33.183304
Epoch 000 | Time 0.0576(0.0610) | [  302/60000 (25%)] | Loss:  100.324921 |rec:  99.961716 | kl:   36.320740
Epoch 000 | Time 0.0593(0.0602) | [  322/60000 (27%)] | Loss:  101.152710 |rec: 100.711853 | kl:   44.085884
Epoch 000 | Time 0.0575(0.0596) | [  342/60000 (28%)] | Loss:  101.586113 |rec: 101.130035 | kl:   45.607128
Epoch 000 | Time 0.0576(0.0592) | [  362/60000 (30%)] | Loss:   95.455864 |rec:  94.879303 | kl:   57.656273
Epoch 000 | Time 0.0595(0.0589) | [  382/60000 (32%)] | Loss:   91.582535 |rec:  90.951309 | kl:   63.123398
Epoch 000 | Time 0.0576(0.0586) | [  402/60000 (33%)] | Loss:   83.808212 |rec:  83.214882 | kl:   59.333012
Epoch 000 | Time 0.0576(0.0584) | [  422/60000 (35%)] | Loss:   81.970673 |rec:  81.371040 | kl:   59.963024
Epoch 000 | Time 0.0576(0.0582) | [  442/60000 (37%)] | Loss:   87.687843 |rec:  87.010704 | kl:   67.714180
Epoch 000 | Time 0.0576(0.0581) | [  462/60000 (38%)] | Loss:   85.207138 |rec:  84.396454 | kl:   81.068481
Epoch 000 | Time 0.0577(0.0580) | [  482/60000 (40%)] | Loss:   77.450058 |rec:  76.584038 | kl:   86.602051
Epoch 000 | Time 0.0576(0.0579) | [  502/60000 (42%)] | Loss:   78.903801 |rec:  78.122963 | kl:   78.084137
Epoch 000 | Time 0.0576(0.0579) | [  522/60000 (43%)] | Loss:   73.324188 |rec:  72.429375 | kl:   89.481384
Epoch 000 | Time 0.0577(0.0578) | [  542/60000 (45%)] | Loss:   67.594856 |rec:  66.572189 | kl:  102.266747
Epoch 000 | Time 0.0576(0.0578) | [  562/60000 (47%)] | Loss:   72.739105 |rec:  71.858658 | kl:   88.044411
Epoch 000 | Time 0.0576(0.0578) | [  582/60000 (48%)] | Loss:   71.978340 |rec:  71.096970 | kl:   88.136734
Epoch 000 | Time 0.0576(0.0578) | [  602/60000 (50%)] | Loss:   66.538376 |rec:  65.549675 | kl:   98.869804
Epoch 000 | Time 0.0576(0.0577) | [  622/60000 (52%)] | Loss:   64.802628 |rec:  63.739727 | kl:  106.290703
Epoch 000 | Time 0.0577(0.0577) | [  642/60000 (53%)] | Loss:   65.267731 |rec:  64.366966 | kl:   90.076454
Epoch 000 | Time 0.0575(0.0577) | [  662/60000 (55%)] | Loss:   63.614872 |rec:  62.543358 | kl:  107.151505
Epoch 000 | Time 0.0576(0.0577) | [  682/60000 (57%)] | Loss:   60.861397 |rec:  59.718208 | kl:  114.318939
Epoch 000 | Time 0.0576(0.0577) | [  702/60000 (58%)] | Loss:   60.922127 |rec:  59.980179 | kl:   94.195068
Epoch 000 | Time 0.0576(0.0577) | [  722/60000 (60%)] | Loss:   59.528500 |rec:  58.421864 | kl:  110.663750
Epoch 000 | Time 0.0576(0.0577) | [  742/60000 (62%)] | Loss:   63.091805 |rec:  61.924011 | kl:  116.779381
Epoch 000 | Time 0.0576(0.0577) | [  762/60000 (63%)] | Loss:   58.606899 |rec:  57.546219 | kl:  106.068100
Epoch 000 | Time 0.0576(0.0577) | [  782/60000 (65%)] | Loss:   56.486835 |rec:  55.349228 | kl:  113.760902
Epoch 000 | Time 0.0576(0.0577) | [  802/60000 (67%)] | Loss:   57.877171 |rec:  56.795868 | kl:  108.130379
Epoch 000 | Time 0.0576(0.0577) | [  822/60000 (68%)] | Loss:   54.515522 |rec:  53.412464 | kl:  110.305435
Epoch 000 | Time 0.0577(0.0577) | [  842/60000 (70%)] | Loss:   53.734814 |rec:  52.599159 | kl:  113.565643
Epoch 000 | Time 0.0576(0.0577) | [  862/60000 (72%)] | Loss:   52.916397 |rec:  51.811180 | kl:  110.521400
Epoch 000 | Time 0.0576(0.0577) | [  882/60000 (73%)] | Loss:   50.113533 |rec:  48.923679 | kl:  118.985298
Epoch 000 | Time 0.0576(0.0577) | [  902/60000 (75%)] | Loss:   50.156425 |rec:  49.020134 | kl:  113.628838
Epoch 000 | Time 0.0576(0.0577) | [  922/60000 (77%)] | Loss:   47.211048 |rec:  46.082455 | kl:  112.859459
Epoch 000 | Time 0.0577(0.0577) | [  942/60000 (78%)] | Loss:   51.643867 |rec:  50.553085 | kl:  109.078178
Epoch 000 | Time 0.0576(0.0577) | [  962/60000 (80%)] | Loss:   51.326912 |rec:  50.255562 | kl:  107.135498
Epoch 000 | Time 0.0576(0.0577) | [  982/60000 (82%)] | Loss:   48.322346 |rec:  47.279663 | kl:  104.268700
Epoch 000 | Time 0.0576(0.0577) | [ 1002/60000 (83%)] | Loss:   49.120972 |rec:  47.980698 | kl:  114.027306
Epoch 000 | Time 0.0577(0.0577) | [ 1022/60000 (85%)] | Loss:   48.109077 |rec:  47.036396 | kl:  107.268280
Epoch 000 | Time 0.0576(0.0577) | [ 1042/60000 (87%)] | Loss:   48.787376 |rec:  47.740395 | kl:  104.698380
Epoch 000 | Time 0.0576(0.0577) | [ 1062/60000 (88%)] | Loss:   47.216709 |rec:  46.130131 | kl:  108.657646
Epoch 000 | Time 0.0576(0.0577) | [ 1082/60000 (90%)] | Loss:   46.199986 |rec:  45.115028 | kl:  108.495689
Epoch 000 | Time 0.0577(0.0577) | [ 1102/60000 (92%)] | Loss:   44.792206 |rec:  43.787701 | kl:  100.450523
Epoch 000 | Time 0.0577(0.0577) | [ 1122/60000 (93%)] | Loss:   47.783730 |rec:  46.749260 | kl:  103.446922
Epoch 000 | Time 0.0576(0.0577) | [ 1142/60000 (95%)] | Loss:   46.642776 |rec:  45.648891 | kl:   99.388435
Epoch 000 | Time 0.0576(0.0577) | [ 1162/60000 (97%)] | Loss:   42.023621 |rec:  41.013165 | kl:  101.046074
Epoch 000 | Time 0.0576(0.0577) | [ 1182/60000 (98%)] | Loss:   44.484745 |rec:  43.519466 | kl:   96.527954
validating...
Epoch 0000 | Time 2.2206 | Loss 43.5878
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3828(0.3828) | [    2/60000 ( 0%)] | Loss:  277.222504 |rec: 277.210876 | kl:    1.161929
Epoch 000 | Time 0.0612(0.2979) | [   22/60000 ( 2%)] | Loss:  246.624115 |rec: 246.613358 | kl:    1.075261
Epoch 000 | Time 0.0573(0.2348) | [   42/60000 ( 3%)] | Loss:  199.608002 |rec: 199.568848 | kl:    3.915957
Epoch 000 | Time 0.0573(0.1884) | [   62/60000 ( 5%)] | Loss:  184.536865 |rec: 184.524139 | kl:    1.272808
Epoch 000 | Time 0.0591(0.1546) | [   82/60000 ( 7%)] | Loss:  171.156113 |rec: 171.131577 | kl:    2.453298
Epoch 000 | Time 0.0574(0.1291) | [  102/60000 ( 8%)] | Loss:  155.433105 |rec: 154.730072 | kl:   70.304703
Epoch 000 | Time 0.0573(0.1103) | [  122/60000 (10%)] | Loss:  165.464249 |rec: 164.287888 | kl:  117.636971
Epoch 000 | Time 0.0572(0.0964) | [  142/60000 (12%)] | Loss:  158.545197 |rec: 158.514191 | kl:    3.099771
Epoch 000 | Time 0.0573(0.0861) | [  162/60000 (13%)] | Loss:  130.988052 |rec: 130.620499 | kl:   36.756329
Epoch 000 | Time 0.0572(0.0786) | [  182/60000 (15%)] | Loss:  115.006805 |rec: 114.577240 | kl:   42.956409
Epoch 000 | Time 0.0572(0.0730) | [  202/60000 (17%)] | Loss:  105.567223 |rec: 105.227745 | kl:   33.948467
Epoch 000 | Time 0.0573(0.0689) | [  222/60000 (18%)] | Loss:   99.596092 |rec:  99.309990 | kl:   28.610346
Epoch 000 | Time 0.0573(0.0659) | [  242/60000 (20%)] | Loss:  104.228806 |rec: 103.915367 | kl:   31.343481
Epoch 000 | Time 0.0573(0.0636) | [  262/60000 (22%)] | Loss:  101.790848 |rec: 101.535995 | kl:   25.485344
Epoch 000 | Time 0.0573(0.0620) | [  282/60000 (23%)] | Loss:  103.100357 |rec: 102.768669 | kl:   33.168526
Epoch 000 | Time 0.0572(0.0608) | [  302/60000 (25%)] | Loss:  100.921371 |rec: 100.621109 | kl:   30.025961
Epoch 000 | Time 0.0572(0.0599) | [  322/60000 (27%)] | Loss:   95.076782 |rec:  94.771355 | kl:   30.543163
Epoch 000 | Time 0.0573(0.0592) | [  342/60000 (28%)] | Loss:   96.830994 |rec:  96.449615 | kl:   38.137352
Epoch 000 | Time 0.0574(0.0587) | [  362/60000 (30%)] | Loss:   95.289558 |rec:  94.894547 | kl:   39.501087
Epoch 000 | Time 0.0572(0.0583) | [  382/60000 (32%)] | Loss:   90.368629 |rec:  89.810997 | kl:   55.763321
Epoch 000 | Time 0.0573(0.0581) | [  402/60000 (33%)] | Loss:   87.530830 |rec:  86.828064 | kl:   70.276276
Epoch 000 | Time 0.0572(0.0579) | [  422/60000 (35%)] | Loss:   90.881714 |rec:  90.431778 | kl:   44.994320
Epoch 000 | Time 0.0572(0.0578) | [  442/60000 (37%)] | Loss:   87.305260 |rec:  86.536232 | kl:   76.903214
Epoch 000 | Time 0.0572(0.0577) | [  462/60000 (38%)] | Loss:   87.046944 |rec:  86.311577 | kl:   73.535675
Epoch 000 | Time 0.0572(0.0576) | [  482/60000 (40%)] | Loss:   75.619560 |rec:  74.817596 | kl:   80.196228
Epoch 000 | Time 0.0572(0.0576) | [  502/60000 (42%)] | Loss:   77.975166 |rec:  77.255516 | kl:   71.964958
Epoch 000 | Time 0.0573(0.0575) | [  522/60000 (43%)] | Loss:   76.068916 |rec:  74.986229 | kl:  108.268524
Epoch 000 | Time 0.0572(0.0575) | [  542/60000 (45%)] | Loss:   73.950821 |rec:  72.953400 | kl:   99.742752
Epoch 000 | Time 0.0573(0.0575) | [  562/60000 (47%)] | Loss:   73.273163 |rec:  72.365494 | kl:   90.766647
Epoch 000 | Time 0.0573(0.0575) | [  582/60000 (48%)] | Loss:   70.833405 |rec:  69.936172 | kl:   89.722816
Epoch 000 | Time 0.0573(0.0575) | [  602/60000 (50%)] | Loss:   73.288864 |rec:  72.472084 | kl:   81.678070
Epoch 000 | Time 0.0572(0.0574) | [  622/60000 (52%)] | Loss:   71.081551 |rec:  69.962196 | kl:  111.935318
Epoch 000 | Time 0.0573(0.0574) | [  642/60000 (53%)] | Loss:   68.694687 |rec:  67.691391 | kl:  100.329605
Epoch 000 | Time 0.0572(0.0574) | [  662/60000 (55%)] | Loss:   65.900581 |rec:  64.903008 | kl:   99.757599
Epoch 000 | Time 0.0572(0.0574) | [  682/60000 (57%)] | Loss:   64.846481 |rec:  63.755394 | kl:  109.108299
Epoch 000 | Time 0.0574(0.0574) | [  702/60000 (58%)] | Loss:   60.693672 |rec:  59.661415 | kl:  103.225616
Epoch 000 | Time 0.0575(0.0575) | [  722/60000 (60%)] | Loss:   62.427837 |rec:  61.284149 | kl:  114.368759
Epoch 000 | Time 0.0577(0.0575) | [  742/60000 (62%)] | Loss:   61.154701 |rec:  59.917400 | kl:  123.729744
Epoch 000 | Time 0.0575(0.0576) | [  762/60000 (63%)] | Loss:   57.498577 |rec:  56.403744 | kl:  109.483383
Epoch 000 | Time 0.0576(0.0576) | [  782/60000 (65%)] | Loss:   55.795555 |rec:  54.537102 | kl:  125.845146
Epoch 000 | Time 0.0576(0.0576) | [  802/60000 (67%)] | Loss:   55.145290 |rec:  54.153435 | kl:   99.185509
Epoch 000 | Time 0.0576(0.0576) | [  822/60000 (68%)] | Loss:   57.382072 |rec:  56.080341 | kl:  130.173233
Epoch 000 | Time 0.0576(0.0576) | [  842/60000 (70%)] | Loss:   53.772419 |rec:  52.776726 | kl:   99.569580
Epoch 000 | Time 0.0575(0.0576) | [  862/60000 (72%)] | Loss:   50.528690 |rec:  49.264355 | kl:  126.433746
Epoch 000 | Time 0.0576(0.0576) | [  882/60000 (73%)] | Loss:   52.847240 |rec:  51.727695 | kl:  111.954468
Epoch 000 | Time 0.0576(0.0576) | [  902/60000 (75%)] | Loss:   51.456303 |rec:  50.288162 | kl:  116.813797
Epoch 000 | Time 0.0576(0.0577) | [  922/60000 (77%)] | Loss:   52.180096 |rec:  51.172806 | kl:  100.728790
Epoch 000 | Time 0.0575(0.0577) | [  942/60000 (78%)] | Loss:   51.935169 |rec:  50.839535 | kl:  109.563652
Epoch 000 | Time 0.0575(0.0576) | [  962/60000 (80%)] | Loss:   50.392517 |rec:  49.161678 | kl:  123.083855
Epoch 000 | Time 0.0576(0.0576) | [  982/60000 (82%)] | Loss:   47.001228 |rec:  45.893826 | kl:  110.740356
Epoch 000 | Time 0.0576(0.0577) | [ 1002/60000 (83%)] | Loss:   48.061840 |rec:  47.011257 | kl:  105.057877
Epoch 000 | Time 0.0575(0.0577) | [ 1022/60000 (85%)] | Loss:   45.167915 |rec:  44.109524 | kl:  105.838882
Epoch 000 | Time 0.0576(0.0577) | [ 1042/60000 (87%)] | Loss:   47.650959 |rec:  46.553852 | kl:  109.710846
Epoch 000 | Time 0.0575(0.0577) | [ 1062/60000 (88%)] | Loss:   47.739624 |rec:  46.689014 | kl:  105.061111
Epoch 000 | Time 0.0576(0.0577) | [ 1082/60000 (90%)] | Loss:   45.273926 |rec:  44.174332 | kl:  109.959274
Epoch 000 | Time 0.0576(0.0577) | [ 1102/60000 (92%)] | Loss:   46.068344 |rec:  45.010147 | kl:  105.819595
Epoch 000 | Time 0.0576(0.0577) | [ 1122/60000 (93%)] | Loss:   44.843899 |rec:  43.795742 | kl:  104.816063
Epoch 000 | Time 0.0576(0.0577) | [ 1142/60000 (95%)] | Loss:   45.549610 |rec:  44.462646 | kl:  108.696335
Epoch 000 | Time 0.0576(0.0577) | [ 1162/60000 (97%)] | Loss:   45.183147 |rec:  44.115009 | kl:  106.813843
Epoch 000 | Time 0.0577(0.0577) | [ 1182/60000 (98%)] | Loss:   43.083347 |rec:  42.049587 | kl:  103.376167
validating...
Epoch 0000 | Time 2.2499 | Loss 44.4666
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3712(0.3712) | [    2/60000 ( 0%)] | Loss:  292.754456 |rec: 292.743286 | kl:    1.119990
Epoch 000 | Time 0.0567(0.2888) | [   22/60000 ( 2%)] | Loss:  214.092209 |rec: 214.076797 | kl:    1.541542
Epoch 000 | Time 0.0566(0.2279) | [   42/60000 ( 3%)] | Loss:  222.361572 |rec: 221.933472 | kl:   42.811512
Epoch 000 | Time 0.0566(0.1830) | [   62/60000 ( 5%)] | Loss:  143.771515 |rec: 143.357574 | kl:   41.393139
Epoch 000 | Time 0.0567(0.1499) | [   82/60000 ( 7%)] | Loss:  155.338089 |rec: 155.281693 | kl:    5.640591
Epoch 000 | Time 0.0567(0.1255) | [  102/60000 ( 8%)] | Loss:  117.086861 |rec: 114.243690 | kl:  284.317261
Epoch 000 | Time 0.0566(0.1075) | [  122/60000 (10%)] | Loss:  105.014725 |rec: 103.354713 | kl:  166.000763
Epoch 000 | Time 0.0566(0.0942) | [  142/60000 (12%)] | Loss:  103.508842 |rec: 102.523483 | kl:   98.536148
Epoch 000 | Time 0.0570(0.0845) | [  162/60000 (13%)] | Loss:  100.952774 |rec: 100.242638 | kl:   71.013214
Epoch 000 | Time 0.0569(0.0773) | [  182/60000 (15%)] | Loss:  100.037666 |rec:  99.485855 | kl:   55.180859
Epoch 000 | Time 0.0587(0.0721) | [  202/60000 (17%)] | Loss:   98.981850 |rec:  98.490486 | kl:   49.136635
Epoch 000 | Time 0.0570(0.0682) | [  222/60000 (18%)] | Loss:   95.922729 |rec:  95.456703 | kl:   46.602940
Epoch 000 | Time 0.0570(0.0653) | [  242/60000 (20%)] | Loss:   99.584038 |rec:  99.162697 | kl:   42.135067
Epoch 000 | Time 0.0584(0.0633) | [  262/60000 (22%)] | Loss:   99.777489 |rec:  99.347618 | kl:   42.987732
Epoch 000 | Time 0.0569(0.0617) | [  282/60000 (23%)] | Loss:   99.481270 |rec:  99.063072 | kl:   41.818855
Epoch 000 | Time 0.0570(0.0605) | [  302/60000 (25%)] | Loss:  101.491196 |rec: 101.072105 | kl:   41.909264
Epoch 000 | Time 0.0571(0.0596) | [  322/60000 (27%)] | Loss:   91.550636 |rec:  90.685371 | kl:   86.526695
Epoch 000 | Time 0.0569(0.0591) | [  342/60000 (28%)] | Loss:   87.653336 |rec:  86.940437 | kl:   71.289902
Epoch 000 | Time 0.0569(0.0586) | [  362/60000 (30%)] | Loss:   83.217232 |rec:  82.423180 | kl:   79.405586
Epoch 000 | Time 0.0569(0.0582) | [  382/60000 (32%)] | Loss:   88.657753 |rec:  87.471031 | kl:  118.671944
Epoch 000 | Time 0.0570(0.0580) | [  402/60000 (33%)] | Loss:   82.066422 |rec:  81.067947 | kl:   99.847237
Epoch 000 | Time 0.0570(0.0577) | [  422/60000 (35%)] | Loss:   81.715126 |rec:  80.702705 | kl:  101.242409
Epoch 000 | Time 0.0570(0.0575) | [  442/60000 (37%)] | Loss:   77.200111 |rec:  76.285088 | kl:   91.502144
Epoch 000 | Time 0.0570(0.0574) | [  462/60000 (38%)] | Loss:   77.183060 |rec:  76.129250 | kl:  105.381264
Epoch 000 | Time 0.0570(0.0573) | [  482/60000 (40%)] | Loss:   73.953728 |rec:  73.006073 | kl:   94.765312
Epoch 000 | Time 0.0569(0.0573) | [  502/60000 (42%)] | Loss:   69.866547 |rec:  68.646378 | kl:  122.017090
Epoch 000 | Time 0.0569(0.0572) | [  522/60000 (43%)] | Loss:   68.712204 |rec:  67.824295 | kl:   88.790611
Epoch 000 | Time 0.0570(0.0572) | [  542/60000 (45%)] | Loss:   69.979424 |rec:  68.857460 | kl:  112.196953
Epoch 000 | Time 0.0570(0.0572) | [  562/60000 (47%)] | Loss:   71.715355 |rec:  70.675781 | kl:  103.957695
Epoch 000 | Time 0.0570(0.0571) | [  582/60000 (48%)] | Loss:   64.641129 |rec:  63.496853 | kl:  114.427246
Epoch 000 | Time 0.0569(0.0571) | [  602/60000 (50%)] | Loss:   67.845146 |rec:  66.842232 | kl:  100.291122
Epoch 000 | Time 0.0569(0.0571) | [  622/60000 (52%)] | Loss:   57.981567 |rec:  56.651020 | kl:  133.054504
Epoch 000 | Time 0.0570(0.0571) | [  642/60000 (53%)] | Loss:   61.773907 |rec:  60.670361 | kl:  110.354248
Epoch 000 | Time 0.0569(0.0571) | [  662/60000 (55%)] | Loss:   58.746773 |rec:  57.628792 | kl:  111.797729
Epoch 000 | Time 0.0569(0.0571) | [  682/60000 (57%)] | Loss:   58.827885 |rec:  57.673443 | kl:  115.444138
Epoch 000 | Time 0.0570(0.0571) | [  702/60000 (58%)] | Loss:   61.728962 |rec:  60.560722 | kl:  116.824440
Epoch 000 | Time 0.0570(0.0571) | [  722/60000 (60%)] | Loss:   55.094570 |rec:  53.945019 | kl:  114.954865
Epoch 000 | Time 0.0569(0.0571) | [  742/60000 (62%)] | Loss:   57.699036 |rec:  56.582817 | kl:  111.622101
Epoch 000 | Time 0.0569(0.0571) | [  762/60000 (63%)] | Loss:   55.064037 |rec:  53.928581 | kl:  113.545380
Epoch 000 | Time 0.0570(0.0571) | [  782/60000 (65%)] | Loss:   54.732410 |rec:  53.602806 | kl:  112.960541
Epoch 000 | Time 0.0570(0.0571) | [  802/60000 (67%)] | Loss:   52.877319 |rec:  51.718739 | kl:  115.858055
Epoch 000 | Time 0.0572(0.0571) | [  822/60000 (68%)] | Loss:   52.078392 |rec:  50.893997 | kl:  118.439400
Epoch 000 | Time 0.0572(0.0572) | [  842/60000 (70%)] | Loss:   54.086029 |rec:  52.939198 | kl:  114.682945
Epoch 000 | Time 0.0572(0.0572) | [  862/60000 (72%)] | Loss:   53.689964 |rec:  52.520023 | kl:  116.994370
Epoch 000 | Time 0.0571(0.0572) | [  882/60000 (73%)] | Loss:   55.761040 |rec:  54.654057 | kl:  110.698425
Epoch 000 | Time 0.0571(0.0572) | [  902/60000 (75%)] | Loss:   51.065655 |rec:  49.830044 | kl:  123.561172
Epoch 000 | Time 0.0571(0.0572) | [  922/60000 (77%)] | Loss:   53.623909 |rec:  52.460068 | kl:  116.384102
Epoch 000 | Time 0.0572(0.0573) | [  942/60000 (78%)] | Loss:   50.425926 |rec:  49.431702 | kl:   99.422150
Epoch 000 | Time 0.0571(0.0573) | [  962/60000 (80%)] | Loss:   48.475117 |rec:  47.400337 | kl:  107.478027
Epoch 000 | Time 0.0572(0.0573) | [  982/60000 (82%)] | Loss:   51.398872 |rec:  50.328953 | kl:  106.991844
Epoch 000 | Time 0.0571(0.0573) | [ 1002/60000 (83%)] | Loss:   47.379623 |rec:  46.329964 | kl:  104.965591
Epoch 000 | Time 0.0572(0.0573) | [ 1022/60000 (85%)] | Loss:   49.624012 |rec:  48.446587 | kl:  117.742920
Epoch 000 | Time 0.0572(0.0573) | [ 1042/60000 (87%)] | Loss:   48.770393 |rec:  47.738907 | kl:  103.148796
Epoch 000 | Time 0.0572(0.0573) | [ 1062/60000 (88%)] | Loss:   48.980942 |rec:  47.915249 | kl:  106.569138
Epoch 000 | Time 0.0573(0.0573) | [ 1082/60000 (90%)] | Loss:   49.083916 |rec:  47.968311 | kl:  111.560631
Epoch 000 | Time 0.0572(0.0573) | [ 1102/60000 (92%)] | Loss:   47.834900 |rec:  46.815968 | kl:  101.893570
Epoch 000 | Time 0.0572(0.0573) | [ 1122/60000 (93%)] | Loss:   50.132359 |rec:  49.131462 | kl:  100.089264
Epoch 000 | Time 0.0570(0.0573) | [ 1142/60000 (95%)] | Loss:   46.236210 |rec:  45.181004 | kl:  105.520561
Epoch 000 | Time 0.0571(0.0573) | [ 1162/60000 (97%)] | Loss:   47.179096 |rec:  46.134998 | kl:  104.409500
Epoch 000 | Time 0.0572(0.0573) | [ 1182/60000 (98%)] | Loss:   45.894703 |rec:  44.814289 | kl:  108.040894
validating...
Epoch 0000 | Time 2.2295 | Loss 45.7093
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3675(0.3675) | [    2/60000 ( 0%)] | Loss:  283.776215 |rec: 283.767059 | kl:    0.914274
Epoch 000 | Time 0.0568(0.2861) | [   22/60000 ( 2%)] | Loss:  237.405930 |rec: 237.396912 | kl:    0.903229
Epoch 000 | Time 0.0568(0.2259) | [   42/60000 ( 3%)] | Loss:  174.278976 |rec: 174.249817 | kl:    2.916363
Epoch 000 | Time 0.0575(0.1816) | [   62/60000 ( 5%)] | Loss:  163.754395 |rec: 163.742233 | kl:    1.215344
Epoch 000 | Time 0.0569(0.1490) | [   82/60000 ( 7%)] | Loss:  150.690796 |rec: 150.647385 | kl:    4.340758
Epoch 000 | Time 0.0569(0.1249) | [  102/60000 ( 8%)] | Loss:  140.771423 |rec: 135.279861 | kl:  549.156128
Epoch 000 | Time 0.0568(0.1071) | [  122/60000 (10%)] | Loss:  118.843521 |rec: 117.960052 | kl:   88.346642
Epoch 000 | Time 0.0569(0.0940) | [  142/60000 (12%)] | Loss:  105.926598 |rec: 105.340057 | kl:   58.653900
Epoch 000 | Time 0.0568(0.0843) | [  162/60000 (13%)] | Loss:  109.735146 |rec: 109.359077 | kl:   37.606598
Epoch 000 | Time 0.0569(0.0772) | [  182/60000 (15%)] | Loss:  106.415947 |rec: 105.886620 | kl:   52.932484
Epoch 000 | Time 0.0569(0.0719) | [  202/60000 (17%)] | Loss:  105.463150 |rec: 104.992165 | kl:   47.098633
Epoch 000 | Time 0.0573(0.0681) | [  222/60000 (18%)] | Loss:  104.435272 |rec: 103.891464 | kl:   54.380760
Epoch 000 | Time 0.0573(0.0653) | [  242/60000 (20%)] | Loss:   99.382668 |rec:  98.946358 | kl:   43.630898
Epoch 000 | Time 0.0571(0.0632) | [  262/60000 (22%)] | Loss:  100.703735 |rec: 100.367188 | kl:   33.654831
Epoch 000 | Time 0.0572(0.0617) | [  282/60000 (23%)] | Loss:  100.506729 |rec: 100.211754 | kl:   29.497463
Epoch 000 | Time 0.0571(0.0606) | [  302/60000 (25%)] | Loss:   97.736374 |rec:  97.421394 | kl:   31.497580
Epoch 000 | Time 0.0572(0.0598) | [  322/60000 (27%)] | Loss:   95.122536 |rec:  94.834969 | kl:   28.756977
Epoch 000 | Time 0.0572(0.0592) | [  342/60000 (28%)] | Loss:   89.104118 |rec:  88.508621 | kl:   59.550053
Epoch 000 | Time 0.0572(0.0588) | [  362/60000 (30%)] | Loss:   91.933632 |rec:  91.362556 | kl:   57.107372
Epoch 000 | Time 0.0572(0.0585) | [  382/60000 (32%)] | Loss:   88.474487 |rec:  87.852371 | kl:   62.211678
Epoch 000 | Time 0.0572(0.0582) | [  402/60000 (33%)] | Loss:   83.263504 |rec:  82.519432 | kl:   74.407249
Epoch 000 | Time 0.0572(0.0580) | [  422/60000 (35%)] | Loss:   77.704552 |rec:  77.080811 | kl:   62.374184
Epoch 000 | Time 0.0572(0.0578) | [  442/60000 (37%)] | Loss:   77.340843 |rec:  76.423203 | kl:   91.763535
Epoch 000 | Time 0.0573(0.0577) | [  462/60000 (38%)] | Loss:   76.802811 |rec:  76.003036 | kl:   79.977364
Epoch 000 | Time 0.0573(0.0576) | [  482/60000 (40%)] | Loss:   74.839119 |rec:  73.760651 | kl:  107.846832
Epoch 000 | Time 0.0572(0.0575) | [  502/60000 (42%)] | Loss:   63.392879 |rec:  62.388290 | kl:  100.458923
Epoch 000 | Time 0.0572(0.0575) | [  522/60000 (43%)] | Loss:   68.884933 |rec:  68.036209 | kl:   84.872711
Epoch 000 | Time 0.0572(0.0574) | [  542/60000 (45%)] | Loss:   65.447037 |rec:  64.532753 | kl:   91.428726
Epoch 000 | Time 0.0572(0.0574) | [  562/60000 (47%)] | Loss:   66.720284 |rec:  65.599113 | kl:  112.117401
Epoch 000 | Time 0.0572(0.0574) | [  582/60000 (48%)] | Loss:   62.417084 |rec:  61.502251 | kl:   91.483383
Epoch 000 | Time 0.0572(0.0574) | [  602/60000 (50%)] | Loss:   57.747246 |rec:  56.815243 | kl:   93.200249
Epoch 000 | Time 0.0572(0.0573) | [  622/60000 (52%)] | Loss:   62.165852 |rec:  61.196526 | kl:   96.932404
Epoch 000 | Time 0.0572(0.0573) | [  642/60000 (53%)] | Loss:   60.261829 |rec:  59.245632 | kl:  101.619492
Epoch 000 | Time 0.0571(0.0573) | [  662/60000 (55%)] | Loss:   57.756126 |rec:  56.722595 | kl:  103.352791
Epoch 000 | Time 0.0572(0.0573) | [  682/60000 (57%)] | Loss:   56.815796 |rec:  55.772713 | kl:  104.308197
Epoch 000 | Time 0.0573(0.0573) | [  702/60000 (58%)] | Loss:   54.112244 |rec:  53.054306 | kl:  105.794098
Epoch 000 | Time 0.0572(0.0573) | [  722/60000 (60%)] | Loss:   54.369183 |rec:  53.269470 | kl:  109.971329
Epoch 000 | Time 0.0572(0.0573) | [  742/60000 (62%)] | Loss:   56.782112 |rec:  55.717659 | kl:  106.445496
Epoch 000 | Time 0.0572(0.0573) | [  762/60000 (63%)] | Loss:   52.670101 |rec:  51.645515 | kl:  102.458641
Epoch 000 | Time 0.0572(0.0573) | [  782/60000 (65%)] | Loss:   51.540329 |rec:  50.453140 | kl:  108.719147
Epoch 000 | Time 0.0572(0.0573) | [  802/60000 (67%)] | Loss:   50.760159 |rec:  49.684605 | kl:  107.555435
Epoch 000 | Time 0.0572(0.0573) | [  822/60000 (68%)] | Loss:   49.119862 |rec:  48.147205 | kl:   97.265762
Epoch 000 | Time 0.0572(0.0573) | [  842/60000 (70%)] | Loss:   48.905693 |rec:  47.869354 | kl:  103.633774
Epoch 000 | Time 0.0572(0.0573) | [  862/60000 (72%)] | Loss:   49.828442 |rec:  48.833199 | kl:   99.524521
Epoch 000 | Time 0.0572(0.0573) | [  882/60000 (73%)] | Loss:   50.212387 |rec:  49.068432 | kl:  114.395386
Epoch 000 | Time 0.0572(0.0573) | [  902/60000 (75%)] | Loss:   49.069828 |rec:  48.076077 | kl:   99.375221
Epoch 000 | Time 0.0572(0.0573) | [  922/60000 (77%)] | Loss:   48.686928 |rec:  47.629261 | kl:  105.766426
Epoch 000 | Time 0.0572(0.0573) | [  942/60000 (78%)] | Loss:   46.902588 |rec:  45.861767 | kl:  104.081978
Epoch 000 | Time 0.0573(0.0573) | [  962/60000 (80%)] | Loss:   47.945927 |rec:  46.897530 | kl:  104.839958
Epoch 000 | Time 0.0573(0.0573) | [  982/60000 (82%)] | Loss:   49.379486 |rec:  48.424046 | kl:   95.543816
Epoch 000 | Time 0.0571(0.0573) | [ 1002/60000 (83%)] | Loss:   47.092361 |rec:  46.065449 | kl:  102.691490
Epoch 000 | Time 0.0572(0.0573) | [ 1022/60000 (85%)] | Loss:   46.566372 |rec:  45.629211 | kl:   93.715973
Epoch 000 | Time 0.0572(0.0573) | [ 1042/60000 (87%)] | Loss:   45.919632 |rec:  44.933575 | kl:   98.606041
Epoch 000 | Time 0.0572(0.0573) | [ 1062/60000 (88%)] | Loss:   46.438763 |rec:  45.423695 | kl:  101.506935
Epoch 000 | Time 0.0575(0.0574) | [ 1082/60000 (90%)] | Loss:   43.984016 |rec:  43.010189 | kl:   97.382843
Epoch 000 | Time 0.0575(0.0574) | [ 1102/60000 (92%)] | Loss:   44.104927 |rec:  43.118992 | kl:   98.593437
Epoch 000 | Time 0.0575(0.0574) | [ 1122/60000 (93%)] | Loss:   44.549706 |rec:  43.557816 | kl:   99.188858
Epoch 000 | Time 0.0575(0.0575) | [ 1142/60000 (95%)] | Loss:   43.044067 |rec:  42.073875 | kl:   97.018814
Epoch 000 | Time 0.0574(0.0575) | [ 1162/60000 (97%)] | Loss:   41.414734 |rec:  40.434292 | kl:   98.043999
Epoch 000 | Time 0.0574(0.0575) | [ 1182/60000 (98%)] | Loss:   42.286438 |rec:  41.313709 | kl:   97.273125
validating...
Epoch 0000 | Time 2.1966 | Loss 43.1051
Epoch: 1 	Beta: 0.02
Epoch 001 | Time 0.0589(0.0576) | [    2/60000 ( 0%)] | Loss:   44.191391 |rec:  42.106083 | kl:  104.265411
Epoch 001 | Time 0.0577(0.0577) | [   22/60000 ( 2%)] | Loss:   41.675587 |rec:  40.035015 | kl:   82.028526
Epoch 001 | Time 0.0576(0.0577) | [   42/60000 ( 3%)] | Loss:   42.184326 |rec:  40.682251 | kl:   75.103661
Epoch 001 | Time 0.0574(0.0576) | [   62/60000 ( 5%)] | Loss:   42.861607 |rec:  41.340668 | kl:   76.046898
Epoch 001 | Time 0.0590(0.0577) | [   82/60000 ( 7%)] | Loss:   40.502823 |rec:  39.081593 | kl:   71.061508
Epoch 001 | Time 0.0575(0.0579) | [  102/60000 ( 8%)] | Loss:   43.410217 |rec:  41.995781 | kl:   70.721954
Epoch 001 | Time 0.0575(0.0582) | [  122/60000 (10%)] | Loss:   42.519985 |rec:  41.167385 | kl:   67.629913
Epoch 001 | Time 0.0575(0.0583) | [  142/60000 (12%)] | Loss:   43.620522 |rec:  42.145454 | kl:   73.753304
Epoch 001 | Time 0.0587(0.0582) | [  162/60000 (13%)] | Loss:   43.240932 |rec:  41.838528 | kl:   70.120125
Epoch 001 | Time 0.0576(0.0588) | [  182/60000 (15%)] | Loss:   40.737865 |rec:  39.382637 | kl:   67.761414
Epoch 001 | Time 0.0624(0.0587) | [  202/60000 (17%)] | Loss:   42.370369 |rec:  41.062534 | kl:   65.391876
Epoch 001 | Time 0.0574(0.0586) | [  222/60000 (18%)] | Loss:   40.439075 |rec:  39.147434 | kl:   64.581940
Epoch 001 | Time 0.0574(0.0586) | [  242/60000 (20%)] | Loss:   41.904736 |rec:  40.588917 | kl:   65.790825
Epoch 001 | Time 0.0577(0.0588) | [  262/60000 (22%)] | Loss:   41.892937 |rec:  40.611774 | kl:   64.058189
Epoch 001 | Time 0.0594(0.0586) | [  282/60000 (23%)] | Loss:   41.685081 |rec:  40.365257 | kl:   65.991264
Epoch 001 | Time 0.0587(0.0584) | [  302/60000 (25%)] | Loss:   40.412395 |rec:  39.032097 | kl:   69.014885
Epoch 001 | Time 0.0575(0.0584) | [  322/60000 (27%)] | Loss:   39.407063 |rec:  38.116295 | kl:   64.538330
Epoch 001 | Time 0.0579(0.0583) | [  342/60000 (28%)] | Loss:   41.022465 |rec:  39.728916 | kl:   64.677521
Epoch 001 | Time 0.0575(0.0581) | [  362/60000 (30%)] | Loss:   38.485195 |rec:  37.200314 | kl:   64.244011
Epoch 001 | Time 0.0575(0.0580) | [  382/60000 (32%)] | Loss:   38.735153 |rec:  37.509037 | kl:   61.305813
Epoch 001 | Time 0.0575(0.0579) | [  402/60000 (33%)] | Loss:   40.038311 |rec:  38.783710 | kl:   62.730106
Epoch 001 | Time 0.0589(0.0578) | [  422/60000 (35%)] | Loss:   38.576851 |rec:  37.345982 | kl:   61.543304
Epoch 001 | Time 0.0592(0.0580) | [  442/60000 (37%)] | Loss:   40.021015 |rec:  38.791126 | kl:   61.494457
Epoch 001 | Time 0.0576(0.0581) | [  462/60000 (38%)] | Loss:   40.305515 |rec:  38.995289 | kl:   65.511208
Epoch 001 | Time 0.0613(0.0584) | [  482/60000 (40%)] | Loss:   41.042328 |rec:  39.775524 | kl:   63.340179
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='true', rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='t', rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/snf/checkpt.pth', rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/snf/checkpt.pth', rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Epoch: 0 	Beta: 0.01
Epoch 000 | Time 0.3808(0.3808) | [    2/60000 ( 0%)] | Loss:  269.247284 |rec: 269.235962 | kl:    1.133712
Epoch 000 | Time 0.0621(0.2970) | [   22/60000 ( 2%)] | Loss:  249.702820 |rec: 249.693024 | kl:    0.979893
Epoch 000 | Time 0.0622(0.2351) | [   42/60000 ( 3%)] | Loss:  242.542847 |rec: 242.523865 | kl:    1.899370
Epoch 000 | Time 0.0615(0.1895) | [   62/60000 ( 5%)] | Loss:  213.892471 |rec: 213.855606 | kl:    3.687761
Epoch 000 | Time 0.0624(0.1559) | [   82/60000 ( 7%)] | Loss:  174.592422 |rec: 174.364182 | kl:   22.823507
Epoch 000 | Time 0.0608(0.1311) | [  102/60000 ( 8%)] | Loss:  155.165009 |rec: 152.315933 | kl:  284.907288
Epoch 000 | Time 0.0611(0.1128) | [  122/60000 (10%)] | Loss:  164.392456 |rec: 164.221619 | kl:   17.084736
Epoch 000 | Time 0.0611(0.0993) | [  142/60000 (12%)] | Loss:  145.041824 |rec: 139.330811 | kl:  571.101929
Epoch 000 | Time 0.0611(0.0894) | [  162/60000 (13%)] | Loss:  124.676491 |rec: 123.257050 | kl:  141.944366
Epoch 000 | Time 0.0615(0.0820) | [  182/60000 (15%)] | Loss:  115.984428 |rec: 113.940254 | kl:  204.418427
Epoch 000 | Time 0.0621(0.0766) | [  202/60000 (17%)] | Loss:  106.981476 |rec: 105.765945 | kl:  121.553093
Epoch 000 | Time 0.0621(0.0726) | [  222/60000 (18%)] | Loss:  100.843796 |rec:  99.964340 | kl:   87.945221
Epoch 000 | Time 0.0606(0.0697) | [  242/60000 (20%)] | Loss:  109.714470 |rec: 108.502045 | kl:  121.242416
Epoch 000 | Time 0.0604(0.0675) | [  262/60000 (22%)] | Loss:  104.273865 |rec: 103.089111 | kl:  118.475594
Epoch 000 | Time 0.0624(0.0660) | [  282/60000 (23%)] | Loss:  106.694565 |rec: 105.542984 | kl:  115.158478
Epoch 000 | Time 0.0616(0.0649) | [  302/60000 (25%)] | Loss:   99.678581 |rec:  98.881882 | kl:   79.670006
Epoch 000 | Time 0.0616(0.0640) | [  322/60000 (27%)] | Loss:  104.218056 |rec: 103.348640 | kl:   86.941177
Epoch 000 | Time 0.0602(0.0633) | [  342/60000 (28%)] | Loss:   96.854149 |rec:  96.138947 | kl:   71.520256
Epoch 000 | Time 0.0607(0.0628) | [  362/60000 (30%)] | Loss:  104.896469 |rec: 104.290161 | kl:   60.631245
Epoch 000 | Time 0.0612(0.0624) | [  382/60000 (32%)] | Loss:  100.821686 |rec: 100.180977 | kl:   64.071716
Epoch 000 | Time 0.0611(0.0621) | [  402/60000 (33%)] | Loss:   99.600273 |rec:  98.854332 | kl:   74.594185
Epoch 000 | Time 0.0628(0.0620) | [  422/60000 (35%)] | Loss:  123.078613 |rec: 122.575577 | kl:   50.303581
Epoch 000 | Time 0.0610(0.0618) | [  442/60000 (37%)] | Loss:  107.931236 |rec: 107.346420 | kl:   58.481239
Epoch 000 | Time 0.0619(0.0618) | [  462/60000 (38%)] | Loss:  107.700790 |rec: 107.098709 | kl:   60.208031
Epoch 000 | Time 0.0620(0.0617) | [  482/60000 (40%)] | Loss:  105.955528 |rec: 105.334801 | kl:   62.072357
Epoch 000 | Time 0.0609(0.0616) | [  502/60000 (42%)] | Loss:  101.338783 |rec: 100.741432 | kl:   59.735477
Epoch 000 | Time 0.0611(0.0615) | [  522/60000 (43%)] | Loss:  101.885117 |rec: 101.345116 | kl:   54.000160
Epoch 000 | Time 0.0625(0.0616) | [  542/60000 (45%)] | Loss:  100.078384 |rec:  99.506149 | kl:   57.223267
Epoch 000 | Time 0.0608(0.0616) | [  562/60000 (47%)] | Loss:  103.391319 |rec: 102.897987 | kl:   49.333145
Epoch 000 | Time 0.0613(0.0615) | [  582/60000 (48%)] | Loss:   99.807076 |rec:  99.325935 | kl:   48.114437
Epoch 000 | Time 0.0615(0.0614) | [  602/60000 (50%)] | Loss:   95.345963 |rec:  94.883507 | kl:   46.246342
Epoch 000 | Time 0.0617(0.0614) | [  622/60000 (52%)] | Loss:   95.605156 |rec:  95.201225 | kl:   40.392326
Epoch 000 | Time 0.0609(0.0615) | [  642/60000 (53%)] | Loss:  100.304344 |rec:  99.863121 | kl:   44.121998
Epoch 000 | Time 0.0624(0.0616) | [  662/60000 (55%)] | Loss:   98.488602 |rec:  98.122910 | kl:   36.569168
Epoch 000 | Time 0.0617(0.0617) | [  682/60000 (57%)] | Loss:   96.109802 |rec:  95.695747 | kl:   41.405056
Epoch 000 | Time 0.0617(0.0617) | [  702/60000 (58%)] | Loss:  100.953682 |rec: 100.530792 | kl:   42.289177
Epoch 000 | Time 0.0611(0.0617) | [  722/60000 (60%)] | Loss:   97.631454 |rec:  97.142067 | kl:   48.938770
Epoch 000 | Time 0.0617(0.0617) | [  742/60000 (62%)] | Loss:   96.894913 |rec:  96.302071 | kl:   59.284206
Epoch 000 | Time 0.0628(0.0617) | [  762/60000 (63%)] | Loss:   91.582420 |rec:  91.038536 | kl:   54.388988
Epoch 000 | Time 0.0615(0.0617) | [  782/60000 (65%)] | Loss:   93.258530 |rec:  92.665932 | kl:   59.259647
Epoch 000 | Time 0.0626(0.0618) | [  802/60000 (67%)] | Loss:   90.333153 |rec:  89.743935 | kl:   58.921562
Epoch 000 | Time 0.0629(0.0618) | [  822/60000 (68%)] | Loss:   92.633751 |rec:  91.951393 | kl:   68.235313
Epoch 000 | Time 0.0611(0.0617) | [  842/60000 (70%)] | Loss:   88.083717 |rec:  87.428085 | kl:   65.563049
Epoch 000 | Time 0.0619(0.0616) | [  862/60000 (72%)] | Loss:   88.495392 |rec:  87.787720 | kl:   70.766945
Epoch 000 | Time 0.0611(0.0616) | [  882/60000 (73%)] | Loss:   87.763618 |rec:  87.116241 | kl:   64.738701
Epoch 000 | Time 0.0623(0.0616) | [  902/60000 (75%)] | Loss:   87.883339 |rec:  87.054413 | kl:   82.892319
Epoch 000 | Time 0.0623(0.0617) | [  922/60000 (77%)] | Loss:   84.465546 |rec:  83.689087 | kl:   77.645981
Epoch 000 | Time 0.0614(0.0618) | [  942/60000 (78%)] | Loss:   82.870300 |rec:  81.804794 | kl:  106.550056
Epoch 000 | Time 0.0618(0.0619) | [  962/60000 (80%)] | Loss:   80.674225 |rec:  79.727745 | kl:   94.648071
Epoch 000 | Time 0.0617(0.0620) | [  982/60000 (82%)] | Loss:   77.578247 |rec:  76.463188 | kl:  111.505440
Epoch 000 | Time 0.0625(0.0620) | [ 1002/60000 (83%)] | Loss:   72.152138 |rec:  71.033501 | kl:  111.863853
Epoch 000 | Time 0.0614(0.0620) | [ 1022/60000 (85%)] | Loss:   73.055099 |rec:  71.834808 | kl:  122.028572
Epoch 000 | Time 0.0614(0.0620) | [ 1042/60000 (87%)] | Loss:   73.307907 |rec:  71.971565 | kl:  133.633591
Epoch 000 | Time 0.0613(0.0620) | [ 1062/60000 (88%)] | Loss:   70.394760 |rec:  69.075241 | kl:  131.952011
Epoch 000 | Time 0.0616(0.0620) | [ 1082/60000 (90%)] | Loss:   65.099045 |rec:  64.030067 | kl:  106.897705
Epoch 000 | Time 0.0617(0.0619) | [ 1102/60000 (92%)] | Loss:   64.037567 |rec:  62.789917 | kl:  124.765312
Epoch 000 | Time 0.0623(0.0619) | [ 1122/60000 (93%)] | Loss:   64.242790 |rec:  62.967201 | kl:  127.558769
Epoch 000 | Time 0.0609(0.0619) | [ 1142/60000 (95%)] | Loss:   69.290604 |rec:  67.985855 | kl:  130.474640
Epoch 000 | Time 0.0623(0.0619) | [ 1162/60000 (97%)] | Loss:   65.655159 |rec:  64.399094 | kl:  125.606659
Epoch 000 | Time 0.0614(0.0619) | [ 1182/60000 (98%)] | Loss:   64.788338 |rec:  63.544258 | kl:  124.408112
validating...
Epoch 0000 | Time 2.3269 | Loss 62.1242
Epoch: 1 	Beta: 0.02
Epoch 001 | Time 0.0627(0.0619) | [    2/60000 ( 0%)] | Loss:   67.157005 |rec:  64.168999 | kl:  149.400070
Epoch 001 | Time 0.0613(0.0618) | [   22/60000 ( 2%)] | Loss:   62.770927 |rec:  60.479252 | kl:  114.583794
Epoch 001 | Time 0.0625(0.0618) | [   42/60000 ( 3%)] | Loss:   57.782871 |rec:  55.843079 | kl:   96.989395
Epoch 001 | Time 0.0610(0.0618) | [   62/60000 ( 5%)] | Loss:   62.708874 |rec:  60.734905 | kl:   98.698364
Epoch 001 | Time 0.0626(0.0618) | [   82/60000 ( 7%)] | Loss:   58.563068 |rec:  56.626099 | kl:   96.848686
Epoch 001 | Time 0.0615(0.0618) | [  102/60000 ( 8%)] | Loss:   60.234795 |rec:  58.281609 | kl:   97.659264
Epoch 001 | Time 0.0614(0.0618) | [  122/60000 (10%)] | Loss:   60.339443 |rec:  58.496445 | kl:   92.149902
Epoch 001 | Time 0.0627(0.0618) | [  142/60000 (12%)] | Loss:   57.546570 |rec:  55.630032 | kl:   95.826851
Epoch 001 | Time 0.0631(0.0618) | [  162/60000 (13%)] | Loss:   56.174595 |rec:  54.260975 | kl:   95.680801
Epoch 001 | Time 0.0617(0.0617) | [  182/60000 (15%)] | Loss:   55.645775 |rec:  53.875637 | kl:   88.506851
Epoch 001 | Time 0.0618(0.0617) | [  202/60000 (17%)] | Loss:   54.238247 |rec:  52.417671 | kl:   91.028748
Epoch 001 | Time 0.0634(0.0617) | [  222/60000 (18%)] | Loss:   53.226128 |rec:  51.337715 | kl:   94.420555
Epoch 001 | Time 0.0627(0.0616) | [  242/60000 (20%)] | Loss:   51.843975 |rec:  49.967148 | kl:   93.841286
Epoch 001 | Time 0.0615(0.0617) | [  262/60000 (22%)] | Loss:   54.336113 |rec:  52.558151 | kl:   88.898048
Epoch 001 | Time 0.0610(0.0618) | [  282/60000 (23%)] | Loss:   52.745609 |rec:  50.830643 | kl:   95.748360
Epoch 001 | Time 0.0635(0.0620) | [  302/60000 (25%)] | Loss:   49.821148 |rec:  47.991455 | kl:   91.484688
Epoch 001 | Time 0.0621(0.0620) | [  322/60000 (27%)] | Loss:   51.406467 |rec:  49.678040 | kl:   86.421295
Epoch 001 | Time 0.0632(0.0622) | [  342/60000 (28%)] | Loss:   49.122334 |rec:  47.360615 | kl:   88.085876
Epoch 001 | Time 0.0618(0.0622) | [  362/60000 (30%)] | Loss:   51.042500 |rec:  49.233505 | kl:   90.449776
Epoch 001 | Time 0.0627(0.0622) | [  382/60000 (32%)] | Loss:   50.445782 |rec:  48.786655 | kl:   82.956245
Epoch 001 | Time 0.0618(0.0622) | [  402/60000 (33%)] | Loss:   47.386250 |rec:  45.554348 | kl:   91.594887
Epoch 001 | Time 0.0629(0.0623) | [  422/60000 (35%)] | Loss:   51.180534 |rec:  49.296936 | kl:   94.179848
Epoch 001 | Time 0.0628(0.0623) | [  442/60000 (37%)] | Loss:   47.695858 |rec:  45.858002 | kl:   91.892769
Epoch 001 | Time 0.0620(0.0623) | [  462/60000 (38%)] | Loss:   47.911228 |rec:  46.335976 | kl:   78.762604
Epoch 001 | Time 0.0631(0.0624) | [  482/60000 (40%)] | Loss:   47.021748 |rec:  45.269989 | kl:   87.587929
Epoch 001 | Time 0.0621(0.0623) | [  502/60000 (42%)] | Loss:   48.822845 |rec:  47.143291 | kl:   83.977730
Epoch 001 | Time 0.0621(0.0623) | [  522/60000 (43%)] | Loss:   47.927475 |rec:  46.292664 | kl:   81.740456
Epoch 001 | Time 0.0636(0.0623) | [  542/60000 (45%)] | Loss:   47.188911 |rec:  45.584511 | kl:   80.220016
Epoch 001 | Time 0.0617(0.0622) | [  562/60000 (47%)] | Loss:   45.091755 |rec:  43.424881 | kl:   83.343834
Epoch 001 | Time 0.0616(0.0623) | [  582/60000 (48%)] | Loss:   44.228485 |rec:  42.571903 | kl:   82.829117
Epoch 001 | Time 0.0624(0.0623) | [  602/60000 (50%)] | Loss:   45.950642 |rec:  44.313232 | kl:   81.870697
Epoch 001 | Time 0.0626(0.0622) | [  622/60000 (52%)] | Loss:   45.764851 |rec:  44.131931 | kl:   81.645882
Epoch 001 | Time 0.0626(0.0622) | [  642/60000 (53%)] | Loss:   45.806450 |rec:  44.247093 | kl:   77.967857
Epoch 001 | Time 0.0620(0.0622) | [  662/60000 (55%)] | Loss:   44.591221 |rec:  43.104237 | kl:   74.349129
Epoch 001 | Time 0.0612(0.0622) | [  682/60000 (57%)] | Loss:   44.454639 |rec:  42.987133 | kl:   73.375183
Epoch 001 | Time 0.0614(0.0621) | [  702/60000 (58%)] | Loss:   46.226215 |rec:  44.638123 | kl:   79.404594
Epoch 001 | Time 0.0627(0.0622) | [  722/60000 (60%)] | Loss:   44.489311 |rec:  42.910091 | kl:   78.960930
Epoch 001 | Time 0.0621(0.0621) | [  742/60000 (62%)] | Loss:   46.049271 |rec:  44.493546 | kl:   77.786339
Epoch 001 | Time 0.0635(0.0622) | [  762/60000 (63%)] | Loss:   43.516285 |rec:  42.031128 | kl:   74.257759
Epoch 001 | Time 0.0622(0.0622) | [  782/60000 (65%)] | Loss:   44.103954 |rec:  42.592022 | kl:   75.596725
Epoch 001 | Time 0.0614(0.0622) | [  802/60000 (67%)] | Loss:   44.128887 |rec:  42.502079 | kl:   81.340439
Epoch 001 | Time 0.0624(0.0622) | [  822/60000 (68%)] | Loss:   44.508720 |rec:  42.963837 | kl:   77.244148
Epoch 001 | Time 0.0616(0.0622) | [  842/60000 (70%)] | Loss:   44.911728 |rec:  43.354832 | kl:   77.844803
Epoch 001 | Time 0.0623(0.0623) | [  862/60000 (72%)] | Loss:   42.241196 |rec:  40.759346 | kl:   74.092415
Epoch 001 | Time 0.0617(0.0622) | [  882/60000 (73%)] | Loss:   43.581337 |rec:  42.097252 | kl:   74.204308
Epoch 001 | Time 0.0622(0.0623) | [  902/60000 (75%)] | Loss:   43.730091 |rec:  42.178089 | kl:   77.600021
Epoch 001 | Time 0.0632(0.0623) | [  922/60000 (77%)] | Loss:   42.127369 |rec:  40.706413 | kl:   71.047691
Epoch 001 | Time 0.0623(0.0623) | [  942/60000 (78%)] | Loss:   43.299126 |rec:  41.816238 | kl:   74.144218
Epoch 001 | Time 0.0615(0.0623) | [  962/60000 (80%)] | Loss:   42.336571 |rec:  40.892593 | kl:   72.198883
Epoch 001 | Time 0.0621(0.0624) | [  982/60000 (82%)] | Loss:   43.921741 |rec:  42.496723 | kl:   71.250885
Epoch 001 | Time 0.0628(0.0623) | [ 1002/60000 (83%)] | Loss:   43.697319 |rec:  42.255562 | kl:   72.087906
Epoch 001 | Time 0.0627(0.0624) | [ 1022/60000 (85%)] | Loss:   42.564892 |rec:  41.137680 | kl:   71.360649
Epoch 001 | Time 0.0621(0.0623) | [ 1042/60000 (87%)] | Loss:   41.470913 |rec:  40.032448 | kl:   71.923180
Epoch 001 | Time 0.0625(0.0623) | [ 1062/60000 (88%)] | Loss:   41.838123 |rec:  40.456551 | kl:   69.078506
Epoch 001 | Time 0.0615(0.0623) | [ 1082/60000 (90%)] | Loss:   40.469479 |rec:  39.062611 | kl:   70.343376
Epoch 001 | Time 0.0617(0.0622) | [ 1102/60000 (92%)] | Loss:   43.116837 |rec:  41.704731 | kl:   70.605202
Epoch 001 | Time 0.0629(0.0623) | [ 1122/60000 (93%)] | Loss:   43.231773 |rec:  41.802494 | kl:   71.463898
Epoch 001 | Time 0.0622(0.0623) | [ 1142/60000 (95%)] | Loss:   41.586452 |rec:  40.227970 | kl:   67.924248
Epoch 001 | Time 0.0633(0.0623) | [ 1162/60000 (97%)] | Loss:   42.151417 |rec:  40.743725 | kl:   70.384590
Epoch 001 | Time 0.0620(0.0622) | [ 1182/60000 (98%)] | Loss:   41.040527 |rec:  39.695320 | kl:   67.260368
validating...
Epoch 0001 | Time 2.2564 | Loss 40.5672
Epoch: 2 	Beta: 0.03
Epoch 002 | Time 0.0627(0.0622) | [    2/60000 ( 0%)] | Loss:   40.738148 |rec:  38.798389 | kl:   64.658577
Epoch 002 | Time 0.0617(0.0621) | [   22/60000 ( 2%)] | Loss:   41.021275 |rec:  39.095905 | kl:   64.178879
Epoch 002 | Time 0.0618(0.0621) | [   42/60000 ( 3%)] | Loss:   41.944237 |rec:  40.026695 | kl:   63.918114
Epoch 002 | Time 0.0619(0.0620) | [   62/60000 ( 5%)] | Loss:   42.449886 |rec:  40.561195 | kl:   62.956367
Epoch 002 | Time 0.0626(0.0621) | [   82/60000 ( 7%)] | Loss:   39.740150 |rec:  38.025143 | kl:   57.166908
Epoch 002 | Time 0.0619(0.0621) | [  102/60000 ( 8%)] | Loss:   41.075977 |rec:  39.313381 | kl:   58.753113
Epoch 002 | Time 0.0610(0.0622) | [  122/60000 (10%)] | Loss:   42.306179 |rec:  40.547791 | kl:   58.612984
Epoch 002 | Time 0.0629(0.0622) | [  142/60000 (12%)] | Loss:   40.328968 |rec:  38.618900 | kl:   57.002274
Epoch 002 | Time 0.0620(0.0622) | [  162/60000 (13%)] | Loss:   40.009014 |rec:  38.381214 | kl:   54.259922
Epoch 002 | Time 0.0615(0.0622) | [  182/60000 (15%)] | Loss:   38.805584 |rec:  37.083450 | kl:   57.404488
Epoch 002 | Time 0.0628(0.0622) | [  202/60000 (17%)] | Loss:   39.856453 |rec:  38.162376 | kl:   56.469227
Epoch 002 | Time 0.0614(0.0623) | [  222/60000 (18%)] | Loss:   39.692730 |rec:  38.055481 | kl:   54.574924
Epoch 002 | Time 0.0631(0.0623) | [  242/60000 (20%)] | Loss:   40.411583 |rec:  38.704014 | kl:   56.918999
Epoch 002 | Time 0.0619(0.0623) | [  262/60000 (22%)] | Loss:   39.895081 |rec:  38.221878 | kl:   55.773357
Epoch 002 | Time 0.0619(0.0623) | [  282/60000 (23%)] | Loss:   40.162128 |rec:  38.553162 | kl:   53.632187
Epoch 002 | Time 0.0634(0.0624) | [  302/60000 (25%)] | Loss:   37.690166 |rec:  36.118530 | kl:   52.387897
Epoch 002 | Time 0.0624(0.0624) | [  322/60000 (27%)] | Loss:   39.872444 |rec:  38.183277 | kl:   56.305561
Epoch 002 | Time 0.0620(0.0624) | [  342/60000 (28%)] | Loss:   41.156197 |rec:  39.488804 | kl:   55.579784
Epoch 002 | Time 0.0620(0.0625) | [  362/60000 (30%)] | Loss:   39.186958 |rec:  37.595440 | kl:   53.050632
Epoch 002 | Time 0.0629(0.0624) | [  382/60000 (32%)] | Loss:   38.352562 |rec:  36.720299 | kl:   54.408836
Epoch 002 | Time 0.0634(0.0625) | [  402/60000 (33%)] | Loss:   41.268562 |rec:  39.602428 | kl:   55.537876
Epoch 002 | Time 0.0632(0.0626) | [  422/60000 (35%)] | Loss:   41.005283 |rec:  39.385654 | kl:   53.987541
Epoch 002 | Time 0.0627(0.0627) | [  442/60000 (37%)] | Loss:   37.111675 |rec:  35.568329 | kl:   51.444870
Epoch 002 | Time 0.0624(0.0627) | [  462/60000 (38%)] | Loss:   39.408779 |rec:  37.892288 | kl:   50.549706
Epoch 002 | Time 0.0632(0.0627) | [  482/60000 (40%)] | Loss:   39.974171 |rec:  38.385967 | kl:   52.940090
Epoch 002 | Time 0.0635(0.0627) | [  502/60000 (42%)] | Loss:   39.562347 |rec:  37.957767 | kl:   53.485985
Epoch 002 | Time 0.0621(0.0627) | [  522/60000 (43%)] | Loss:   41.031231 |rec:  39.384533 | kl:   54.889961
Epoch 002 | Time 0.0615(0.0627) | [  542/60000 (45%)] | Loss:   38.240318 |rec:  36.689980 | kl:   51.678017
Epoch 002 | Time 0.0619(0.0627) | [  562/60000 (47%)] | Loss:   40.583641 |rec:  39.032425 | kl:   51.707188
Epoch 002 | Time 0.0620(0.0627) | [  582/60000 (48%)] | Loss:   39.652359 |rec:  38.076126 | kl:   52.541191
Epoch 002 | Time 0.0619(0.0627) | [  602/60000 (50%)] | Loss:   40.380661 |rec:  38.773895 | kl:   53.558819
Epoch 002 | Time 0.0636(0.0627) | [  622/60000 (52%)] | Loss:   38.961067 |rec:  37.327225 | kl:   54.461426
Epoch 002 | Time 0.0625(0.0626) | [  642/60000 (53%)] | Loss:   39.315453 |rec:  37.817181 | kl:   49.942368
Epoch 002 | Time 0.0618(0.0625) | [  662/60000 (55%)] | Loss:   39.359303 |rec:  37.748699 | kl:   53.686661
Epoch 002 | Time 0.0620(0.0626) | [  682/60000 (57%)] | Loss:   38.886795 |rec:  37.324928 | kl:   52.062275
Epoch 002 | Time 0.0628(0.0626) | [  702/60000 (58%)] | Loss:   39.654675 |rec:  38.061268 | kl:   53.113602
Epoch 002 | Time 0.0626(0.0626) | [  722/60000 (60%)] | Loss:   39.623447 |rec:  38.020943 | kl:   53.416840
Epoch 002 | Time 0.0624(0.0626) | [  742/60000 (62%)] | Loss:   36.618580 |rec:  35.058968 | kl:   51.987015
Epoch 002 | Time 0.0622(0.0625) | [  762/60000 (63%)] | Loss:   39.602955 |rec:  38.060276 | kl:   51.422531
Epoch 002 | Time 0.0634(0.0626) | [  782/60000 (65%)] | Loss:   36.394600 |rec:  34.859722 | kl:   51.162621
Epoch 002 | Time 0.0620(0.0626) | [  802/60000 (67%)] | Loss:   37.259171 |rec:  35.693584 | kl:   52.186195
Epoch 002 | Time 0.0623(0.0627) | [  822/60000 (68%)] | Loss:   38.322369 |rec:  36.791187 | kl:   51.039394
Epoch 002 | Time 0.0634(0.0627) | [  842/60000 (70%)] | Loss:   40.107964 |rec:  38.554413 | kl:   51.785023
Epoch 002 | Time 0.0628(0.0627) | [  862/60000 (72%)] | Loss:   37.554310 |rec:  36.065590 | kl:   49.624065
Epoch 002 | Time 0.0629(0.0628) | [  882/60000 (73%)] | Loss:   37.295094 |rec:  35.819935 | kl:   49.171947
Epoch 002 | Time 0.0625(0.0628) | [  902/60000 (75%)] | Loss:   38.651253 |rec:  37.082615 | kl:   52.287865
Epoch 002 | Time 0.0623(0.0628) | [  922/60000 (77%)] | Loss:   38.941746 |rec:  37.365746 | kl:   52.533340
Epoch 002 | Time 0.0624(0.0629) | [  942/60000 (78%)] | Loss:   38.420692 |rec:  36.867886 | kl:   51.760223
Epoch 002 | Time 0.0631(0.0628) | [  962/60000 (80%)] | Loss:   37.483543 |rec:  35.960022 | kl:   50.784027
Epoch 002 | Time 0.0622(0.0628) | [  982/60000 (82%)] | Loss:   37.862942 |rec:  36.283783 | kl:   52.638695
Epoch 002 | Time 0.0640(0.0629) | [ 1002/60000 (83%)] | Loss:   36.838711 |rec:  35.365128 | kl:   49.119511
Epoch 002 | Time 0.0628(0.0628) | [ 1022/60000 (85%)] | Loss:   38.493484 |rec:  36.963692 | kl:   50.993134
Epoch 002 | Time 0.0625(0.0629) | [ 1042/60000 (87%)] | Loss:   39.306702 |rec:  37.789158 | kl:   50.584881
Epoch 002 | Time 0.0623(0.0629) | [ 1062/60000 (88%)] | Loss:   36.819462 |rec:  35.307499 | kl:   50.398796
Epoch 002 | Time 0.0628(0.0628) | [ 1082/60000 (90%)] | Loss:   38.030056 |rec:  36.556942 | kl:   49.103828
Epoch 002 | Time 0.0623(0.0628) | [ 1102/60000 (92%)] | Loss:   38.783207 |rec:  37.252254 | kl:   51.031754
Epoch 002 | Time 0.0635(0.0629) | [ 1122/60000 (93%)] | Loss:   38.186405 |rec:  36.640892 | kl:   51.517097
Epoch 002 | Time 0.0623(0.0628) | [ 1142/60000 (95%)] | Loss:   37.298409 |rec:  35.792564 | kl:   50.194843
Epoch 002 | Time 0.0642(0.0628) | [ 1162/60000 (97%)] | Loss:   37.584446 |rec:  36.057148 | kl:   50.909912
Epoch 002 | Time 0.0633(0.0628) | [ 1182/60000 (98%)] | Loss:   38.036621 |rec:  36.540440 | kl:   49.872681
validating...
Epoch 0002 | Time 2.2512 | Loss 36.9048
Epoch: 3 	Beta: 0.04
Epoch 003 | Time 0.0636(0.0628) | [    2/60000 ( 0%)] | Loss:   36.971870 |rec:  34.905579 | kl:   51.657288
Epoch 003 | Time 0.0629(0.0627) | [   22/60000 ( 2%)] | Loss:   39.331921 |rec:  37.406372 | kl:   48.138744
Epoch 003 | Time 0.0622(0.0627) | [   42/60000 ( 3%)] | Loss:   40.093418 |rec:  38.082054 | kl:   50.284100
Epoch 003 | Time 0.0637(0.0627) | [   62/60000 ( 5%)] | Loss:   39.237057 |rec:  37.367466 | kl:   46.739773
Epoch 003 | Time 0.0629(0.0627) | [   82/60000 ( 7%)] | Loss:   38.425709 |rec:  36.514664 | kl:   47.776104
Epoch 003 | Time 0.0624(0.0627) | [  102/60000 ( 8%)] | Loss:   37.296513 |rec:  35.356735 | kl:   48.494442
Epoch 003 | Time 0.0636(0.0627) | [  122/60000 (10%)] | Loss:   38.509983 |rec:  36.612221 | kl:   47.444042
Epoch 003 | Time 0.0619(0.0627) | [  142/60000 (12%)] | Loss:   38.099808 |rec:  36.180374 | kl:   47.985825
Epoch 003 | Time 0.0634(0.0627) | [  162/60000 (13%)] | Loss:   37.365322 |rec:  35.589603 | kl:   44.392967
Epoch 003 | Time 0.0621(0.0626) | [  182/60000 (15%)] | Loss:   40.190456 |rec:  38.266617 | kl:   48.095985
Epoch 003 | Time 0.0637(0.0627) | [  202/60000 (17%)] | Loss:   37.472237 |rec:  35.662617 | kl:   45.240498
Epoch 003 | Time 0.0629(0.0627) | [  222/60000 (18%)] | Loss:   36.076973 |rec:  34.207436 | kl:   46.738438
Epoch 003 | Time 0.0619(0.0627) | [  242/60000 (20%)] | Loss:   39.213707 |rec:  37.330959 | kl:   47.068645
Epoch 003 | Time 0.0630(0.0627) | [  262/60000 (22%)] | Loss:   38.803524 |rec:  36.887257 | kl:   47.906746
Epoch 003 | Time 0.0632(0.0627) | [  282/60000 (23%)] | Loss:   37.297657 |rec:  35.475815 | kl:   45.546093
Epoch 003 | Time 0.0625(0.0628) | [  302/60000 (25%)] | Loss:   38.243984 |rec:  36.448887 | kl:   44.877434
Epoch 003 | Time 0.0627(0.0627) | [  322/60000 (27%)] | Loss:   37.865948 |rec:  36.080669 | kl:   44.631962
Epoch 003 | Time 0.0621(0.0628) | [  342/60000 (28%)] | Loss:   39.009922 |rec:  37.257397 | kl:   43.813198
Epoch 003 | Time 0.0624(0.0628) | [  362/60000 (30%)] | Loss:   35.667091 |rec:  33.970318 | kl:   42.419304
Epoch 003 | Time 0.0640(0.0628) | [  382/60000 (32%)] | Loss:   36.549213 |rec:  34.809288 | kl:   43.498100
Epoch 003 | Time 0.0632(0.0628) | [  402/60000 (33%)] | Loss:   37.526569 |rec:  35.717007 | kl:   45.239029
Epoch 003 | Time 0.0625(0.0627) | [  422/60000 (35%)] | Loss:   37.742744 |rec:  35.973621 | kl:   44.228119
Epoch 003 | Time 0.0637(0.0627) | [  442/60000 (37%)] | Loss:   37.436958 |rec:  35.617001 | kl:   45.499004
Epoch 003 | Time 0.0624(0.0627) | [  462/60000 (38%)] | Loss:   36.895271 |rec:  35.014793 | kl:   47.011894
Epoch 003 | Time 0.0618(0.0627) | [  482/60000 (40%)] | Loss:   37.552517 |rec:  35.719875 | kl:   45.815952
Epoch 003 | Time 0.0624(0.0627) | [  502/60000 (42%)] | Loss:   36.847881 |rec:  34.978138 | kl:   46.743572
Epoch 003 | Time 0.0624(0.0627) | [  522/60000 (43%)] | Loss:   37.380943 |rec:  35.616978 | kl:   44.099148
Epoch 003 | Time 0.0634(0.0627) | [  542/60000 (45%)] | Loss:   37.378510 |rec:  35.615620 | kl:   44.072273
Epoch 003 | Time 0.0621(0.0626) | [  562/60000 (47%)] | Loss:   35.770012 |rec:  34.007793 | kl:   44.055408
Epoch 003 | Time 0.0625(0.0626) | [  582/60000 (48%)] | Loss:   34.964039 |rec:  33.203491 | kl:   44.013733
Epoch 003 | Time 0.0620(0.0626) | [  602/60000 (50%)] | Loss:   36.580433 |rec:  34.875172 | kl:   42.631592
Epoch 003 | Time 0.0621(0.0626) | [  622/60000 (52%)] | Loss:   37.458679 |rec:  35.679104 | kl:   44.489471
Epoch 003 | Time 0.0636(0.0626) | [  642/60000 (53%)] | Loss:   37.986084 |rec:  36.134327 | kl:   46.294003
Epoch 003 | Time 0.0619(0.0625) | [  662/60000 (55%)] | Loss:   36.615398 |rec:  34.889835 | kl:   43.139061
Epoch 003 | Time 0.0631(0.0626) | [  682/60000 (57%)] | Loss:   36.577206 |rec:  34.750618 | kl:   45.664642
Epoch 003 | Time 0.0615(0.0626) | [  702/60000 (58%)] | Loss:   36.081921 |rec:  34.281956 | kl:   44.999138
Epoch 003 | Time 0.0628(0.0626) | [  722/60000 (60%)] | Loss:   36.373402 |rec:  34.561752 | kl:   45.291176
Epoch 003 | Time 0.0617(0.0626) | [  742/60000 (62%)] | Loss:   35.881886 |rec:  34.140381 | kl:   43.537586
Epoch 003 | Time 0.0634(0.0626) | [  762/60000 (63%)] | Loss:   38.187389 |rec:  36.294430 | kl:   47.324062
Epoch 003 | Time 0.0623(0.0626) | [  782/60000 (65%)] | Loss:   36.324276 |rec:  34.546078 | kl:   44.454929
Epoch 003 | Time 0.0623(0.0627) | [  802/60000 (67%)] | Loss:   37.018570 |rec:  35.212231 | kl:   45.158524
Epoch 003 | Time 0.0623(0.0627) | [  822/60000 (68%)] | Loss:   34.410336 |rec:  32.674404 | kl:   43.398251
Epoch 003 | Time 0.0621(0.0627) | [  842/60000 (70%)] | Loss:   35.780304 |rec:  33.956879 | kl:   45.585644
Epoch 003 | Time 0.0622(0.0628) | [  862/60000 (72%)] | Loss:   35.885418 |rec:  34.122055 | kl:   44.084042
Epoch 003 | Time 0.0633(0.0627) | [  882/60000 (73%)] | Loss:   36.779537 |rec:  34.981567 | kl:   44.949242
Epoch 003 | Time 0.0627(0.0627) | [  902/60000 (75%)] | Loss:   36.452454 |rec:  34.650848 | kl:   45.040154
Epoch 003 | Time 0.0619(0.0628) | [  922/60000 (77%)] | Loss:   35.927639 |rec:  34.178352 | kl:   43.732132
Epoch 003 | Time 0.0631(0.0628) | [  942/60000 (78%)] | Loss:   35.656208 |rec:  33.949608 | kl:   42.665039
Epoch 003 | Time 0.0621(0.0628) | [  962/60000 (80%)] | Loss:   36.029011 |rec:  34.213196 | kl:   45.395340
Epoch 003 | Time 0.0625(0.0630) | [  982/60000 (82%)] | Loss:   36.703770 |rec:  34.947292 | kl:   43.911926
Epoch 003 | Time 0.0631(0.0630) | [ 1002/60000 (83%)] | Loss:   35.712299 |rec:  33.983086 | kl:   43.230354
Epoch 003 | Time 0.0613(0.0627) | [ 1022/60000 (85%)] | Loss:   36.274693 |rec:  34.483212 | kl:   44.787067
Epoch 003 | Time 0.0579(0.0623) | [ 1042/60000 (87%)] | Loss:   34.923866 |rec:  33.226269 | kl:   42.439941
Epoch 003 | Time 0.0580(0.0614) | [ 1062/60000 (88%)] | Loss:   36.273109 |rec:  34.470901 | kl:   45.055183
Epoch 003 | Time 0.0617(0.0607) | [ 1082/60000 (90%)] | Loss:   36.900269 |rec:  35.095062 | kl:   45.130119
Epoch 003 | Time 0.0637(0.0609) | [ 1102/60000 (92%)] | Loss:   34.646713 |rec:  32.931644 | kl:   42.876724
Epoch 003 | Time 0.0580(0.0610) | [ 1122/60000 (93%)] | Loss:   36.331219 |rec:  34.544044 | kl:   44.679314
Epoch 003 | Time 0.0579(0.0602) | [ 1142/60000 (95%)] | Loss:   36.270119 |rec:  34.453560 | kl:   45.413994
Epoch 003 | Time 0.0579(0.0596) | [ 1162/60000 (97%)] | Loss:   35.988136 |rec:  34.174210 | kl:   45.348186
Epoch 003 | Time 0.0583(0.0593) | [ 1182/60000 (98%)] | Loss:   35.332848 |rec:  33.500626 | kl:   45.805527
validating...
Epoch 0003 | Time 2.3052 | Loss 35.7011
Epoch: 4 	Beta: 0.05
Epoch 004 | Time 0.0589(0.0591) | [    2/60000 ( 0%)] | Loss:   35.576702 |rec:  33.381226 | kl:   43.909580
Epoch 004 | Time 0.0618(0.0595) | [   22/60000 ( 2%)] | Loss:   37.885254 |rec:  35.715179 | kl:   43.401524
Epoch 004 | Time 0.0609(0.0602) | [   42/60000 ( 3%)] | Loss:   36.654049 |rec:  34.551983 | kl:   42.041294
Epoch 004 | Time 0.0582(0.0598) | [   62/60000 ( 5%)] | Loss:   36.197205 |rec:  34.154102 | kl:   40.862022
Epoch 004 | Time 0.0583(0.0597) | [   82/60000 ( 7%)] | Loss:   38.519337 |rec:  36.414955 | kl:   42.087601
Epoch 004 | Time 0.0608(0.0595) | [  102/60000 ( 8%)] | Loss:   36.280220 |rec:  34.117382 | kl:   43.256775
Epoch 004 | Time 0.0594(0.0593) | [  122/60000 (10%)] | Loss:   36.586975 |rec:  34.499817 | kl:   41.743172
Epoch 004 | Time 0.0582(0.0595) | [  142/60000 (12%)] | Loss:   37.536156 |rec:  35.407349 | kl:   42.576107
Epoch 004 | Time 0.0581(0.0592) | [  162/60000 (13%)] | Loss:   36.898628 |rec:  34.815823 | kl:   41.656075
Epoch 004 | Time 0.0585(0.0590) | [  182/60000 (15%)] | Loss:   36.323284 |rec:  34.288666 | kl:   40.692337
Epoch 004 | Time 0.0591(0.0589) | [  202/60000 (17%)] | Loss:   35.688999 |rec:  33.514091 | kl:   43.498203
Epoch 004 | Time 0.0584(0.0589) | [  222/60000 (18%)] | Loss:   36.592014 |rec:  34.533733 | kl:   41.165554
Epoch 004 | Time 0.0583(0.0588) | [  242/60000 (20%)] | Loss:   35.725876 |rec:  33.663319 | kl:   41.251102
Epoch 004 | Time 0.0622(0.0594) | [  262/60000 (22%)] | Loss:   37.452644 |rec:  35.327003 | kl:   42.512791
Epoch 004 | Time 0.0604(0.0596) | [  282/60000 (23%)] | Loss:   35.432919 |rec:  33.310371 | kl:   42.450893
Epoch 004 | Time 0.0626(0.0601) | [  302/60000 (25%)] | Loss:   36.846222 |rec:  34.713120 | kl:   42.662060
Epoch 004 | Time 0.0628(0.0609) | [  322/60000 (27%)] | Loss:   36.888237 |rec:  34.769356 | kl:   42.377640
Epoch 004 | Time 0.0587(0.0610) | [  342/60000 (28%)] | Loss:   36.143143 |rec:  34.038231 | kl:   42.098194
Epoch 004 | Time 0.0582(0.0612) | [  362/60000 (30%)] | Loss:   36.466682 |rec:  34.376602 | kl:   41.801590
Epoch 004 | Time 0.0584(0.0615) | [  382/60000 (32%)] | Loss:   35.947178 |rec:  33.936260 | kl:   40.218395
Epoch 004 | Time 0.0598(0.0617) | [  402/60000 (33%)] | Loss:   34.683075 |rec:  32.662655 | kl:   40.408386
Epoch 004 | Time 0.0583(0.0613) | [  422/60000 (35%)] | Loss:   35.288841 |rec:  33.194267 | kl:   41.891468
Epoch 004 | Time 0.0633(0.0612) | [  442/60000 (37%)] | Loss:   34.604290 |rec:  32.544064 | kl:   41.204609
Epoch 004 | Time 0.0625(0.0613) | [  462/60000 (38%)] | Loss:   35.734383 |rec:  33.654926 | kl:   41.589092
Epoch 004 | Time 0.0597(0.0615) | [  482/60000 (40%)] | Loss:   34.806751 |rec:  32.817581 | kl:   39.783413
Epoch 004 | Time 0.0583(0.0607) | [  502/60000 (42%)] | Loss:   36.732998 |rec:  34.639671 | kl:   41.866543
Epoch 004 | Time 0.0583(0.0601) | [  522/60000 (43%)] | Loss:   35.067909 |rec:  33.023048 | kl:   40.897213
Epoch 004 | Time 0.0583(0.0597) | [  542/60000 (45%)] | Loss:   37.809292 |rec:  35.648773 | kl:   43.210342
Epoch 004 | Time 0.0584(0.0593) | [  562/60000 (47%)] | Loss:   35.490513 |rec:  33.409698 | kl:   41.616249
Epoch 004 | Time 0.0583(0.0591) | [  582/60000 (48%)] | Loss:   36.765377 |rec:  34.627796 | kl:   42.751648
Epoch 004 | Time 0.0582(0.0589) | [  602/60000 (50%)] | Loss:   35.170708 |rec:  33.122070 | kl:   40.972710
Epoch 004 | Time 0.0584(0.0588) | [  622/60000 (52%)] | Loss:   37.833355 |rec:  35.790962 | kl:   40.847839
Epoch 004 | Time 0.0583(0.0586) | [  642/60000 (53%)] | Loss:   37.184185 |rec:  35.083897 | kl:   42.005745
Epoch 004 | Time 0.0583(0.0586) | [  662/60000 (55%)] | Loss:   37.047318 |rec:  34.947781 | kl:   41.990662
Epoch 004 | Time 0.0581(0.0585) | [  682/60000 (57%)] | Loss:   36.314766 |rec:  34.271595 | kl:   40.863365
Epoch 004 | Time 0.0584(0.0585) | [  702/60000 (58%)] | Loss:   36.407444 |rec:  34.275784 | kl:   42.633221
Epoch 004 | Time 0.0582(0.0585) | [  722/60000 (60%)] | Loss:   35.109837 |rec:  33.059132 | kl:   41.014091
Epoch 004 | Time 0.0583(0.0585) | [  742/60000 (62%)] | Loss:   36.022663 |rec:  33.923969 | kl:   41.973866
Epoch 004 | Time 0.0584(0.0584) | [  762/60000 (63%)] | Loss:   37.030483 |rec:  34.917053 | kl:   42.268559
Epoch 004 | Time 0.0583(0.0584) | [  782/60000 (65%)] | Loss:   36.861294 |rec:  34.779804 | kl:   41.629852
Epoch 004 | Time 0.0583(0.0584) | [  802/60000 (67%)] | Loss:   36.604309 |rec:  34.593025 | kl:   40.225670
Epoch 004 | Time 0.0583(0.0584) | [  822/60000 (68%)] | Loss:   34.540028 |rec:  32.521355 | kl:   40.373455
Epoch 004 | Time 0.0584(0.0584) | [  842/60000 (70%)] | Loss:   35.984146 |rec:  33.905910 | kl:   41.564754
Epoch 004 | Time 0.0584(0.0585) | [  862/60000 (72%)] | Loss:   36.804756 |rec:  34.686794 | kl:   42.359200
Epoch 004 | Time 0.0584(0.0585) | [  882/60000 (73%)] | Loss:   36.661366 |rec:  34.517727 | kl:   42.872738
Epoch 004 | Time 0.0584(0.0585) | [  902/60000 (75%)] | Loss:   36.208523 |rec:  34.136177 | kl:   41.446861
Epoch 004 | Time 0.0583(0.0585) | [  922/60000 (77%)] | Loss:   35.594151 |rec:  33.504166 | kl:   41.799671
Epoch 004 | Time 0.0583(0.0584) | [  942/60000 (78%)] | Loss:   34.213196 |rec:  32.198025 | kl:   40.303432
Epoch 004 | Time 0.0583(0.0584) | [  962/60000 (80%)] | Loss:   36.066711 |rec:  33.943291 | kl:   42.468445
Epoch 004 | Time 0.0584(0.0584) | [  982/60000 (82%)] | Loss:   36.728943 |rec:  34.621063 | kl:   42.157566
Epoch 004 | Time 0.0584(0.0584) | [ 1002/60000 (83%)] | Loss:   34.598995 |rec:  32.604275 | kl:   39.894447
Epoch 004 | Time 0.0584(0.0584) | [ 1022/60000 (85%)] | Loss:   36.619606 |rec:  34.557011 | kl:   41.251915
Epoch 004 | Time 0.0584(0.0585) | [ 1042/60000 (87%)] | Loss:   36.564392 |rec:  34.466164 | kl:   41.964565
Epoch 004 | Time 0.0583(0.0584) | [ 1062/60000 (88%)] | Loss:   34.261288 |rec:  32.258614 | kl:   40.053539
Epoch 004 | Time 0.0584(0.0584) | [ 1082/60000 (90%)] | Loss:   35.210987 |rec:  33.186069 | kl:   40.498344
Epoch 004 | Time 0.0584(0.0584) | [ 1102/60000 (92%)] | Loss:   35.950146 |rec:  33.850391 | kl:   41.995125
Epoch 004 | Time 0.0584(0.0585) | [ 1122/60000 (93%)] | Loss:   34.939182 |rec:  33.017750 | kl:   38.428646
Epoch 004 | Time 0.0584(0.0585) | [ 1142/60000 (95%)] | Loss:   35.270203 |rec:  33.227112 | kl:   40.861786
Epoch 004 | Time 0.0583(0.0585) | [ 1162/60000 (97%)] | Loss:   36.091976 |rec:  33.977417 | kl:   42.291172
Epoch 004 | Time 0.0584(0.0585) | [ 1182/60000 (98%)] | Loss:   35.605652 |rec:  33.501907 | kl:   42.074902
validating...
Epoch 0004 | Time 2.2174 | Loss 35.7571
Epoch: 5 	Beta: 0.060000000000000005
Epoch 005 | Time 0.0599(0.0585) | [    2/60000 ( 0%)] | Loss:   35.418179 |rec:  32.951736 | kl:   41.107338
Epoch 005 | Time 0.0582(0.0585) | [   22/60000 ( 2%)] | Loss:   36.143688 |rec:  33.726803 | kl:   40.281467
Epoch 005 | Time 0.0583(0.0585) | [   42/60000 ( 3%)] | Loss:   36.834469 |rec:  34.444237 | kl:   39.837147
Epoch 005 | Time 0.0583(0.0585) | [   62/60000 ( 5%)] | Loss:   36.067455 |rec:  33.605606 | kl:   41.030865
Epoch 005 | Time 0.0582(0.0584) | [   82/60000 ( 7%)] | Loss:   35.781372 |rec:  33.418026 | kl:   39.389145
Epoch 005 | Time 0.0582(0.0585) | [  102/60000 ( 8%)] | Loss:   36.129517 |rec:  33.766609 | kl:   39.381805
Epoch 005 | Time 0.0584(0.0584) | [  122/60000 (10%)] | Loss:   36.122131 |rec:  33.687817 | kl:   40.571922
Epoch 005 | Time 0.0583(0.0584) | [  142/60000 (12%)] | Loss:   35.351036 |rec:  32.887093 | kl:   41.065674
Epoch 005 | Time 0.0584(0.0584) | [  162/60000 (13%)] | Loss:   36.332520 |rec:  33.933502 | kl:   39.983597
Epoch 005 | Time 0.0583(0.0584) | [  182/60000 (15%)] | Loss:   37.781643 |rec:  35.424969 | kl:   39.277920
Epoch 005 | Time 0.0586(0.0584) | [  202/60000 (17%)] | Loss:   37.027454 |rec:  34.669094 | kl:   39.305954
Epoch 005 | Time 0.0582(0.0584) | [  222/60000 (18%)] | Loss:   37.503948 |rec:  35.049938 | kl:   40.900139
Epoch 005 | Time 0.0582(0.0584) | [  242/60000 (20%)] | Loss:   35.901085 |rec:  33.584003 | kl:   38.618011
Epoch 005 | Time 0.0583(0.0584) | [  262/60000 (22%)] | Loss:   35.326733 |rec:  32.993492 | kl:   38.887371
Epoch 005 | Time 0.0584(0.0584) | [  282/60000 (23%)] | Loss:   35.683857 |rec:  33.302979 | kl:   39.681259
Epoch 005 | Time 0.0583(0.0584) | [  302/60000 (25%)] | Loss:   35.745937 |rec:  33.328426 | kl:   40.291817
Epoch 005 | Time 0.0583(0.0584) | [  322/60000 (27%)] | Loss:   34.351379 |rec:  32.083023 | kl:   37.805958
Epoch 005 | Time 0.0583(0.0584) | [  342/60000 (28%)] | Loss:   36.376923 |rec:  33.900581 | kl:   41.272381
Epoch 005 | Time 0.0582(0.0584) | [  362/60000 (30%)] | Loss:   36.021889 |rec:  33.681744 | kl:   39.002426
Epoch 005 | Time 0.0582(0.0584) | [  382/60000 (32%)] | Loss:   35.299755 |rec:  32.925648 | kl:   39.568451
Epoch 005 | Time 0.0583(0.0584) | [  402/60000 (33%)] | Loss:   35.363232 |rec:  32.973160 | kl:   39.834560
Epoch 005 | Time 0.0583(0.0584) | [  422/60000 (35%)] | Loss:   36.446201 |rec:  34.148125 | kl:   38.301273
Epoch 005 | Time 0.0582(0.0584) | [  442/60000 (37%)] | Loss:   36.610928 |rec:  34.207661 | kl:   40.054493
Epoch 005 | Time 0.0583(0.0584) | [  462/60000 (38%)] | Loss:   36.357742 |rec:  34.039551 | kl:   38.636524
Epoch 005 | Time 0.0583(0.0584) | [  482/60000 (40%)] | Loss:   36.460693 |rec:  34.034729 | kl:   40.432686
Epoch 005 | Time 0.0583(0.0584) | [  502/60000 (42%)] | Loss:   36.665508 |rec:  34.307327 | kl:   39.302979
Epoch 005 | Time 0.0583(0.0584) | [  522/60000 (43%)] | Loss:   36.393978 |rec:  34.053028 | kl:   39.015831
Epoch 005 | Time 0.0583(0.0584) | [  542/60000 (45%)] | Loss:   36.989754 |rec:  34.645916 | kl:   39.063934
Epoch 005 | Time 0.0583(0.0584) | [  562/60000 (47%)] | Loss:   37.104595 |rec:  34.708157 | kl:   39.940647
Epoch 005 | Time 0.0584(0.0584) | [  582/60000 (48%)] | Loss:   35.644848 |rec:  33.278572 | kl:   39.437954
Epoch 005 | Time 0.0583(0.0584) | [  602/60000 (50%)] | Loss:   36.421196 |rec:  33.968014 | kl:   40.886337
Epoch 005 | Time 0.0584(0.0584) | [  622/60000 (52%)] | Loss:   36.528721 |rec:  34.187851 | kl:   39.014484
Epoch 005 | Time 0.0585(0.0584) | [  642/60000 (53%)] | Loss:   35.274097 |rec:  32.873474 | kl:   40.010334
Epoch 005 | Time 0.0584(0.0584) | [  662/60000 (55%)] | Loss:   35.952240 |rec:  33.626209 | kl:   38.767178
Epoch 005 | Time 0.0583(0.0584) | [  682/60000 (57%)] | Loss:   37.680679 |rec:  35.272972 | kl:   40.128452
Epoch 005 | Time 0.0581(0.0584) | [  702/60000 (58%)] | Loss:   36.113369 |rec:  33.737461 | kl:   39.598499
Epoch 005 | Time 0.0580(0.0584) | [  722/60000 (60%)] | Loss:   35.942715 |rec:  33.522526 | kl:   40.336517
Epoch 005 | Time 0.0581(0.0583) | [  742/60000 (62%)] | Loss:   37.246586 |rec:  34.752956 | kl:   41.560478
Epoch 005 | Time 0.0582(0.0583) | [  762/60000 (63%)] | Loss:   35.285263 |rec:  32.990665 | kl:   38.243271
Epoch 005 | Time 0.0581(0.0583) | [  782/60000 (65%)] | Loss:   33.969254 |rec:  31.629623 | kl:   38.993801
Epoch 005 | Time 0.0583(0.0584) | [  802/60000 (67%)] | Loss:   35.960083 |rec:  33.531136 | kl:   40.482479
Epoch 005 | Time 0.0582(0.0583) | [  822/60000 (68%)] | Loss:   34.637714 |rec:  32.295925 | kl:   39.029785
Epoch 005 | Time 0.0582(0.0583) | [  842/60000 (70%)] | Loss:   36.002682 |rec:  33.652798 | kl:   39.164688
Epoch 005 | Time 0.0582(0.0583) | [  862/60000 (72%)] | Loss:   35.182800 |rec:  32.872486 | kl:   38.505272
Epoch 005 | Time 0.0581(0.0583) | [  882/60000 (73%)] | Loss:   35.697632 |rec:  33.317886 | kl:   39.662457
Epoch 005 | Time 0.0584(0.0583) | [  902/60000 (75%)] | Loss:   33.805374 |rec:  31.520786 | kl:   38.076500
Epoch 005 | Time 0.0587(0.0584) | [  922/60000 (77%)] | Loss:   36.199192 |rec:  33.820511 | kl:   39.644733
Epoch 005 | Time 0.0583(0.0584) | [  942/60000 (78%)] | Loss:   35.459953 |rec:  33.093510 | kl:   39.440716
Epoch 005 | Time 0.0582(0.0584) | [  962/60000 (80%)] | Loss:   34.963821 |rec:  32.591190 | kl:   39.543823
Epoch 005 | Time 0.0582(0.0584) | [  982/60000 (82%)] | Loss:   35.746750 |rec:  33.426857 | kl:   38.664879
Epoch 005 | Time 0.0584(0.0584) | [ 1002/60000 (83%)] | Loss:   36.658463 |rec:  34.321201 | kl:   38.954319
Epoch 005 | Time 0.0582(0.0584) | [ 1022/60000 (85%)] | Loss:   35.468410 |rec:  33.094482 | kl:   39.565525
Epoch 005 | Time 0.0583(0.0584) | [ 1042/60000 (87%)] | Loss:   35.234432 |rec:  33.003735 | kl:   37.178291
Epoch 005 | Time 0.0584(0.0584) | [ 1062/60000 (88%)] | Loss:   35.635635 |rec:  33.324940 | kl:   38.511581
Epoch 005 | Time 0.0584(0.0584) | [ 1082/60000 (90%)] | Loss:   35.849239 |rec:  33.537045 | kl:   38.536587
Epoch 005 | Time 0.0583(0.0584) | [ 1102/60000 (92%)] | Loss:   35.995541 |rec:  33.616863 | kl:   39.644638
Epoch 005 | Time 0.0584(0.0584) | [ 1122/60000 (93%)] | Loss:   36.381802 |rec:  33.945492 | kl:   40.605171
Epoch 005 | Time 0.0582(0.0584) | [ 1142/60000 (95%)] | Loss:   35.829651 |rec:  33.425404 | kl:   40.070774
Epoch 005 | Time 0.0584(0.0584) | [ 1162/60000 (97%)] | Loss:   34.875278 |rec:  32.500847 | kl:   39.573860
Epoch 005 | Time 0.0583(0.0584) | [ 1182/60000 (98%)] | Loss:   35.875477 |rec:  33.577393 | kl:   38.301445
validating...
Epoch 0005 | Time 2.1589 | Loss 35.9618
Epoch: 6 	Beta: 0.07
Epoch 006 | Time 0.0599(0.0584) | [    2/60000 ( 0%)] | Loss:   38.053326 |rec:  35.141590 | kl:   41.596218
Epoch 006 | Time 0.0581(0.0585) | [   22/60000 ( 2%)] | Loss:   37.248436 |rec:  34.520439 | kl:   38.971397
Epoch 006 | Time 0.0583(0.0584) | [   42/60000 ( 3%)] | Loss:   36.899746 |rec:  34.232796 | kl:   38.099300
Epoch 006 | Time 0.0582(0.0584) | [   62/60000 ( 5%)] | Loss:   35.280544 |rec:  32.564449 | kl:   38.801418
Epoch 006 | Time 0.0582(0.0585) | [   82/60000 ( 7%)] | Loss:   36.242455 |rec:  33.594761 | kl:   37.824207
Epoch 006 | Time 0.0584(0.0585) | [  102/60000 ( 8%)] | Loss:   36.265671 |rec:  33.647865 | kl:   37.397217
Epoch 006 | Time 0.0582(0.0585) | [  122/60000 (10%)] | Loss:   37.214069 |rec:  34.453598 | kl:   39.435307
Epoch 006 | Time 0.0584(0.0584) | [  142/60000 (12%)] | Loss:   36.214676 |rec:  33.667774 | kl:   36.384296
Epoch 006 | Time 0.0583(0.0584) | [  162/60000 (13%)] | Loss:   36.562992 |rec:  33.856838 | kl:   38.659317
Epoch 006 | Time 0.0582(0.0584) | [  182/60000 (15%)] | Loss:   36.492455 |rec:  33.842896 | kl:   37.850868
Epoch 006 | Time 0.0585(0.0584) | [  202/60000 (17%)] | Loss:   35.684628 |rec:  33.006294 | kl:   38.261909
Epoch 006 | Time 0.0582(0.0584) | [  222/60000 (18%)] | Loss:   35.400532 |rec:  32.836929 | kl:   36.622921
Epoch 006 | Time 0.0584(0.0584) | [  242/60000 (20%)] | Loss:   36.394695 |rec:  33.734444 | kl:   38.003551
Epoch 006 | Time 0.0584(0.0584) | [  262/60000 (22%)] | Loss:   36.915272 |rec:  34.179668 | kl:   39.080059
Epoch 006 | Time 0.0582(0.0584) | [  282/60000 (23%)] | Loss:   36.608345 |rec:  33.922318 | kl:   38.371868
Epoch 006 | Time 0.0582(0.0584) | [  302/60000 (25%)] | Loss:   35.715363 |rec:  33.104710 | kl:   37.295029
Epoch 006 | Time 0.0584(0.0584) | [  322/60000 (27%)] | Loss:   35.624706 |rec:  32.998318 | kl:   37.519825
Epoch 006 | Time 0.0583(0.0584) | [  342/60000 (28%)] | Loss:   35.980495 |rec:  33.336033 | kl:   37.778011
Epoch 006 | Time 0.0581(0.0584) | [  362/60000 (30%)] | Loss:   35.662098 |rec:  33.205276 | kl:   35.097397
Epoch 006 | Time 0.0583(0.0584) | [  382/60000 (32%)] | Loss:   36.743233 |rec:  34.027348 | kl:   38.798325
Epoch 006 | Time 0.0584(0.0584) | [  402/60000 (33%)] | Loss:   36.964108 |rec:  34.250889 | kl:   38.760269
Epoch 006 | Time 0.0582(0.0584) | [  422/60000 (35%)] | Loss:   36.523899 |rec:  33.872135 | kl:   37.882313
Epoch 006 | Time 0.0581(0.0584) | [  442/60000 (37%)] | Loss:   35.565350 |rec:  33.000305 | kl:   36.643520
Epoch 006 | Time 0.0582(0.0584) | [  462/60000 (38%)] | Loss:   35.749046 |rec:  33.075291 | kl:   38.196476
Epoch 006 | Time 0.0582(0.0584) | [  482/60000 (40%)] | Loss:   35.473148 |rec:  32.863445 | kl:   37.281460
Epoch 006 | Time 0.0584(0.0584) | [  502/60000 (42%)] | Loss:   35.267200 |rec:  32.699276 | kl:   36.684631
Epoch 006 | Time 0.0584(0.0584) | [  522/60000 (43%)] | Loss:   36.024910 |rec:  33.457470 | kl:   36.677723
Epoch 006 | Time 0.0582(0.0584) | [  542/60000 (45%)] | Loss:   35.134441 |rec:  32.537086 | kl:   37.105087
Epoch 006 | Time 0.0584(0.0584) | [  562/60000 (47%)] | Loss:   36.606075 |rec:  33.864044 | kl:   39.171848
Epoch 006 | Time 0.0582(0.0584) | [  582/60000 (48%)] | Loss:   35.895008 |rec:  33.197559 | kl:   38.534988
Epoch 006 | Time 0.0584(0.0584) | [  602/60000 (50%)] | Loss:   37.877708 |rec:  35.204441 | kl:   38.189529
Epoch 006 | Time 0.0583(0.0584) | [  622/60000 (52%)] | Loss:   35.743954 |rec:  33.125069 | kl:   37.412636
Epoch 006 | Time 0.0583(0.0584) | [  642/60000 (53%)] | Loss:   36.593109 |rec:  33.867004 | kl:   38.944351
Epoch 006 | Time 0.0582(0.0584) | [  662/60000 (55%)] | Loss:   36.408558 |rec:  33.759827 | kl:   37.839024
Epoch 006 | Time 0.0581(0.0584) | [  682/60000 (57%)] | Loss:   37.378510 |rec:  34.642601 | kl:   39.084446
Epoch 006 | Time 0.0582(0.0584) | [  702/60000 (58%)] | Loss:   36.959110 |rec:  34.327305 | kl:   37.597210
Epoch 006 | Time 0.0584(0.0584) | [  722/60000 (60%)] | Loss:   35.861023 |rec:  33.285606 | kl:   36.791683
Epoch 006 | Time 0.0582(0.0584) | [  742/60000 (62%)] | Loss:   37.682259 |rec:  34.910656 | kl:   39.594360
Epoch 006 | Time 0.0582(0.0584) | [  762/60000 (63%)] | Loss:   34.555992 |rec:  32.016171 | kl:   36.283165
Epoch 006 | Time 0.0583(0.0584) | [  782/60000 (65%)] | Loss:   37.108063 |rec:  34.408409 | kl:   38.566463
Epoch 006 | Time 0.0581(0.0584) | [  802/60000 (67%)] | Loss:   36.392273 |rec:  33.652794 | kl:   39.135380
Epoch 006 | Time 0.0583(0.0584) | [  822/60000 (68%)] | Loss:   36.978428 |rec:  34.246807 | kl:   39.023178
Epoch 006 | Time 0.0584(0.0584) | [  842/60000 (70%)] | Loss:   35.517784 |rec:  32.846741 | kl:   38.157734
Epoch 006 | Time 0.0582(0.0584) | [  862/60000 (72%)] | Loss:   35.297146 |rec:  32.787163 | kl:   35.856949
Epoch 006 | Time 0.0582(0.0584) | [  882/60000 (73%)] | Loss:   36.093826 |rec:  33.476059 | kl:   37.396645
Epoch 006 | Time 0.0584(0.0584) | [  902/60000 (75%)] | Loss:   36.324722 |rec:  33.670898 | kl:   37.911777
Epoch 006 | Time 0.0582(0.0584) | [  922/60000 (77%)] | Loss:   36.775299 |rec:  34.137745 | kl:   37.679375
Epoch 006 | Time 0.0582(0.0584) | [  942/60000 (78%)] | Loss:   36.455692 |rec:  33.786476 | kl:   38.131676
Epoch 006 | Time 0.0584(0.0584) | [  962/60000 (80%)] | Loss:   36.059849 |rec:  33.412701 | kl:   37.816414
Epoch 006 | Time 0.0582(0.0584) | [  982/60000 (82%)] | Loss:   35.664276 |rec:  33.033577 | kl:   37.581406
Epoch 006 | Time 0.0584(0.0584) | [ 1002/60000 (83%)] | Loss:   35.313519 |rec:  32.733967 | kl:   36.850746
Epoch 006 | Time 0.0584(0.0584) | [ 1022/60000 (85%)] | Loss:   37.124763 |rec:  34.512039 | kl:   37.324646
Epoch 006 | Time 0.0583(0.0584) | [ 1042/60000 (87%)] | Loss:   35.953449 |rec:  33.259998 | kl:   38.477867
Epoch 006 | Time 0.0584(0.0584) | [ 1062/60000 (88%)] | Loss:   35.029610 |rec:  32.468475 | kl:   36.587620
Epoch 006 | Time 0.0605(0.0587) | [ 1082/60000 (90%)] | Loss:   35.569130 |rec:  32.946243 | kl:   37.469807
Epoch 006 | Time 0.0583(0.0592) | [ 1102/60000 (92%)] | Loss:   36.062031 |rec:  33.534229 | kl:   36.111454
Epoch 006 | Time 0.0623(0.0592) | [ 1122/60000 (93%)] | Loss:   35.824604 |rec:  33.049622 | kl:   39.642582
Epoch 006 | Time 0.0582(0.0599) | [ 1142/60000 (95%)] | Loss:   36.910221 |rec:  34.286366 | kl:   37.483658
Epoch 006 | Time 0.0639(0.0605) | [ 1162/60000 (97%)] | Loss:   35.830406 |rec:  33.256424 | kl:   36.771172
Epoch 006 | Time 0.0600(0.0606) | [ 1182/60000 (98%)] | Loss:   35.010670 |rec:  32.344440 | kl:   38.088978
validating...
Epoch 0006 | Time 2.3135 | Loss 35.9656
Epoch: 7 	Beta: 0.08
Epoch 007 | Time 0.0597(0.0601) | [    2/60000 ( 0%)] | Loss:   36.070267 |rec:  32.906292 | kl:   39.549675
Epoch 007 | Time 0.0584(0.0600) | [   22/60000 ( 2%)] | Loss:   38.135315 |rec:  35.174343 | kl:   37.012180
Epoch 007 | Time 0.0583(0.0600) | [   42/60000 ( 3%)] | Loss:   35.889065 |rec:  32.893970 | kl:   37.438690
Epoch 007 | Time 0.0635(0.0604) | [   62/60000 ( 5%)] | Loss:   36.545677 |rec:  33.709148 | kl:   35.456619
Epoch 007 | Time 0.0594(0.0611) | [   82/60000 ( 7%)] | Loss:   36.836590 |rec:  33.895115 | kl:   36.768421
Epoch 007 | Time 0.0583(0.0611) | [  102/60000 ( 8%)] | Loss:   35.403297 |rec:  32.316040 | kl:   38.590725
Epoch 007 | Time 0.0619(0.0612) | [  122/60000 (10%)] | Loss:   37.068192 |rec:  34.077820 | kl:   37.379601
Epoch 007 | Time 0.0583(0.0614) | [  142/60000 (12%)] | Loss:   36.001095 |rec:  33.173676 | kl:   35.342701
Epoch 007 | Time 0.0583(0.0612) | [  162/60000 (13%)] | Loss:   35.995762 |rec:  33.054745 | kl:   36.762737
Epoch 007 | Time 0.0637(0.0615) | [  182/60000 (15%)] | Loss:   36.886192 |rec:  33.963482 | kl:   36.533875
Epoch 007 | Time 0.0586(0.0608) | [  202/60000 (17%)] | Loss:   36.937992 |rec:  34.007763 | kl:   36.627876
Epoch 007 | Time 0.0582(0.0601) | [  222/60000 (18%)] | Loss:   35.277355 |rec:  32.417931 | kl:   35.742821
Epoch 007 | Time 0.0584(0.0597) | [  242/60000 (20%)] | Loss:   36.253418 |rec:  33.301773 | kl:   36.895580
Epoch 007 | Time 0.0594(0.0602) | [  262/60000 (22%)] | Loss:   36.944923 |rec:  33.982311 | kl:   37.032650
Epoch 007 | Time 0.0582(0.0598) | [  282/60000 (23%)] | Loss:   35.826450 |rec:  32.855347 | kl:   37.138836
Epoch 007 | Time 0.0581(0.0594) | [  302/60000 (25%)] | Loss:   36.484024 |rec:  33.473202 | kl:   37.635281
Epoch 007 | Time 0.0582(0.0591) | [  322/60000 (27%)] | Loss:   35.846973 |rec:  32.904842 | kl:   36.776615
Epoch 007 | Time 0.0582(0.0589) | [  342/60000 (28%)] | Loss:   36.582397 |rec:  33.663059 | kl:   36.491684
Epoch 007 | Time 0.0582(0.0587) | [  362/60000 (30%)] | Loss:   36.467342 |rec:  33.646023 | kl:   35.266487
Epoch 007 | Time 0.0582(0.0586) | [  382/60000 (32%)] | Loss:   37.282322 |rec:  34.283627 | kl:   37.483711
Epoch 007 | Time 0.0582(0.0585) | [  402/60000 (33%)] | Loss:   36.810074 |rec:  33.784370 | kl:   37.821281
Epoch 007 | Time 0.0582(0.0585) | [  422/60000 (35%)] | Loss:   34.285648 |rec:  31.498161 | kl:   34.843590
Epoch 007 | Time 0.0582(0.0584) | [  442/60000 (37%)] | Loss:   36.276672 |rec:  33.458424 | kl:   35.228153
Epoch 007 | Time 0.0581(0.0584) | [  462/60000 (38%)] | Loss:   36.664974 |rec:  33.689857 | kl:   37.188950
Epoch 007 | Time 0.0582(0.0583) | [  482/60000 (40%)] | Loss:   36.351780 |rec:  33.443611 | kl:   36.352112
Epoch 007 | Time 0.0582(0.0583) | [  502/60000 (42%)] | Loss:   35.833336 |rec:  33.025715 | kl:   35.095242
Epoch 007 | Time 0.0609(0.0585) | [  522/60000 (43%)] | Loss:   36.394577 |rec:  33.578091 | kl:   35.206055
Epoch 007 | Time 0.0620(0.0592) | [  542/60000 (45%)] | Loss:   35.684986 |rec:  32.849430 | kl:   35.444466
Epoch 007 | Time 0.0626(0.0601) | [  562/60000 (47%)] | Loss:   35.977879 |rec:  32.907471 | kl:   38.380093
Epoch 007 | Time 0.0618(0.0608) | [  582/60000 (48%)] | Loss:   34.545963 |rec:  31.731621 | kl:   35.179291
Epoch 007 | Time 0.0590(0.0609) | [  602/60000 (50%)] | Loss:   36.522137 |rec:  33.630875 | kl:   36.140774
Epoch 007 | Time 0.0594(0.0603) | [  622/60000 (52%)] | Loss:   36.364223 |rec:  33.382786 | kl:   37.267960
Epoch 007 | Time 0.0613(0.0605) | [  642/60000 (53%)] | Loss:   36.363579 |rec:  33.359222 | kl:   37.554443
Epoch 007 | Time 0.0602(0.0606) | [  662/60000 (55%)] | Loss:   36.449669 |rec:  33.464390 | kl:   37.315952
Epoch 007 | Time 0.0639(0.0607) | [  682/60000 (57%)] | Loss:   35.344021 |rec:  32.545883 | kl:   34.976738
Epoch 007 | Time 0.0625(0.0613) | [  702/60000 (58%)] | Loss:   36.582272 |rec:  33.628723 | kl:   36.919319
Epoch 007 | Time 0.0620(0.0618) | [  722/60000 (60%)] | Loss:   36.457211 |rec:  33.590870 | kl:   35.829254
Epoch 007 | Time 0.0627(0.0623) | [  742/60000 (62%)] | Loss:   34.974350 |rec:  32.033325 | kl:   36.762814
Epoch 007 | Time 0.0625(0.0625) | [  762/60000 (63%)] | Loss:   35.676067 |rec:  32.855629 | kl:   35.255505
Epoch 007 | Time 0.0628(0.0627) | [  782/60000 (65%)] | Loss:   36.068233 |rec:  33.163036 | kl:   36.314930
Epoch 007 | Time 0.0633(0.0629) | [  802/60000 (67%)] | Loss:   36.007359 |rec:  33.089657 | kl:   36.471279
Epoch 007 | Time 0.0627(0.0630) | [  822/60000 (68%)] | Loss:   34.248936 |rec:  31.414188 | kl:   35.434349
Epoch 007 | Time 0.0626(0.0630) | [  842/60000 (70%)] | Loss:   35.995049 |rec:  33.061615 | kl:   36.667908
Epoch 007 | Time 0.0626(0.0632) | [  862/60000 (72%)] | Loss:   35.389988 |rec:  32.623714 | kl:   34.578430
Epoch 007 | Time 0.0631(0.0634) | [  882/60000 (73%)] | Loss:   36.069973 |rec:  33.190357 | kl:   35.995235
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/snf/checkpt.pth', rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
Resuming at epoch 4 with args Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='snf', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/snf', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64).
Epoch: 4 	Beta: 0.01
Epoch 004 | Time 0.3520(0.3520) | [    2/60000 ( 0%)] | Loss:   34.886044 |rec:  34.444206 | kl:   44.183632
Epoch 004 | Time 0.0624(0.2761) | [   22/60000 ( 2%)] | Loss:   34.292583 |rec:  33.770142 | kl:   52.244221
Epoch 004 | Time 0.0632(0.2201) | [   42/60000 ( 3%)] | Loss:   35.657764 |rec:  35.112789 | kl:   54.497505
Epoch 004 | Time 0.0626(0.1786) | [   62/60000 ( 5%)] | Loss:   34.902725 |rec:  34.359764 | kl:   54.296127
Epoch 004 | Time 0.0623(0.1481) | [   82/60000 ( 7%)] | Loss:   33.724613 |rec:  33.155704 | kl:   56.890858
Epoch 004 | Time 0.0630(0.1256) | [  102/60000 ( 8%)] | Loss:   35.126911 |rec:  34.571877 | kl:   55.503452
Epoch 004 | Time 0.0643(0.1090) | [  122/60000 (10%)] | Loss:   33.673317 |rec:  33.101109 | kl:   57.220642
Epoch 004 | Time 0.0614(0.0968) | [  142/60000 (12%)] | Loss:   33.944328 |rec:  33.382607 | kl:   56.172054
Epoch 004 | Time 0.0625(0.0878) | [  162/60000 (13%)] | Loss:   33.330547 |rec:  32.746407 | kl:   58.414413
Epoch 004 | Time 0.0635(0.0814) | [  182/60000 (15%)] | Loss:   34.817371 |rec:  34.264442 | kl:   55.292850
Epoch 004 | Time 0.0632(0.0765) | [  202/60000 (17%)] | Loss:   35.658600 |rec:  35.087879 | kl:   57.071991
Epoch 004 | Time 0.0632(0.0730) | [  222/60000 (18%)] | Loss:   35.053429 |rec:  34.451958 | kl:   60.146885
Epoch 004 | Time 0.0633(0.0705) | [  242/60000 (20%)] | Loss:   34.478050 |rec:  33.866714 | kl:   61.133862
Epoch 004 | Time 0.0637(0.0687) | [  262/60000 (22%)] | Loss:   34.179543 |rec:  33.580627 | kl:   59.891590
Epoch 004 | Time 0.0630(0.0673) | [  282/60000 (23%)] | Loss:   33.752102 |rec:  33.157742 | kl:   59.435860
Epoch 004 | Time 0.0618(0.0661) | [  302/60000 (25%)] | Loss:   31.241535 |rec:  30.665192 | kl:   57.634335
Epoch 004 | Time 0.0641(0.0654) | [  322/60000 (27%)] | Loss:   34.871090 |rec:  34.278378 | kl:   59.271320
Epoch 004 | Time 0.0630(0.0648) | [  342/60000 (28%)] | Loss:   33.975967 |rec:  33.380211 | kl:   59.575630
Epoch 004 | Time 0.0668(0.0646) | [  362/60000 (30%)] | Loss:   32.592552 |rec:  32.000938 | kl:   59.161335
Epoch 004 | Time 0.0684(0.0652) | [  382/60000 (32%)] | Loss:   33.046608 |rec:  32.444328 | kl:   60.228085
Epoch 004 | Time 0.0624(0.0647) | [  402/60000 (33%)] | Loss:   34.528107 |rec:  33.917686 | kl:   61.041973
Epoch 004 | Time 0.0622(0.0642) | [  422/60000 (35%)] | Loss:   34.895138 |rec:  34.286766 | kl:   60.837181
Epoch 004 | Time 0.0637(0.0640) | [  442/60000 (37%)] | Loss:   34.632950 |rec:  34.000469 | kl:   63.247929
Epoch 004 | Time 0.0638(0.0637) | [  462/60000 (38%)] | Loss:   33.740498 |rec:  33.112312 | kl:   62.818474
Epoch 004 | Time 0.0630(0.0637) | [  482/60000 (40%)] | Loss:   34.173172 |rec:  33.532360 | kl:   64.081306
Epoch 004 | Time 0.0640(0.0636) | [  502/60000 (42%)] | Loss:   34.665836 |rec:  34.065189 | kl:   60.064732
Epoch 004 | Time 0.0624(0.0633) | [  522/60000 (43%)] | Loss:   34.508865 |rec:  33.894936 | kl:   61.392986
Epoch 004 | Time 0.0637(0.0632) | [  542/60000 (45%)] | Loss:   33.119160 |rec:  32.506405 | kl:   61.275616
Epoch 004 | Time 0.0635(0.0631) | [  562/60000 (47%)] | Loss:   33.147083 |rec:  32.514431 | kl:   63.264851
Epoch 004 | Time 0.0611(0.0630) | [  582/60000 (48%)] | Loss:   33.105328 |rec:  32.494583 | kl:   61.074451
Epoch 004 | Time 0.0626(0.0628) | [  602/60000 (50%)] | Loss:   33.181000 |rec:  32.575096 | kl:   60.590313
Epoch 004 | Time 0.0615(0.0627) | [  622/60000 (52%)] | Loss:   33.882050 |rec:  33.275536 | kl:   60.651482
Epoch 004 | Time 0.0625(0.0626) | [  642/60000 (53%)] | Loss:   34.407310 |rec:  33.764538 | kl:   64.277359
Epoch 004 | Time 0.0627(0.0626) | [  662/60000 (55%)] | Loss:   34.558468 |rec:  33.935738 | kl:   62.273232
Epoch 004 | Time 0.0614(0.0626) | [  682/60000 (57%)] | Loss:   34.025074 |rec:  33.412464 | kl:   61.261032
Epoch 004 | Time 0.0636(0.0626) | [  702/60000 (58%)] | Loss:   34.670300 |rec:  34.056335 | kl:   61.396152
Epoch 004 | Time 0.0628(0.0626) | [  722/60000 (60%)] | Loss:   34.542175 |rec:  33.923382 | kl:   61.879570
Epoch 004 | Time 0.0625(0.0626) | [  742/60000 (62%)] | Loss:   35.131470 |rec:  34.501465 | kl:   63.000134
Epoch 004 | Time 0.0626(0.0626) | [  762/60000 (63%)] | Loss:   35.136139 |rec:  34.497288 | kl:   63.885010
Epoch 004 | Time 0.0633(0.0628) | [  782/60000 (65%)] | Loss:   33.401653 |rec:  32.781498 | kl:   62.015362
Epoch 004 | Time 0.0647(0.0629) | [  802/60000 (67%)] | Loss:   34.602516 |rec:  33.952000 | kl:   65.051582
Epoch 004 | Time 0.0669(0.0631) | [  822/60000 (68%)] | Loss:   34.226971 |rec:  33.590496 | kl:   63.647549
Epoch 004 | Time 0.0621(0.0635) | [  842/60000 (70%)] | Loss:   33.164249 |rec:  32.537540 | kl:   62.670975
Epoch 004 | Time 0.0629(0.0633) | [  862/60000 (72%)] | Loss:   34.596523 |rec:  33.984436 | kl:   61.208588
Epoch 004 | Time 0.0630(0.0631) | [  882/60000 (73%)] | Loss:   34.684410 |rec:  34.064049 | kl:   62.036236
Epoch 004 | Time 0.0619(0.0630) | [  902/60000 (75%)] | Loss:   33.830929 |rec:  33.204552 | kl:   62.637341
Epoch 004 | Time 0.0627(0.0629) | [  922/60000 (77%)] | Loss:   35.195713 |rec:  34.544655 | kl:   65.106003
Epoch 004 | Time 0.0623(0.0629) | [  942/60000 (78%)] | Loss:   33.437962 |rec:  32.789562 | kl:   64.840172
Epoch 004 | Time 0.0633(0.0630) | [  962/60000 (80%)] | Loss:   33.836849 |rec:  33.209442 | kl:   62.740673
Epoch 004 | Time 0.0623(0.0631) | [  982/60000 (82%)] | Loss:   33.463581 |rec:  32.843563 | kl:   62.001675
Epoch 004 | Time 0.0615(0.0629) | [ 1002/60000 (83%)] | Loss:   33.670261 |rec:  33.032173 | kl:   63.808876
Epoch 004 | Time 0.0629(0.0629) | [ 1022/60000 (85%)] | Loss:   34.527073 |rec:  33.885288 | kl:   64.178513
Epoch 004 | Time 0.0614(0.0629) | [ 1042/60000 (87%)] | Loss:   34.178024 |rec:  33.523766 | kl:   65.425865
Epoch 004 | Time 0.0619(0.0629) | [ 1062/60000 (88%)] | Loss:   34.629734 |rec:  34.005333 | kl:   62.440510
Epoch 004 | Time 0.0628(0.0628) | [ 1082/60000 (90%)] | Loss:   33.607906 |rec:  32.974594 | kl:   63.331268
Epoch 004 | Time 0.0611(0.0626) | [ 1102/60000 (92%)] | Loss:   34.947269 |rec:  34.316387 | kl:   63.088238
Epoch 004 | Time 0.0625(0.0626) | [ 1122/60000 (93%)] | Loss:   34.724361 |rec:  34.069675 | kl:   65.468399
Epoch 004 | Time 0.0635(0.0628) | [ 1142/60000 (95%)] | Loss:   32.972366 |rec:  32.351643 | kl:   62.072407
Epoch 004 | Time 0.0614(0.0630) | [ 1162/60000 (97%)] | Loss:   34.088268 |rec:  33.442055 | kl:   64.621277
Epoch 004 | Time 0.0639(0.0630) | [ 1182/60000 (98%)] | Loss:   33.806244 |rec:  33.204659 | kl:   60.158474
validating...
Epoch 0004 | Time 2.4180 | Loss 33.8243
Epoch: 5 	Beta: 0.02
Epoch 005 | Time 0.0642(0.0630) | [    2/60000 ( 0%)] | Loss:   34.704819 |rec:  33.420265 | kl:   64.227608
Epoch 005 | Time 0.0638(0.0631) | [   22/60000 ( 2%)] | Loss:   33.382366 |rec:  32.196407 | kl:   59.298016
Epoch 005 | Time 0.0626(0.0632) | [   42/60000 ( 3%)] | Loss:   33.365547 |rec:  32.224655 | kl:   57.044617
Epoch 005 | Time 0.0616(0.0631) | [   62/60000 ( 5%)] | Loss:   34.019287 |rec:  32.805103 | kl:   60.709229
Epoch 005 | Time 0.0621(0.0630) | [   82/60000 ( 7%)] | Loss:   34.475140 |rec:  33.362202 | kl:   55.646870
Epoch 005 | Time 0.0631(0.0630) | [  102/60000 ( 8%)] | Loss:   34.018509 |rec:  32.878441 | kl:   57.003578
Epoch 005 | Time 0.0637(0.0631) | [  122/60000 (10%)] | Loss:   34.391197 |rec:  33.260162 | kl:   56.551704
Epoch 005 | Time 0.0617(0.0631) | [  142/60000 (12%)] | Loss:   32.370285 |rec:  31.293613 | kl:   53.833599
Epoch 005 | Time 0.0632(0.0631) | [  162/60000 (13%)] | Loss:   34.450588 |rec:  33.331810 | kl:   55.939022
Epoch 005 | Time 0.0640(0.0631) | [  182/60000 (15%)] | Loss:   33.882565 |rec:  32.738392 | kl:   57.208641
Epoch 005 | Time 0.0629(0.0633) | [  202/60000 (17%)] | Loss:   34.758572 |rec:  33.654598 | kl:   55.198654
Epoch 005 | Time 0.0621(0.0632) | [  222/60000 (18%)] | Loss:   32.688576 |rec:  31.605864 | kl:   54.135578
Epoch 005 | Time 0.0629(0.0631) | [  242/60000 (20%)] | Loss:   33.772732 |rec:  32.665653 | kl:   55.353832
Epoch 005 | Time 0.0635(0.0632) | [  262/60000 (22%)] | Loss:   34.815918 |rec:  33.667370 | kl:   57.427517
Epoch 005 | Time 0.0624(0.0631) | [  282/60000 (23%)] | Loss:   34.614964 |rec:  33.502552 | kl:   55.620510
Epoch 005 | Time 0.0629(0.0630) | [  302/60000 (25%)] | Loss:   33.856289 |rec:  32.730225 | kl:   56.303123
Epoch 005 | Time 0.0634(0.0630) | [  322/60000 (27%)] | Loss:   34.739849 |rec:  33.627056 | kl:   55.639729
Epoch 005 | Time 0.0628(0.0631) | [  342/60000 (28%)] | Loss:   35.299290 |rec:  34.232441 | kl:   53.342522
Epoch 005 | Time 0.0611(0.0628) | [  362/60000 (30%)] | Loss:   33.807087 |rec:  32.726894 | kl:   54.009502
Epoch 005 | Time 0.0639(0.0629) | [  382/60000 (32%)] | Loss:   33.585983 |rec:  32.518333 | kl:   53.382504
Epoch 005 | Time 0.0643(0.0631) | [  402/60000 (33%)] | Loss:   33.739422 |rec:  32.632797 | kl:   55.331284
Epoch 005 | Time 0.0628(0.0630) | [  422/60000 (35%)] | Loss:   34.581478 |rec:  33.501080 | kl:   54.019817
Epoch 005 | Time 0.0620(0.0628) | [  442/60000 (37%)] | Loss:   35.585968 |rec:  34.449741 | kl:   56.811436
Epoch 005 | Time 0.0618(0.0627) | [  462/60000 (38%)] | Loss:   34.556492 |rec:  33.438107 | kl:   55.919479
Epoch 005 | Time 0.0631(0.0626) | [  482/60000 (40%)] | Loss:   34.322624 |rec:  33.198769 | kl:   56.192703
Epoch 005 | Time 0.0622(0.0626) | [  502/60000 (42%)] | Loss:   32.557930 |rec:  31.486603 | kl:   53.566463
Epoch 005 | Time 0.0609(0.0625) | [  522/60000 (43%)] | Loss:   33.158421 |rec:  32.069569 | kl:   54.442566
Epoch 005 | Time 0.0630(0.0625) | [  542/60000 (45%)] | Loss:   34.084316 |rec:  33.006485 | kl:   53.891541
Epoch 005 | Time 0.0628(0.0624) | [  562/60000 (47%)] | Loss:   34.084217 |rec:  32.994007 | kl:   54.510448
Epoch 005 | Time 0.0614(0.0624) | [  582/60000 (48%)] | Loss:   33.258198 |rec:  32.168289 | kl:   54.495396
Epoch 005 | Time 0.0627(0.0625) | [  602/60000 (50%)] | Loss:   34.329922 |rec:  33.185219 | kl:   57.235153
Epoch 005 | Time 0.0624(0.0624) | [  622/60000 (52%)] | Loss:   33.395321 |rec:  32.291611 | kl:   55.185467
Epoch 005 | Time 0.0616(0.0625) | [  642/60000 (53%)] | Loss:   32.621582 |rec:  31.533039 | kl:   54.427097
Epoch 005 | Time 0.0627(0.0624) | [  662/60000 (55%)] | Loss:   34.263729 |rec:  33.207577 | kl:   52.807713
Epoch 005 | Time 0.0625(0.0625) | [  682/60000 (57%)] | Loss:   33.963245 |rec:  32.856411 | kl:   55.341629
Epoch 005 | Time 0.0615(0.0625) | [  702/60000 (58%)] | Loss:   34.824154 |rec:  33.723961 | kl:   55.009769
Epoch 005 | Time 0.0631(0.0625) | [  722/60000 (60%)] | Loss:   34.016304 |rec:  32.954227 | kl:   53.103783
Epoch 005 | Time 0.0616(0.0626) | [  742/60000 (62%)] | Loss:   34.089439 |rec:  32.984829 | kl:   55.230469
Epoch 005 | Time 0.0636(0.0626) | [  762/60000 (63%)] | Loss:   34.721943 |rec:  33.680523 | kl:   52.071018
Epoch 005 | Time 0.0643(0.0630) | [  782/60000 (65%)] | Loss:   33.543663 |rec:  32.484764 | kl:   52.944939
Epoch 005 | Time 0.0636(0.0630) | [  802/60000 (67%)] | Loss:   33.678776 |rec:  32.613579 | kl:   53.259861
Epoch 005 | Time 0.0616(0.0628) | [  822/60000 (68%)] | Loss:   33.336174 |rec:  32.272545 | kl:   53.181347
Epoch 005 | Time 0.0637(0.0629) | [  842/60000 (70%)] | Loss:   32.567818 |rec:  31.510725 | kl:   52.854668
Epoch 005 | Time 0.0628(0.0631) | [  862/60000 (72%)] | Loss:   34.205631 |rec:  33.131283 | kl:   53.717548
Epoch 005 | Time 0.0618(0.0629) | [  882/60000 (73%)] | Loss:   33.806423 |rec:  32.734978 | kl:   53.572285
Epoch 005 | Time 0.0620(0.0627) | [  902/60000 (75%)] | Loss:   33.260303 |rec:  32.136784 | kl:   56.175941
Epoch 005 | Time 0.0626(0.0627) | [  922/60000 (77%)] | Loss:   34.365955 |rec:  33.268448 | kl:   54.875286
Epoch 005 | Time 0.0626(0.0626) | [  942/60000 (78%)] | Loss:   34.110733 |rec:  33.008244 | kl:   55.124481
Epoch 005 | Time 0.0624(0.0626) | [  962/60000 (80%)] | Loss:   34.310635 |rec:  33.233833 | kl:   53.840115
Epoch 005 | Time 0.0636(0.0626) | [  982/60000 (82%)] | Loss:   33.502193 |rec:  32.412659 | kl:   54.476608
Epoch 005 | Time 0.0624(0.0626) | [ 1002/60000 (83%)] | Loss:   34.145546 |rec:  33.084740 | kl:   53.040264
Epoch 005 | Time 0.0616(0.0625) | [ 1022/60000 (85%)] | Loss:   33.779312 |rec:  32.695736 | kl:   54.178825
