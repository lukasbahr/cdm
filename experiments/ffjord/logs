/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.3047(0.3047) | Bit/dim 27.5632(27.5632) | Steps 41(41.00) | Grad Norm 16.7291(16.7291) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1207(0.2567) | Bit/dim 27.6788(27.6096) | Steps 41(41.00) | Grad Norm 16.8898(16.8251) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1206(0.2211) | Bit/dim 27.7206(27.6212) | Steps 41(41.00) | Grad Norm 17.1221(16.8923) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1207(0.1948) | Bit/dim 27.5115(27.5896) | Steps 41(41.00) | Grad Norm 16.9362(16.9007) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1207(0.1755) | Bit/dim 27.1948(27.4986) | Steps 41(41.00) | Grad Norm 17.0779(16.8747) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1215(0.1612) | Bit/dim 26.7755(27.3523) | Steps 41(41.00) | Grad Norm 16.8367(16.8463) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1205(0.1507) | Bit/dim 26.3844(27.1327) | Steps 41(41.00) | Grad Norm 16.8717(16.8445) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1206(0.1429) | Bit/dim 25.6758(26.8253) | Steps 41(41.00) | Grad Norm 17.8760(16.9647) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1218(0.1371) | Bit/dim 24.7824(26.3843) | Steps 41(41.00) | Grad Norm 18.8084(17.2930) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1207(0.1329) | Bit/dim 23.3689(25.7482) | Steps 41(41.00) | Grad Norm 22.4442(18.1780) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.3030(0.3030) | Bit/dim 29.1280(29.1280) | Steps 41(41.00) | Grad Norm 16.2135(16.2135) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1247(0.2558) | Bit/dim 29.1584(29.1116) | Steps 41(41.00) | Grad Norm 16.1206(16.1784) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1231(0.2211) | Bit/dim 28.9918(29.0771) | Steps 41(41.00) | Grad Norm 16.1415(16.1418) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 2.4318, Bit/dim 28.9667
Iter 0030 | Time 0.1219(0.1952) | Bit/dim 29.0118(29.0324) | Steps 41(41.00) | Grad Norm 16.1213(16.0973) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1235(0.1760) | Bit/dim 28.7303(28.9685) | Steps 41(41.00) | Grad Norm 15.9841(16.0501) | Total Time 1.00(1.00)
validating...
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.2993(0.2993) | Bit/dim 25.1173(25.1173) | Steps 41(41.00) | Grad Norm 14.2259(14.2259) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1240(0.2531) | Bit/dim 25.0699(25.0928) | Steps 41(41.00) | Grad Norm 14.3360(14.2443) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1203(0.2183) | Bit/dim 24.9830(25.0673) | Steps 41(41.00) | Grad Norm 14.4172(14.2553) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1203(0.1927) | Bit/dim 24.8652(25.0367) | Steps 41(41.00) | Grad Norm 14.5629(14.3004) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1209(0.1738) | Bit/dim 24.7446(24.9505) | Steps 41(41.00) | Grad Norm 14.6708(14.3344) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1203(0.1598) | Bit/dim 24.4449(24.8324) | Steps 41(41.00) | Grad Norm 15.1146(14.4527) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1217(0.1496) | Bit/dim 24.0533(24.6541) | Steps 41(41.00) | Grad Norm 15.5201(14.6272) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1205(0.1420) | Bit/dim 23.4536(24.4073) | Steps 41(41.00) | Grad Norm 17.0871(15.0520) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1203(0.1364) | Bit/dim 22.5630(24.0392) | Steps 41(41.00) | Grad Norm 19.4195(15.8370) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1204(0.1323) | Bit/dim 21.0956(23.4427) | Steps 41(41.00) | Grad Norm 22.7849(17.2052) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1203(0.1292) | Bit/dim 19.3934(22.5767) | Steps 41(41.00) | Grad Norm 23.9467(18.8410) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1443(0.1309) | Bit/dim 18.0355(21.5574) | Steps 47(41.98) | Grad Norm 24.7512(20.1981) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1446(0.1344) | Bit/dim 16.9135(20.4256) | Steps 47(43.30) | Grad Norm 24.2570(21.5116) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1445(0.1371) | Bit/dim 15.6298(19.2853) | Steps 47(44.27) | Grad Norm 23.0089(22.0977) | Total Time 1.00(1.00)
Iter 0140 | Time 0.2029(0.1508) | Bit/dim 14.3411(18.1241) | Steps 65(48.87) | Grad Norm 22.1849(22.2154) | Total Time 1.00(1.00)
Iter 0150 | Time 0.2036(0.1645) | Bit/dim 13.2036(17.0036) | Steps 65(53.10) | Grad Norm 20.5644(21.8978) | Total Time 1.00(1.00)
Iter 0160 | Time 0.2047(0.1750) | Bit/dim 11.8532(15.8676) | Steps 65(56.23) | Grad Norm 18.8156(21.2179) | Total Time 1.00(1.00)
Iter 0170 | Time 0.2201(0.1854) | Bit/dim 11.1624(14.7490) | Steps 71(59.22) | Grad Norm 15.8529(20.0982) | Total Time 1.00(1.00)
Iter 0180 | Time 0.2209(0.1944) | Bit/dim 10.4341(13.6777) | Steps 71(62.31) | Grad Norm 13.4482(18.6300) | Total Time 1.00(1.00)
Iter 0190 | Time 0.2626(0.2136) | Bit/dim 9.5289(12.6592) | Steps 83(67.74) | Grad Norm 11.6200(16.9981) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2628(0.2265) | Bit/dim 8.5733(11.6351) | Steps 83(71.75) | Grad Norm 10.3213(15.3972) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2465(0.2335) | Bit/dim 7.5348(10.6794) | Steps 83(74.86) | Grad Norm 8.7703(13.8339) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2467(0.2370) | Bit/dim 7.3416(9.8065) | Steps 83(77.00) | Grad Norm 7.4178(12.2901) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2466(0.2395) | Bit/dim 6.6876(9.0437) | Steps 83(78.57) | Grad Norm 6.3101(10.8369) | Total Time 1.00(1.00)
Iter 0240 | Time 0.3054(0.2537) | Bit/dim 6.1898(8.3508) | Steps 101(83.32) | Grad Norm 5.4018(9.5239) | Total Time 1.00(1.00)
Iter 0250 | Time 0.3055(0.2673) | Bit/dim 5.8602(7.7780) | Steps 101(87.97) | Grad Norm 4.9417(8.3829) | Total Time 1.00(1.00)
Iter 0260 | Time 0.3054(0.2774) | Bit/dim 5.9598(7.2937) | Steps 101(91.39) | Grad Norm 4.7463(7.4310) | Total Time 1.00(1.00)
Iter 0270 | Time 0.3055(0.2841) | Bit/dim 5.6702(6.8653) | Steps 101(93.74) | Grad Norm 4.4327(6.6540) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3069(0.2892) | Bit/dim 5.4309(6.4918) | Steps 101(95.50) | Grad Norm 4.1382(6.0154) | Total Time 1.00(1.00)
Iter 0290 | Time 0.3069(0.2914) | Bit/dim 5.3493(6.1802) | Steps 101(96.33) | Grad Norm 4.2279(5.5397) | Total Time 1.00(1.00)
Iter 0300 | Time 0.3067(0.2904) | Bit/dim 5.0773(5.8964) | Steps 101(96.30) | Grad Norm 4.3690(5.1749) | Total Time 1.00(1.00)
Iter 0310 | Time 0.2904(0.2886) | Bit/dim 4.8387(5.6512) | Steps 101(96.14) | Grad Norm 4.2340(4.9357) | Total Time 1.00(1.00)
Iter 0320 | Time 0.2904(0.2896) | Bit/dim 4.8612(5.4168) | Steps 101(97.41) | Grad Norm 4.6826(4.7891) | Total Time 1.00(1.00)
Iter 0330 | Time 0.2902(0.2899) | Bit/dim 4.4246(5.1992) | Steps 101(98.36) | Grad Norm 4.4140(4.7055) | Total Time 1.00(1.00)
Iter 0340 | Time 0.2975(0.2909) | Bit/dim 4.2824(4.9859) | Steps 101(99.05) | Grad Norm 4.4708(4.6642) | Total Time 1.00(1.00)
Iter 0350 | Time 0.3473(0.2949) | Bit/dim 4.0545(4.7731) | Steps 113(100.09) | Grad Norm 4.5117(4.6421) | Total Time 1.00(1.00)
Iter 0360 | Time 0.3512(0.3069) | Bit/dim 3.9022(4.5766) | Steps 113(102.89) | Grad Norm 4.0716(4.5701) | Total Time 1.00(1.00)
Iter 0370 | Time 0.3447(0.3161) | Bit/dim 3.8494(4.3917) | Steps 113(105.55) | Grad Norm 3.7378(4.3629) | Total Time 1.00(1.00)
Iter 0380 | Time 0.3397(0.3224) | Bit/dim 3.6270(4.2075) | Steps 113(107.50) | Grad Norm 3.4153(4.0739) | Total Time 1.00(1.00)
Iter 0390 | Time 0.3394(0.3269) | Bit/dim 3.5769(4.0439) | Steps 113(108.95) | Grad Norm 3.0891(3.7850) | Total Time 1.00(1.00)
Iter 0400 | Time 0.3509(0.3320) | Bit/dim 3.4621(3.9013) | Steps 113(110.18) | Grad Norm 3.0532(3.4731) | Total Time 1.00(1.00)
Iter 0410 | Time 0.3478(0.3359) | Bit/dim 3.1871(3.7702) | Steps 119(112.22) | Grad Norm 2.8548(3.3149) | Total Time 1.00(1.00)
Iter 0420 | Time 0.3475(0.3406) | Bit/dim 3.3651(3.6567) | Steps 119(114.00) | Grad Norm 3.3957(3.1859) | Total Time 1.00(1.00)
Iter 0430 | Time 0.3479(0.3425) | Bit/dim 3.2720(3.5579) | Steps 119(115.31) | Grad Norm 2.1065(3.1364) | Total Time 1.00(1.00)
Iter 0440 | Time 0.3491(0.3464) | Bit/dim 3.2077(3.4729) | Steps 119(116.28) | Grad Norm 4.1827(3.1007) | Total Time 1.00(1.00)
Iter 0450 | Time 0.3969(0.3568) | Bit/dim 3.0872(3.3847) | Steps 119(116.99) | Grad Norm 3.0721(3.0478) | Total Time 1.00(1.00)
Iter 0460 | Time 0.4492(0.3638) | Bit/dim 3.1573(3.3134) | Steps 119(117.52) | Grad Norm 5.7865(3.1343) | Total Time 1.00(1.00)
Iter 0470 | Time 0.3617(0.3680) | Bit/dim 2.9915(3.2460) | Steps 119(117.91) | Grad Norm 2.1525(3.3178) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3591(0.3674) | Bit/dim 3.0510(3.1864) | Steps 119(118.20) | Grad Norm 3.1499(3.3143) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3612(0.3676) | Bit/dim 2.8902(3.1324) | Steps 119(118.41) | Grad Norm 3.3371(3.2160) | Total Time 1.00(1.00)
Iter 0500 | Time 0.3589(0.3651) | Bit/dim 2.8970(3.0887) | Steps 119(118.56) | Grad Norm 2.3283(3.1291) | Total Time 1.00(1.00)
Iter 0510 | Time 0.3500(0.3634) | Bit/dim 2.9249(3.0453) | Steps 119(118.68) | Grad Norm 2.3960(2.9286) | Total Time 1.00(1.00)
Iter 0520 | Time 0.3516(0.3606) | Bit/dim 2.8378(2.9979) | Steps 119(118.76) | Grad Norm 1.6033(2.7701) | Total Time 1.00(1.00)
Iter 0530 | Time 0.3638(0.3600) | Bit/dim 2.8054(2.9569) | Steps 119(118.32) | Grad Norm 2.8232(2.8578) | Total Time 1.00(1.00)
Iter 0540 | Time 0.4010(0.3599) | Bit/dim 2.8075(2.9239) | Steps 113(117.36) | Grad Norm 3.6532(3.0778) | Total Time 1.00(1.00)
Iter 0550 | Time 0.3506(0.3615) | Bit/dim 2.7740(2.8915) | Steps 113(116.21) | Grad Norm 4.1700(3.3668) | Total Time 1.00(1.00)
Iter 0560 | Time 0.4050(0.3696) | Bit/dim 2.7271(2.8587) | Steps 113(115.37) | Grad Norm 4.9333(3.7191) | Total Time 1.00(1.00)
Iter 0570 | Time 0.3624(0.3661) | Bit/dim 2.6718(2.8295) | Steps 113(114.75) | Grad Norm 2.6010(3.6141) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3553(0.3645) | Bit/dim 2.7396(2.8065) | Steps 113(114.29) | Grad Norm 2.9828(3.5374) | Total Time 1.00(1.00)
Iter 0590 | Time 0.3525(0.3632) | Bit/dim 2.7809(2.7820) | Steps 113(113.95) | Grad Norm 2.0794(3.3407) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 4.8406, Bit/dim 2.6603
Iter 0600 | Time 0.3683(0.3624) | Bit/dim 2.6855(2.7599) | Steps 113(113.70) | Grad Norm 2.1716(3.0842) | Total Time 1.00(1.00)
Iter 0610 | Time 0.3515(0.3594) | Bit/dim 2.5903(2.7374) | Steps 113(113.52) | Grad Norm 3.3518(2.9667) | Total Time 1.00(1.00)
Iter 0620 | Time 0.3547(0.3595) | Bit/dim 2.7495(2.7174) | Steps 113(113.38) | Grad Norm 3.1332(2.9210) | Total Time 1.00(1.00)
Iter 0630 | Time 0.3494(0.3591) | Bit/dim 2.6100(2.6944) | Steps 113(113.28) | Grad Norm 4.5402(2.9433) | Total Time 1.00(1.00)
Iter 0640 | Time 0.3427(0.3556) | Bit/dim 2.6511(2.6804) | Steps 113(113.04) | Grad Norm 2.0807(2.9791) | Total Time 1.00(1.00)
Iter 0650 | Time 0.3427(0.3491) | Bit/dim 2.5820(2.6632) | Steps 113(112.25) | Grad Norm 4.7401(3.3108) | Total Time 1.00(1.00)
Iter 0660 | Time 0.3186(0.3423) | Bit/dim 2.6401(2.6498) | Steps 107(111.18) | Grad Norm 3.0439(3.3487) | Total Time 1.00(1.00)
Iter 0670 | Time 0.3186(0.3377) | Bit/dim 2.5571(2.6306) | Steps 107(110.08) | Grad Norm 4.6659(3.2429) | Total Time 1.00(1.00)
Iter 0680 | Time 0.3184(0.3326) | Bit/dim 2.5649(2.6136) | Steps 107(109.27) | Grad Norm 4.6938(3.4556) | Total Time 1.00(1.00)
Iter 0690 | Time 0.3183(0.3289) | Bit/dim 2.4987(2.5922) | Steps 107(108.68) | Grad Norm 2.0496(3.3635) | Total Time 1.00(1.00)
Iter 0700 | Time 0.3313(0.3273) | Bit/dim 2.6175(2.5772) | Steps 107(108.24) | Grad Norm 2.8287(3.3296) | Total Time 1.00(1.00)
Iter 0710 | Time 0.3261(0.3268) | Bit/dim 2.4894(2.5597) | Steps 107(107.91) | Grad Norm 4.4036(3.5753) | Total Time 1.00(1.00)
Iter 0720 | Time 0.3325(0.3271) | Bit/dim 2.4564(2.5429) | Steps 107(107.67) | Grad Norm 3.7999(3.5042) | Total Time 1.00(1.00)
Iter 0730 | Time 0.3170(0.3244) | Bit/dim 2.5676(2.5309) | Steps 107(107.50) | Grad Norm 6.7433(4.0904) | Total Time 1.00(1.00)
Iter 0740 | Time 0.3310(0.3261) | Bit/dim 2.4480(2.5173) | Steps 107(107.37) | Grad Norm 6.1679(4.3779) | Total Time 1.00(1.00)
Iter 0750 | Time 0.3295(0.3270) | Bit/dim 2.5283(2.5040) | Steps 107(107.27) | Grad Norm 2.5290(4.1154) | Total Time 1.00(1.00)
Iter 0760 | Time 0.3222(0.3274) | Bit/dim 2.3609(2.4898) | Steps 107(107.20) | Grad Norm 3.3286(3.8602) | Total Time 1.00(1.00)
Iter 0770 | Time 0.3279(0.3272) | Bit/dim 2.4683(2.4848) | Steps 107(107.15) | Grad Norm 3.8398(3.8714) | Total Time 1.00(1.00)
Iter 0780 | Time 0.3322(0.3277) | Bit/dim 2.4187(2.4696) | Steps 107(107.11) | Grad Norm 3.1449(3.4681) | Total Time 1.00(1.00)
Iter 0790 | Time 0.3185(0.3280) | Bit/dim 2.4085(2.4662) | Steps 107(107.08) | Grad Norm 5.0684(3.5839) | Total Time 1.00(1.00)
Iter 0800 | Time 0.3181(0.3255) | Bit/dim 2.4363(2.4552) | Steps 107(107.06) | Grad Norm 5.3753(3.5700) | Total Time 1.00(1.00)
Iter 0810 | Time 0.3182(0.3236) | Bit/dim 2.4207(2.4475) | Steps 107(107.04) | Grad Norm 4.9654(3.7820) | Total Time 1.00(1.00)
Iter 0820 | Time 0.3182(0.3219) | Bit/dim 2.3808(2.4406) | Steps 107(106.86) | Grad Norm 2.0578(4.1334) | Total Time 1.00(1.00)
Iter 0830 | Time 0.2910(0.3154) | Bit/dim 2.4218(2.4357) | Steps 101(105.62) | Grad Norm 4.1156(4.3254) | Total Time 1.00(1.00)
Iter 0840 | Time 0.2913(0.3069) | Bit/dim 2.3990(2.4212) | Steps 101(103.75) | Grad Norm 7.9138(4.8149) | Total Time 1.00(1.00)
Iter 0850 | Time 0.2765(0.3006) | Bit/dim 2.3754(2.4051) | Steps 95(102.39) | Grad Norm 2.8778(4.4754) | Total Time 1.00(1.00)
Iter 0860 | Time 0.2649(0.2961) | Bit/dim 2.3906(2.3979) | Steps 95(101.54) | Grad Norm 4.0346(4.1718) | Total Time 1.00(1.00)
Iter 0870 | Time 0.2893(0.2926) | Bit/dim 2.3702(2.3898) | Steps 101(100.92) | Grad Norm 2.5110(4.2453) | Total Time 1.00(1.00)
Iter 0880 | Time 0.2934(0.2908) | Bit/dim 2.3406(2.3830) | Steps 101(100.63) | Grad Norm 8.5107(4.5017) | Total Time 1.00(1.00)
Iter 0890 | Time 0.2995(0.2936) | Bit/dim 2.3590(2.3763) | Steps 101(100.87) | Grad Norm 6.3866(4.6342) | Total Time 1.00(1.00)
Iter 0900 | Time 0.2941(0.2938) | Bit/dim 2.3413(2.3728) | Steps 101(100.72) | Grad Norm 6.0609(5.1232) | Total Time 1.00(1.00)
Iter 0910 | Time 0.2772(0.2917) | Bit/dim 2.3598(2.3712) | Steps 95(99.99) | Grad Norm 5.0133(5.3127) | Total Time 1.00(1.00)
Iter 0920 | Time 0.2726(0.2882) | Bit/dim 2.3416(2.3566) | Steps 95(98.82) | Grad Norm 2.6787(4.6409) | Total Time 1.00(1.00)
Iter 0930 | Time 0.2710(0.2848) | Bit/dim 2.3111(2.3533) | Steps 95(97.82) | Grad Norm 3.2355(4.1143) | Total Time 1.00(1.00)
Iter 0940 | Time 0.2791(0.2830) | Bit/dim 2.2933(2.3435) | Steps 95(97.08) | Grad Norm 4.9116(4.0639) | Total Time 1.00(1.00)
Iter 0950 | Time 0.2704(0.2813) | Bit/dim 2.3008(2.3352) | Steps 95(96.53) | Grad Norm 4.3674(4.3152) | Total Time 1.00(1.00)
Iter 0960 | Time 0.2768(0.2792) | Bit/dim 2.3234(2.3292) | Steps 95(96.13) | Grad Norm 2.4523(4.0232) | Total Time 1.00(1.00)
Iter 0970 | Time 0.2744(0.2783) | Bit/dim 2.2901(2.3222) | Steps 95(95.83) | Grad Norm 2.1872(3.6883) | Total Time 1.00(1.00)
Iter 0980 | Time 0.2751(0.2773) | Bit/dim 2.2653(2.3134) | Steps 95(95.61) | Grad Norm 4.2945(4.7289) | Total Time 1.00(1.00)
Iter 0990 | Time 0.2750(0.2762) | Bit/dim 2.2997(2.3060) | Steps 95(95.45) | Grad Norm 4.2438(4.7902) | Total Time 1.00(1.00)
Iter 1000 | Time 0.2765(0.2761) | Bit/dim 2.2669(2.3034) | Steps 95(95.33) | Grad Norm 2.7691(4.2158) | Total Time 1.00(1.00)
Iter 1010 | Time 0.2765(0.2754) | Bit/dim 2.2792(2.2951) | Steps 95(95.25) | Grad Norm 6.7516(4.1866) | Total Time 1.00(1.00)
Iter 1020 | Time 0.2805(0.2764) | Bit/dim 2.3018(2.2870) | Steps 95(95.18) | Grad Norm 5.9827(4.7666) | Total Time 1.00(1.00)
Iter 1030 | Time 0.2705(0.2769) | Bit/dim 2.2161(2.2807) | Steps 95(95.13) | Grad Norm 2.1947(4.7604) | Total Time 1.00(1.00)
Iter 1040 | Time 0.2758(0.2761) | Bit/dim 2.2906(2.2726) | Steps 95(95.10) | Grad Norm 4.1524(4.7758) | Total Time 1.00(1.00)
Iter 1050 | Time 0.2755(0.2752) | Bit/dim 2.2504(2.2642) | Steps 95(95.07) | Grad Norm 3.1320(4.6145) | Total Time 1.00(1.00)
Iter 1060 | Time 0.2998(0.2763) | Bit/dim 2.2528(2.2609) | Steps 101(95.39) | Grad Norm 4.5545(4.5808) | Total Time 1.00(1.00)
Iter 1070 | Time 0.3054(0.2800) | Bit/dim 2.2409(2.2559) | Steps 101(96.07) | Grad Norm 4.7687(4.6273) | Total Time 1.00(1.00)
Iter 1080 | Time 0.2920(0.2830) | Bit/dim 2.2357(2.2468) | Steps 101(97.04) | Grad Norm 3.2343(4.4503) | Total Time 1.00(1.00)
Iter 1090 | Time 0.3044(0.2849) | Bit/dim 2.2250(2.2430) | Steps 101(97.75) | Grad Norm 4.4942(4.3037) | Total Time 1.00(1.00)
Iter 1100 | Time 0.3053(0.2888) | Bit/dim 2.2702(2.2413) | Steps 101(98.29) | Grad Norm 3.2103(3.9621) | Total Time 1.00(1.00)
Iter 1110 | Time 0.2916(0.2905) | Bit/dim 2.1704(2.2275) | Steps 101(99.00) | Grad Norm 2.6978(3.7780) | Total Time 1.00(1.00)
Iter 1120 | Time 0.2917(0.2908) | Bit/dim 2.1833(2.2172) | Steps 101(99.53) | Grad Norm 3.3181(3.5550) | Total Time 1.00(1.00)
Iter 1130 | Time 0.3023(0.2939) | Bit/dim 2.2171(2.2129) | Steps 101(99.91) | Grad Norm 7.7060(3.7993) | Total Time 1.00(1.00)
Iter 1140 | Time 0.3033(0.2962) | Bit/dim 2.2239(2.2124) | Steps 101(100.20) | Grad Norm 4.6582(4.0158) | Total Time 1.00(1.00)
Iter 1150 | Time 0.3035(0.2979) | Bit/dim 2.1475(2.2051) | Steps 101(100.41) | Grad Norm 2.6029(3.9222) | Total Time 1.00(1.00)
Iter 1160 | Time 0.3038(0.2994) | Bit/dim 2.1678(2.2060) | Steps 101(100.56) | Grad Norm 3.3975(3.9319) | Total Time 1.00(1.00)
Iter 1170 | Time 0.3028(0.3003) | Bit/dim 2.1945(2.2065) | Steps 101(100.68) | Grad Norm 2.1583(3.7323) | Total Time 1.00(1.00)
Iter 1180 | Time 0.3022(0.3010) | Bit/dim 2.2247(2.2004) | Steps 101(100.76) | Grad Norm 2.4378(3.6353) | Total Time 1.00(1.00)
Iter 1190 | Time 0.2915(0.3005) | Bit/dim 2.1604(2.1983) | Steps 101(100.83) | Grad Norm 4.7678(3.6610) | Total Time 1.00(1.00)
validating...
Epoch 0002 | Time 4.5954, Bit/dim 2.1704
Iter 1200 | Time 0.3104(0.3009) | Bit/dim 2.1865(2.1969) | Steps 101(100.87) | Grad Norm 7.0287(3.9893) | Total Time 1.00(1.00)
Iter 1210 | Time 0.3005(0.3016) | Bit/dim 2.1420(2.1875) | Steps 101(100.91) | Grad Norm 6.7764(4.3366) | Total Time 1.00(1.00)
Iter 1220 | Time 0.3059(0.3019) | Bit/dim 2.1602(2.1837) | Steps 101(100.93) | Grad Norm 2.4155(4.4553) | Total Time 1.00(1.00)
Iter 1230 | Time 0.2981(0.3028) | Bit/dim 2.1293(2.1773) | Steps 101(101.26) | Grad Norm 5.4228(4.5954) | Total Time 1.00(1.00)
Iter 1240 | Time 0.3180(0.3024) | Bit/dim 2.1681(2.1733) | Steps 107(101.54) | Grad Norm 2.5957(4.4651) | Total Time 1.00(1.00)
Iter 1250 | Time 0.3181(0.3015) | Bit/dim 2.1269(2.1678) | Steps 107(101.73) | Grad Norm 2.3849(3.9146) | Total Time 1.00(1.00)
Iter 1260 | Time 0.3180(0.3021) | Bit/dim 2.1574(2.1661) | Steps 107(102.19) | Grad Norm 3.3253(3.8589) | Total Time 1.00(1.00)
Iter 1270 | Time 0.2938(0.3013) | Bit/dim 2.1564(2.1625) | Steps 101(102.20) | Grad Norm 4.4680(3.7643) | Total Time 1.00(1.00)
Iter 1280 | Time 0.3067(0.3041) | Bit/dim 2.1138(2.1588) | Steps 101(102.51) | Grad Norm 2.1592(3.5759) | Total Time 1.00(1.00)
Iter 1290 | Time 0.3217(0.3075) | Bit/dim 2.2367(2.1538) | Steps 107(102.93) | Grad Norm 4.7508(3.4822) | Total Time 1.00(1.00)
Iter 1300 | Time 0.3276(0.3108) | Bit/dim 2.1915(2.1521) | Steps 107(103.37) | Grad Norm 9.4461(3.8899) | Total Time 1.00(1.00)
Iter 1310 | Time 0.3269(0.3126) | Bit/dim 2.1344(2.1465) | Steps 107(103.73) | Grad Norm 5.9074(4.2924) | Total Time 1.00(1.00)
Iter 1320 | Time 0.3266(0.3160) | Bit/dim 2.1296(2.1433) | Steps 107(104.43) | Grad Norm 6.6339(4.1776) | Total Time 1.00(1.00)
Iter 1330 | Time 0.3272(0.3182) | Bit/dim 2.1414(2.1379) | Steps 107(104.95) | Grad Norm 3.9668(4.0273) | Total Time 1.00(1.00)
Iter 1340 | Time 0.3306(0.3203) | Bit/dim 2.0992(2.1368) | Steps 107(105.47) | Grad Norm 5.3674(3.9318) | Total Time 1.00(1.00)
Iter 1350 | Time 0.3280(0.3223) | Bit/dim 2.0874(2.1306) | Steps 107(105.87) | Grad Norm 2.8285(4.6213) | Total Time 1.00(1.00)
Iter 1360 | Time 0.3180(0.3234) | Bit/dim 2.1209(2.1293) | Steps 107(106.47) | Grad Norm 4.5311(5.1284) | Total Time 1.00(1.00)
Iter 1370 | Time 0.3262(0.3236) | Bit/dim 2.0977(2.1232) | Steps 107(106.61) | Grad Norm 4.2702(4.6777) | Total Time 1.00(1.00)
Iter 1380 | Time 0.3288(0.3275) | Bit/dim 2.1586(2.1246) | Steps 107(107.34) | Grad Norm 2.5310(4.0709) | Total Time 1.00(1.00)
Iter 1390 | Time 0.3278(0.3299) | Bit/dim 2.1549(2.1200) | Steps 107(107.73) | Grad Norm 4.4457(3.7413) | Total Time 1.00(1.00)
Iter 1400 | Time 0.3534(0.3306) | Bit/dim 2.0885(2.1190) | Steps 113(108.03) | Grad Norm 2.6461(3.4557) | Total Time 1.00(1.00)
Iter 1410 | Time 0.3159(0.3312) | Bit/dim 2.1475(2.1194) | Steps 107(108.55) | Grad Norm 5.8889(3.6059) | Total Time 1.00(1.00)
Iter 1420 | Time 0.3404(0.3304) | Bit/dim 2.1748(2.1227) | Steps 113(108.91) | Grad Norm 2.3350(3.5567) | Total Time 1.00(1.00)
Iter 1430 | Time 0.3403(0.3319) | Bit/dim 2.1013(2.1194) | Steps 113(109.69) | Grad Norm 1.9583(3.3168) | Total Time 1.00(1.00)
Iter 1440 | Time 0.3406(0.3317) | Bit/dim 2.0676(2.1120) | Steps 113(109.96) | Grad Norm 5.1684(3.4059) | Total Time 1.00(1.00)
Iter 1450 | Time 0.3403(0.3317) | Bit/dim 2.1035(2.1089) | Steps 113(110.16) | Grad Norm 3.6574(3.5029) | Total Time 1.00(1.00)
Iter 1460 | Time 0.3527(0.3327) | Bit/dim 2.0953(2.1070) | Steps 113(110.28) | Grad Norm 2.6655(3.5013) | Total Time 1.00(1.00)
Iter 1470 | Time 0.3275(0.3338) | Bit/dim 2.0988(2.1033) | Steps 107(110.04) | Grad Norm 2.3714(3.3731) | Total Time 1.00(1.00)
Iter 1480 | Time 0.3558(0.3369) | Bit/dim 2.0904(2.0988) | Steps 113(110.68) | Grad Norm 5.7067(3.3140) | Total Time 1.00(1.00)
Iter 1490 | Time 0.3422(0.3396) | Bit/dim 2.1058(2.0914) | Steps 113(111.11) | Grad Norm 3.3922(3.2885) | Total Time 1.00(1.00)
Iter 1500 | Time 0.3421(0.3398) | Bit/dim 2.0782(2.0894) | Steps 113(111.45) | Grad Norm 3.9841(3.3169) | Total Time 1.00(1.00)
Iter 1510 | Time 0.3426(0.3406) | Bit/dim 2.1159(2.0916) | Steps 113(112.01) | Grad Norm 4.0191(3.1488) | Total Time 1.00(1.00)
Iter 1520 | Time 0.3507(0.3407) | Bit/dim 2.1136(2.0921) | Steps 119(112.60) | Grad Norm 1.7365(3.1371) | Total Time 1.00(1.00)
Iter 1530 | Time 0.3421(0.3414) | Bit/dim 2.0879(2.0872) | Steps 113(112.86) | Grad Norm 3.3752(3.1716) | Total Time 1.00(1.00)
Iter 1540 | Time 0.3420(0.3420) | Bit/dim 2.0682(2.0906) | Steps 113(113.17) | Grad Norm 2.2410(3.2045) | Total Time 1.00(1.00)
Iter 1550 | Time 0.3508(0.3428) | Bit/dim 2.1279(2.0854) | Steps 119(113.79) | Grad Norm 3.2758(3.1025) | Total Time 1.00(1.00)
Iter 1560 | Time 0.3507(0.3444) | Bit/dim 2.0896(2.0847) | Steps 119(114.85) | Grad Norm 2.5786(3.0330) | Total Time 1.00(1.00)
Iter 1570 | Time 0.3687(0.3500) | Bit/dim 2.0747(2.0792) | Steps 119(115.94) | Grad Norm 3.5265(3.0769) | Total Time 1.00(1.00)
Iter 1580 | Time 0.3733(0.3550) | Bit/dim 2.0581(2.0789) | Steps 119(116.75) | Grad Norm 3.1068(3.0382) | Total Time 1.00(1.00)
Iter 1590 | Time 0.3558(0.3583) | Bit/dim 2.0825(2.0764) | Steps 113(117.16) | Grad Norm 4.6971(3.0841) | Total Time 1.00(1.00)
Iter 1600 | Time 0.3484(0.3575) | Bit/dim 2.0774(2.0749) | Steps 119(117.48) | Grad Norm 3.5045(3.1227) | Total Time 1.00(1.00)
Iter 1610 | Time 0.3616(0.3582) | Bit/dim 2.0030(2.0748) | Steps 119(117.88) | Grad Norm 3.5055(3.0995) | Total Time 1.00(1.00)
Iter 1620 | Time 0.3482(0.3576) | Bit/dim 2.0418(2.0690) | Steps 119(118.04) | Grad Norm 1.8956(2.8931) | Total Time 1.00(1.00)
Iter 1630 | Time 0.3483(0.3553) | Bit/dim 2.0633(2.0647) | Steps 119(118.29) | Grad Norm 3.6224(2.9458) | Total Time 1.00(1.00)
Iter 1640 | Time 0.3483(0.3528) | Bit/dim 2.0698(2.0618) | Steps 119(118.30) | Grad Norm 2.6481(2.8984) | Total Time 1.00(1.00)
Iter 1650 | Time 0.3484(0.3511) | Bit/dim 2.0469(2.0644) | Steps 119(118.31) | Grad Norm 2.1099(2.8156) | Total Time 1.00(1.00)
Iter 1660 | Time 0.3620(0.3504) | Bit/dim 2.0355(2.0589) | Steps 119(118.19) | Grad Norm 2.9863(2.9200) | Total Time 1.00(1.00)
Iter 1670 | Time 0.3649(0.3539) | Bit/dim 2.0288(2.0579) | Steps 119(118.40) | Grad Norm 2.8794(3.0470) | Total Time 1.00(1.00)
Iter 1680 | Time 0.3521(0.3549) | Bit/dim 2.0819(2.0589) | Steps 119(118.42) | Grad Norm 3.8361(3.2863) | Total Time 1.00(1.00)
Iter 1690 | Time 0.3524(0.3550) | Bit/dim 2.0594(2.0602) | Steps 119(118.74) | Grad Norm 5.4540(3.4816) | Total Time 1.00(1.00)
Iter 1700 | Time 0.3523(0.3544) | Bit/dim 2.1325(2.0619) | Steps 119(118.81) | Grad Norm 1.8892(3.4019) | Total Time 1.00(1.00)
Iter 1710 | Time 0.3523(0.3533) | Bit/dim 1.9886(2.0566) | Steps 119(118.69) | Grad Norm 3.5820(3.2625) | Total Time 1.00(1.00)
Iter 1720 | Time 0.3524(0.3540) | Bit/dim 2.0475(2.0528) | Steps 119(118.92) | Grad Norm 2.6194(3.1441) | Total Time 1.00(1.00)
Iter 1730 | Time 0.3525(0.3556) | Bit/dim 2.0499(2.0565) | Steps 119(119.38) | Grad Norm 3.0178(2.9209) | Total Time 1.00(1.00)
Iter 1740 | Time 0.3622(0.3552) | Bit/dim 2.0324(2.0554) | Steps 119(118.64) | Grad Norm 2.4442(2.7938) | Total Time 1.00(1.00)
Iter 1750 | Time 0.3638(0.3575) | Bit/dim 2.0531(2.0514) | Steps 119(118.67) | Grad Norm 3.0765(2.8825) | Total Time 1.00(1.00)
Iter 1760 | Time 0.3415(0.3574) | Bit/dim 1.9860(2.0485) | Steps 113(118.29) | Grad Norm 3.6785(3.1594) | Total Time 1.00(1.00)
Iter 1770 | Time 0.3886(0.3656) | Bit/dim 2.0796(2.0442) | Steps 125(119.91) | Grad Norm 1.8859(2.9323) | Total Time 1.00(1.00)
Iter 1780 | Time 0.3509(0.3628) | Bit/dim 2.0965(2.0468) | Steps 119(119.65) | Grad Norm 2.3974(2.7324) | Total Time 1.00(1.00)
Iter 1790 | Time 0.3778(0.3655) | Bit/dim 2.0535(2.0488) | Steps 125(120.79) | Grad Norm 1.9133(2.5893) | Total Time 1.00(1.00)
validating...
Epoch 0003 | Time 4.7248, Bit/dim 2.0298
Iter 1800 | Time 0.3384(0.3648) | Bit/dim 2.0784(2.0472) | Steps 113(120.92) | Grad Norm 2.5435(2.5253) | Total Time 1.00(1.00)
Iter 1810 | Time 0.3243(0.3625) | Bit/dim 2.0597(2.0414) | Steps 113(120.76) | Grad Norm 2.3933(2.4745) | Total Time 1.00(1.00)
Iter 1820 | Time 0.3757(0.3590) | Bit/dim 1.9782(2.0388) | Steps 125(120.25) | Grad Norm 3.7733(2.5890) | Total Time 1.00(1.00)
Iter 1830 | Time 0.3997(0.3620) | Bit/dim 2.0387(2.0410) | Steps 131(121.21) | Grad Norm 1.8212(2.8359) | Total Time 1.00(1.00)
Iter 1840 | Time 0.3240(0.3614) | Bit/dim 2.0859(2.0379) | Steps 113(121.25) | Grad Norm 3.6005(2.8683) | Total Time 1.00(1.00)
Iter 1850 | Time 0.3365(0.3610) | Bit/dim 2.0149(2.0337) | Steps 113(120.92) | Grad Norm 4.6368(2.9980) | Total Time 1.00(1.00)
Iter 1860 | Time 0.3417(0.3597) | Bit/dim 2.0145(2.0293) | Steps 113(119.92) | Grad Norm 2.6018(3.1011) | Total Time 1.00(1.00)
Iter 1870 | Time 0.3306(0.3599) | Bit/dim 2.0296(2.0307) | Steps 113(119.50) | Grad Norm 1.7313(2.8805) | Total Time 1.00(1.00)
Iter 1880 | Time 0.4175(0.3627) | Bit/dim 2.0260(2.0275) | Steps 131(119.91) | Grad Norm 2.2544(2.7421) | Total Time 1.00(1.00)
Iter 1890 | Time 0.3343(0.3606) | Bit/dim 2.0378(2.0301) | Steps 113(119.18) | Grad Norm 3.2043(2.6548) | Total Time 1.00(1.00)
Iter 1900 | Time 0.3548(0.3613) | Bit/dim 1.9672(2.0322) | Steps 119(119.66) | Grad Norm 1.4927(2.6574) | Total Time 1.00(1.00)
Iter 1910 | Time 0.3789(0.3629) | Bit/dim 2.0771(2.0339) | Steps 125(120.35) | Grad Norm 2.8585(2.6514) | Total Time 1.00(1.00)
Iter 1920 | Time 0.3794(0.3665) | Bit/dim 1.9872(2.0278) | Steps 125(121.42) | Grad Norm 3.1096(2.7905) | Total Time 1.00(1.00)
Iter 1930 | Time 0.4057(0.3723) | Bit/dim 2.0520(2.0268) | Steps 131(122.67) | Grad Norm 4.1783(2.9282) | Total Time 1.00(1.00)
Iter 1940 | Time 0.3777(0.3706) | Bit/dim 2.0605(2.0249) | Steps 125(122.37) | Grad Norm 1.6987(2.9389) | Total Time 1.00(1.00)
Iter 1950 | Time 0.3893(0.3729) | Bit/dim 2.0587(2.0268) | Steps 125(123.07) | Grad Norm 3.8857(3.2085) | Total Time 1.00(1.00)
Iter 1960 | Time 0.3773(0.3743) | Bit/dim 2.0068(2.0267) | Steps 125(123.64) | Grad Norm 3.3724(3.2310) | Total Time 1.00(1.00)
Iter 1970 | Time 0.3777(0.3768) | Bit/dim 2.0628(2.0202) | Steps 125(123.89) | Grad Norm 1.9351(3.2017) | Total Time 1.00(1.00)
Iter 1980 | Time 0.3262(0.3736) | Bit/dim 2.0701(2.0218) | Steps 113(123.33) | Grad Norm 3.3248(3.1465) | Total Time 1.00(1.00)
Iter 1990 | Time 0.3261(0.3732) | Bit/dim 1.9349(2.0233) | Steps 113(123.45) | Grad Norm 1.9476(2.9617) | Total Time 1.00(1.00)
Iter 2000 | Time 0.3772(0.3738) | Bit/dim 2.0028(2.0224) | Steps 125(123.74) | Grad Norm 1.9719(2.7805) | Total Time 1.00(1.00)
Iter 2010 | Time 0.3775(0.3742) | Bit/dim 2.0035(2.0246) | Steps 125(123.94) | Grad Norm 3.1915(2.7509) | Total Time 1.00(1.00)
Iter 2020 | Time 0.4135(0.3769) | Bit/dim 2.0035(2.0243) | Steps 131(124.24) | Grad Norm 3.8676(2.7525) | Total Time 1.00(1.00)
Iter 2030 | Time 0.3913(0.3766) | Bit/dim 2.0143(2.0219) | Steps 125(123.61) | Grad Norm 4.2220(2.7758) | Total Time 1.00(1.00)
Iter 2040 | Time 0.3749(0.3765) | Bit/dim 1.9825(2.0202) | Steps 125(124.02) | Grad Norm 3.6500(3.1390) | Total Time 1.00(1.00)
Iter 2050 | Time 0.3750(0.3761) | Bit/dim 2.0436(2.0155) | Steps 125(124.28) | Grad Norm 2.8617(3.3108) | Total Time 1.00(1.00)
Iter 2060 | Time 0.3752(0.3750) | Bit/dim 1.9878(2.0117) | Steps 125(124.29) | Grad Norm 3.3298(3.2480) | Total Time 1.00(1.00)
Iter 2070 | Time 0.3753(0.3764) | Bit/dim 1.9830(2.0093) | Steps 125(124.48) | Grad Norm 3.4959(3.1295) | Total Time 1.00(1.00)
Iter 2080 | Time 0.3752(0.3761) | Bit/dim 1.9510(2.0038) | Steps 125(124.61) | Grad Norm 3.1493(2.9510) | Total Time 1.00(1.00)
Iter 2090 | Time 0.3753(0.3746) | Bit/dim 2.0528(2.0111) | Steps 125(124.40) | Grad Norm 2.4511(2.7921) | Total Time 1.00(1.00)
Iter 2100 | Time 0.3758(0.3750) | Bit/dim 1.9905(2.0082) | Steps 125(124.58) | Grad Norm 2.9564(2.8900) | Total Time 1.00(1.00)
Iter 2110 | Time 0.3996(0.3765) | Bit/dim 2.0271(2.0113) | Steps 131(125.04) | Grad Norm 2.8526(3.2401) | Total Time 1.00(1.00)
Iter 2120 | Time 0.3789(0.3807) | Bit/dim 1.9852(2.0046) | Steps 125(125.35) | Grad Norm 2.5541(2.9976) | Total Time 1.00(1.00)
Iter 2130 | Time 0.3790(0.3809) | Bit/dim 1.9845(2.0075) | Steps 125(125.41) | Grad Norm 1.4448(2.8719) | Total Time 1.00(1.00)
Iter 2140 | Time 0.3793(0.3812) | Bit/dim 1.9603(2.0047) | Steps 125(125.47) | Grad Norm 2.0908(2.7814) | Total Time 1.00(1.00)
Iter 2150 | Time 0.3787(0.3812) | Bit/dim 2.0188(2.0120) | Steps 125(125.50) | Grad Norm 2.5569(2.7048) | Total Time 1.00(1.00)
Iter 2160 | Time 0.4039(0.3828) | Bit/dim 1.9125(2.0055) | Steps 131(125.88) | Grad Norm 1.9911(2.6637) | Total Time 1.00(1.00)
Iter 2170 | Time 0.3923(0.3850) | Bit/dim 1.9693(2.0075) | Steps 125(125.96) | Grad Norm 2.1508(2.7089) | Total Time 1.00(1.00)
Iter 2180 | Time 0.3911(0.3886) | Bit/dim 2.0139(2.0052) | Steps 125(126.17) | Grad Norm 2.4215(2.6913) | Total Time 1.00(1.00)
Iter 2190 | Time 0.4175(0.3901) | Bit/dim 2.0082(2.0067) | Steps 131(126.20) | Grad Norm 3.0680(2.5136) | Total Time 1.00(1.00)
Iter 2200 | Time 0.4152(0.3924) | Bit/dim 1.9918(2.0018) | Steps 131(126.37) | Grad Norm 3.5648(2.6014) | Total Time 1.00(1.00)
Iter 2210 | Time 0.4166(0.3947) | Bit/dim 1.9931(2.0017) | Steps 131(126.68) | Grad Norm 2.1907(2.6768) | Total Time 1.00(1.00)
Iter 2220 | Time 0.3918(0.3958) | Bit/dim 1.9605(1.9947) | Steps 125(126.70) | Grad Norm 2.9373(2.6913) | Total Time 1.00(1.00)
Iter 2230 | Time 0.3827(0.3958) | Bit/dim 1.9304(1.9921) | Steps 125(126.56) | Grad Norm 3.1515(2.7035) | Total Time 1.00(1.00)
Iter 2240 | Time 0.3757(0.3920) | Bit/dim 1.9895(1.9924) | Steps 125(126.44) | Grad Norm 1.8129(2.4612) | Total Time 1.00(1.00)
Iter 2250 | Time 0.3750(0.3882) | Bit/dim 1.9796(1.9932) | Steps 125(126.22) | Grad Norm 2.9943(2.4820) | Total Time 1.00(1.00)
Iter 2260 | Time 0.3746(0.3848) | Bit/dim 1.9987(1.9954) | Steps 125(125.90) | Grad Norm 1.4763(2.4204) | Total Time 1.00(1.00)
Iter 2270 | Time 0.3992(0.3836) | Bit/dim 2.0349(2.0002) | Steps 131(126.01) | Grad Norm 2.9496(2.5886) | Total Time 1.00(1.00)
Iter 2280 | Time 0.3993(0.3833) | Bit/dim 2.0105(1.9993) | Steps 131(126.22) | Grad Norm 2.9966(2.5707) | Total Time 1.00(1.00)
Iter 2290 | Time 0.3748(0.3814) | Bit/dim 1.9111(1.9980) | Steps 125(125.90) | Grad Norm 2.4618(2.7658) | Total Time 1.00(1.00)
Iter 2300 | Time 0.3900(0.3824) | Bit/dim 2.0036(1.9998) | Steps 125(125.99) | Grad Norm 2.8157(2.8419) | Total Time 1.00(1.00)
Iter 2310 | Time 0.3789(0.3831) | Bit/dim 1.9562(1.9968) | Steps 125(125.73) | Grad Norm 3.7595(2.9400) | Total Time 1.00(1.00)
Iter 2320 | Time 0.3794(0.3838) | Bit/dim 2.0048(1.9979) | Steps 125(125.86) | Grad Norm 1.2877(2.6919) | Total Time 1.00(1.00)
Iter 2330 | Time 0.3790(0.3840) | Bit/dim 1.9903(1.9962) | Steps 125(125.96) | Grad Norm 2.2750(2.5383) | Total Time 1.00(1.00)
Iter 2340 | Time 0.3809(0.3840) | Bit/dim 2.0140(1.9952) | Steps 125(126.00) | Grad Norm 1.5563(2.4350) | Total Time 1.00(1.00)
Iter 2350 | Time 0.4040(0.3836) | Bit/dim 2.0221(1.9943) | Steps 131(125.94) | Grad Norm 3.9278(2.4930) | Total Time 1.00(1.00)
Iter 2360 | Time 0.3793(0.3837) | Bit/dim 2.0788(1.9930) | Steps 125(126.00) | Grad Norm 2.2394(2.6305) | Total Time 1.00(1.00)
Iter 2370 | Time 0.3944(0.3842) | Bit/dim 1.9371(1.9909) | Steps 125(125.91) | Grad Norm 1.8393(2.5990) | Total Time 1.00(1.00)
Iter 2380 | Time 0.4084(0.3900) | Bit/dim 1.9509(1.9870) | Steps 125(125.67) | Grad Norm 1.8578(2.3894) | Total Time 1.00(1.00)
Iter 2390 | Time 0.4095(0.3963) | Bit/dim 2.0123(1.9887) | Steps 125(125.67) | Grad Norm 1.7295(2.2894) | Total Time 1.00(1.00)
validating...
Epoch 0004 | Time 5.0699, Bit/dim 1.9811
Iter 2400 | Time 0.4038(0.3995) | Bit/dim 2.0073(1.9934) | Steps 131(126.01) | Grad Norm 2.4233(2.2545) | Total Time 1.00(1.00)
Iter 2410 | Time 0.3895(0.3975) | Bit/dim 1.9114(1.9902) | Steps 125(125.89) | Grad Norm 2.9190(2.3852) | Total Time 1.00(1.00)
Iter 2420 | Time 0.3896(0.3961) | Bit/dim 1.9464(1.9892) | Steps 125(125.80) | Grad Norm 1.9809(2.4081) | Total Time 1.00(1.00)
Iter 2430 | Time 0.3754(0.3936) | Bit/dim 1.9769(1.9835) | Steps 125(125.75) | Grad Norm 1.7905(2.4003) | Total Time 1.00(1.00)
Iter 2440 | Time 0.3847(0.3893) | Bit/dim 1.9723(1.9821) | Steps 125(125.55) | Grad Norm 1.9442(2.2727) | Total Time 1.00(1.00)
Iter 2450 | Time 0.3767(0.3858) | Bit/dim 1.9302(1.9766) | Steps 125(125.41) | Grad Norm 3.0629(2.2207) | Total Time 1.00(1.00)
Iter 2460 | Time 0.3786(0.3847) | Bit/dim 2.0056(1.9805) | Steps 125(125.44) | Grad Norm 1.9976(2.3561) | Total Time 1.00(1.00)
Iter 2470 | Time 0.4020(0.3845) | Bit/dim 1.9977(1.9821) | Steps 131(125.67) | Grad Norm 2.9796(2.5042) | Total Time 1.00(1.00)
Iter 2480 | Time 0.3781(0.3834) | Bit/dim 1.9532(1.9795) | Steps 125(125.64) | Grad Norm 3.0107(2.4357) | Total Time 1.00(1.00)
Iter 2490 | Time 0.3785(0.3821) | Bit/dim 1.9614(1.9788) | Steps 125(125.47) | Grad Norm 2.3979(2.3912) | Total Time 1.00(1.00)
Iter 2500 | Time 0.3783(0.3811) | Bit/dim 1.9589(1.9770) | Steps 125(125.35) | Grad Norm 3.1968(2.4431) | Total Time 1.00(1.00)
Iter 2510 | Time 0.3787(0.3804) | Bit/dim 2.0221(1.9801) | Steps 125(125.26) | Grad Norm 3.9259(2.5921) | Total Time 1.00(1.00)
Iter 2520 | Time 0.3784(0.3819) | Bit/dim 1.9507(1.9761) | Steps 125(125.19) | Grad Norm 1.4456(2.6197) | Total Time 1.00(1.00)
Iter 2530 | Time 0.3941(0.3874) | Bit/dim 1.9588(1.9761) | Steps 125(125.14) | Grad Norm 2.5173(2.6160) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.2975(0.2975) | Bit/dim 24.9914(24.9914) | Steps 41(41.00) | Grad Norm 15.1482(15.1482) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1260(0.2525) | Bit/dim 24.9031(24.9708) | Steps 41(41.00) | Grad Norm 15.3550(15.1332) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1243(0.2187) | Bit/dim 24.8331(24.9320) | Steps 41(41.00) | Grad Norm 15.1621(15.1246) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1276(0.1940) | Bit/dim 24.6454(24.8851) | Steps 41(41.00) | Grad Norm 15.0822(15.1437) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1242(0.1758) | Bit/dim 24.3893(24.7994) | Steps 41(41.00) | Grad Norm 15.3334(15.1703) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1270(0.1633) | Bit/dim 24.2774(24.6792) | Steps 41(41.00) | Grad Norm 15.6426(15.2561) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1229(0.1532) | Bit/dim 23.8410(24.5004) | Steps 41(41.00) | Grad Norm 15.9024(15.4190) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1269(0.1468) | Bit/dim 23.3237(24.2496) | Steps 41(41.00) | Grad Norm 16.7484(15.7327) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1229(0.1414) | Bit/dim 22.5779(23.8897) | Steps 41(41.00) | Grad Norm 18.1893(16.2356) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1287(0.1371) | Bit/dim 21.4952(23.3948) | Steps 41(41.00) | Grad Norm 19.8607(16.9684) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1231(0.1342) | Bit/dim 20.0603(22.7168) | Steps 41(41.00) | Grad Norm 21.6899(17.9651) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1258(0.1319) | Bit/dim 18.8007(21.8435) | Steps 41(41.00) | Grad Norm 21.5073(18.9290) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1265(0.1311) | Bit/dim 17.8609(20.8246) | Steps 41(41.00) | Grad Norm 19.0764(19.4646) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1230(0.1293) | Bit/dim 15.6167(19.7032) | Steps 41(41.00) | Grad Norm 17.7684(19.1757) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1524(0.1335) | Bit/dim 14.8363(18.5117) | Steps 47(42.14) | Grad Norm 12.9406(18.0320) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1472(0.1373) | Bit/dim 13.9676(17.4346) | Steps 47(43.42) | Grad Norm 10.3224(16.2521) | Total Time 1.00(1.00)
Iter 0160 | Time 0.2066(0.1491) | Bit/dim 12.9885(16.4198) | Steps 65(47.21) | Grad Norm 9.1799(14.5000) | Total Time 1.00(1.00)
Iter 0170 | Time 0.2107(0.1645) | Bit/dim 12.6267(15.4872) | Steps 65(51.88) | Grad Norm 8.4589(12.9859) | Total Time 1.00(1.00)
Iter 0180 | Time 0.2093(0.1758) | Bit/dim 11.7466(14.6444) | Steps 65(55.32) | Grad Norm 8.1768(11.7154) | Total Time 1.00(1.00)
Iter 0190 | Time 0.2095(0.1840) | Bit/dim 11.0870(13.8216) | Steps 65(57.86) | Grad Norm 7.7014(10.6827) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2218(0.1930) | Bit/dim 10.7640(13.0296) | Steps 71(61.18) | Grad Norm 7.1948(9.8186) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2660(0.2107) | Bit/dim 9.6954(12.2996) | Steps 83(66.35) | Grad Norm 6.9461(9.1423) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2484(0.2255) | Bit/dim 8.9713(11.5656) | Steps 77(70.54) | Grad Norm 7.1470(8.6208) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2486(0.2345) | Bit/dim 8.6679(10.8979) | Steps 77(72.99) | Grad Norm 7.0685(8.2062) | Total Time 1.00(1.00)
Iter 0240 | Time 0.2417(0.2374) | Bit/dim 8.3816(10.2304) | Steps 77(74.22) | Grad Norm 6.8849(7.8345) | Total Time 1.00(1.00)
Iter 0250 | Time 0.2524(0.2415) | Bit/dim 7.4539(9.6146) | Steps 83(76.52) | Grad Norm 6.1632(7.5022) | Total Time 1.00(1.00)
Iter 0260 | Time 0.2585(0.2446) | Bit/dim 7.1988(9.0237) | Steps 83(78.22) | Grad Norm 5.7625(7.1272) | Total Time 1.00(1.00)
Iter 0270 | Time 0.2302(0.2439) | Bit/dim 6.5633(8.4695) | Steps 77(78.64) | Grad Norm 5.0913(6.7118) | Total Time 1.00(1.00)
Iter 0280 | Time 0.2575(0.2462) | Bit/dim 6.6832(7.9701) | Steps 83(79.64) | Grad Norm 5.1187(6.2942) | Total Time 1.00(1.00)
Iter 0290 | Time 0.2551(0.2482) | Bit/dim 6.3198(7.5051) | Steps 83(80.53) | Grad Norm 4.7345(5.8672) | Total Time 1.00(1.00)
Iter 0300 | Time 0.2508(0.2495) | Bit/dim 5.9502(7.1075) | Steps 83(81.18) | Grad Norm 4.0945(5.3998) | Total Time 1.00(1.00)
Iter 0310 | Time 0.2908(0.2545) | Bit/dim 5.9369(6.7707) | Steps 95(83.58) | Grad Norm 3.7886(4.9809) | Total Time 1.00(1.00)
Iter 0320 | Time 0.2857(0.2634) | Bit/dim 5.4146(6.4655) | Steps 95(86.58) | Grad Norm 3.3591(4.6050) | Total Time 1.00(1.00)
Iter 0330 | Time 0.2897(0.2705) | Bit/dim 5.3890(6.2087) | Steps 95(88.79) | Grad Norm 3.1263(4.2485) | Total Time 1.00(1.00)
Iter 0340 | Time 0.2896(0.2749) | Bit/dim 5.2586(5.9750) | Steps 95(90.42) | Grad Norm 3.2995(3.9679) | Total Time 1.00(1.00)
Iter 0350 | Time 0.2893(0.2791) | Bit/dim 5.1845(5.7596) | Steps 95(91.62) | Grad Norm 3.3280(3.7739) | Total Time 1.00(1.00)
Iter 0360 | Time 0.2918(0.2814) | Bit/dim 4.9420(5.5584) | Steps 95(92.51) | Grad Norm 3.0215(3.5666) | Total Time 1.00(1.00)
Iter 0370 | Time 0.3004(0.2858) | Bit/dim 4.7714(5.3795) | Steps 95(93.16) | Grad Norm 3.0658(3.4073) | Total Time 1.00(1.00)
Iter 0380 | Time 0.2935(0.2878) | Bit/dim 4.5205(5.1987) | Steps 95(93.65) | Grad Norm 2.9557(3.3010) | Total Time 1.00(1.00)
Iter 0390 | Time 0.2917(0.2892) | Bit/dim 4.5625(5.0279) | Steps 95(94.00) | Grad Norm 3.5809(3.2655) | Total Time 1.00(1.00)
Iter 0400 | Time 0.2918(0.2890) | Bit/dim 4.2458(4.8398) | Steps 95(94.26) | Grad Norm 3.4347(3.2315) | Total Time 1.00(1.00)
Iter 0410 | Time 0.2912(0.2898) | Bit/dim 4.1732(4.6735) | Steps 95(94.46) | Grad Norm 3.3700(3.2098) | Total Time 1.00(1.00)
Iter 0420 | Time 0.2820(0.2879) | Bit/dim 4.0440(4.5055) | Steps 95(94.60) | Grad Norm 2.9477(3.1654) | Total Time 1.00(1.00)
Iter 0430 | Time 0.2821(0.2877) | Bit/dim 3.8301(4.3509) | Steps 95(94.70) | Grad Norm 2.9886(3.1391) | Total Time 1.00(1.00)
Iter 0440 | Time 0.2910(0.2871) | Bit/dim 3.7730(4.1867) | Steps 95(94.78) | Grad Norm 2.8630(3.0259) | Total Time 1.00(1.00)
Iter 0450 | Time 0.2859(0.2880) | Bit/dim 3.5932(4.0419) | Steps 95(94.84) | Grad Norm 3.0522(3.0362) | Total Time 1.00(1.00)
Iter 0460 | Time 0.2804(0.2860) | Bit/dim 3.4445(3.8952) | Steps 95(94.88) | Grad Norm 2.2164(2.9960) | Total Time 1.00(1.00)
Iter 0470 | Time 0.2805(0.2849) | Bit/dim 3.4054(3.7622) | Steps 95(95.08) | Grad Norm 2.8172(2.9823) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3015(0.2866) | Bit/dim 3.2553(3.6446) | Steps 101(96.49) | Grad Norm 2.6099(2.9627) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3007(0.2905) | Bit/dim 3.2476(3.5395) | Steps 101(97.67) | Grad Norm 2.2775(2.8113) | Total Time 1.00(1.00)
Iter 0500 | Time 0.2901(0.2921) | Bit/dim 3.1829(3.4476) | Steps 101(98.55) | Grad Norm 3.7255(2.8246) | Total Time 1.00(1.00)
Iter 0510 | Time 0.2901(0.2916) | Bit/dim 3.0889(3.3642) | Steps 101(99.19) | Grad Norm 2.3325(2.7142) | Total Time 1.00(1.00)
Iter 0520 | Time 0.2903(0.2913) | Bit/dim 3.0647(3.2935) | Steps 101(99.67) | Grad Norm 4.5414(2.7967) | Total Time 1.00(1.00)
Iter 0530 | Time 0.2902(0.2910) | Bit/dim 2.9498(3.2334) | Steps 101(100.02) | Grad Norm 3.5474(2.9582) | Total Time 1.00(1.00)
Iter 0540 | Time 0.3021(0.2918) | Bit/dim 2.9950(3.1777) | Steps 101(100.27) | Grad Norm 3.0512(3.0586) | Total Time 1.00(1.00)
Iter 0550 | Time 0.2915(0.2943) | Bit/dim 2.9058(3.1198) | Steps 101(100.46) | Grad Norm 3.1924(2.9889) | Total Time 1.00(1.00)
Iter 0560 | Time 0.2913(0.2936) | Bit/dim 2.8675(3.0631) | Steps 101(100.61) | Grad Norm 2.6515(2.9611) | Total Time 1.00(1.00)
Iter 0570 | Time 0.2913(0.2931) | Bit/dim 2.8083(3.0217) | Steps 101(100.71) | Grad Norm 2.1001(2.8854) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3005(0.2928) | Bit/dim 2.9471(2.9791) | Steps 101(100.61) | Grad Norm 2.6124(2.8567) | Total Time 1.00(1.00)
Iter 0590 | Time 0.2767(0.2910) | Bit/dim 2.8709(2.9440) | Steps 95(99.76) | Grad Norm 3.0120(2.9168) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 4.7193, Bit/dim 2.7765
Iter 0600 | Time 0.2829(0.2884) | Bit/dim 2.7995(2.9094) | Steps 95(98.82) | Grad Norm 1.6568(2.8291) | Total Time 1.00(1.00)
Iter 0610 | Time 0.2656(0.2858) | Bit/dim 2.7573(2.8766) | Steps 95(97.82) | Grad Norm 2.6820(2.9291) | Total Time 1.00(1.00)
Iter 0620 | Time 0.2809(0.2823) | Bit/dim 2.7369(2.8500) | Steps 95(97.08) | Grad Norm 2.9549(3.0386) | Total Time 1.00(1.00)
Iter 0630 | Time 0.2639(0.2792) | Bit/dim 2.7622(2.8207) | Steps 95(96.53) | Grad Norm 2.7931(3.1596) | Total Time 1.00(1.00)
Iter 0640 | Time 0.2734(0.2763) | Bit/dim 2.7256(2.7915) | Steps 95(96.13) | Grad Norm 3.6971(3.2340) | Total Time 1.00(1.00)
Iter 0650 | Time 0.2754(0.2761) | Bit/dim 2.7377(2.7675) | Steps 95(95.83) | Grad Norm 2.6665(3.2603) | Total Time 1.00(1.00)
Iter 0660 | Time 0.2745(0.2760) | Bit/dim 2.6555(2.7439) | Steps 95(95.61) | Grad Norm 4.7422(3.4017) | Total Time 1.00(1.00)
Iter 0670 | Time 0.2755(0.2758) | Bit/dim 2.6562(2.7255) | Steps 95(95.45) | Grad Norm 2.9435(3.6232) | Total Time 1.00(1.00)
Iter 0680 | Time 0.2757(0.2757) | Bit/dim 2.6624(2.7082) | Steps 95(95.33) | Grad Norm 3.8214(3.6505) | Total Time 1.00(1.00)
Iter 0690 | Time 0.2640(0.2733) | Bit/dim 2.5982(2.6916) | Steps 95(95.25) | Grad Norm 3.2477(3.4407) | Total Time 1.00(1.00)
Iter 0700 | Time 0.2639(0.2708) | Bit/dim 2.5087(2.6587) | Steps 95(95.18) | Grad Norm 2.3728(3.4278) | Total Time 1.00(1.00)
Iter 0710 | Time 0.2638(0.2690) | Bit/dim 2.6152(2.6394) | Steps 95(95.13) | Grad Norm 3.3134(3.3672) | Total Time 1.00(1.00)
Iter 0720 | Time 0.2639(0.2677) | Bit/dim 2.5842(2.6196) | Steps 95(95.10) | Grad Norm 2.7998(3.2503) | Total Time 1.00(1.00)
Iter 0730 | Time 0.2667(0.2693) | Bit/dim 2.5809(2.5996) | Steps 89(94.73) | Grad Norm 4.1373(3.5058) | Total Time 1.00(1.00)
Iter 0740 | Time 0.2640(0.2686) | Bit/dim 2.5699(2.5865) | Steps 95(94.49) | Grad Norm 5.4039(3.6072) | Total Time 1.00(1.00)
Iter 0750 | Time 0.2667(0.2699) | Bit/dim 2.4594(2.5676) | Steps 89(94.31) | Grad Norm 3.3278(3.8555) | Total Time 1.00(1.00)
Iter 0760 | Time 0.2563(0.2677) | Bit/dim 2.5835(2.5515) | Steps 89(93.82) | Grad Norm 2.2765(3.6047) | Total Time 1.00(1.00)
Iter 0770 | Time 0.2707(0.2657) | Bit/dim 2.4807(2.5378) | Steps 95(93.19) | Grad Norm 4.5528(3.5917) | Total Time 1.00(1.00)
Iter 0780 | Time 0.2687(0.2656) | Bit/dim 2.4943(2.5314) | Steps 95(92.60) | Grad Norm 4.2236(3.7251) | Total Time 1.00(1.00)
Iter 0790 | Time 0.2575(0.2656) | Bit/dim 2.4868(2.5221) | Steps 89(91.79) | Grad Norm 7.7603(4.2031) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='cifar10', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='cifar10', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 500 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 18260
Iter 0000 | Time 0.3253(0.3253) | Bit/dim 8.5618(8.5618) | Steps 41(41.00) | Grad Norm 1.1100(1.1100) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1371(0.2762) | Bit/dim 8.5696(8.5856) | Steps 41(41.00) | Grad Norm 1.0520(1.1122) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1371(0.2398) | Bit/dim 8.6968(8.6006) | Steps 41(41.00) | Grad Norm 1.0230(1.1074) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1371(0.2129) | Bit/dim 8.4550(8.6387) | Steps 41(41.00) | Grad Norm 0.9773(1.0934) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1370(0.1931) | Bit/dim 8.4778(8.6417) | Steps 41(41.00) | Grad Norm 0.9337(1.0622) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1370(0.1785) | Bit/dim 8.7890(8.6551) | Steps 41(41.00) | Grad Norm 0.9278(1.0217) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1370(0.1677) | Bit/dim 8.4549(8.6513) | Steps 41(41.00) | Grad Norm 0.8124(0.9686) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1371(0.1598) | Bit/dim 8.6377(8.6530) | Steps 41(41.00) | Grad Norm 0.6498(0.9129) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1370(0.1540) | Bit/dim 8.6342(8.6423) | Steps 41(41.00) | Grad Norm 0.6360(0.8508) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1369(0.1495) | Bit/dim 8.8600(8.6375) | Steps 41(41.00) | Grad Norm 0.5363(0.7809) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1370(0.1463) | Bit/dim 8.6124(8.6100) | Steps 41(41.00) | Grad Norm 0.4987(0.7094) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1369(0.1439) | Bit/dim 8.4822(8.5968) | Steps 41(41.00) | Grad Norm 0.3737(0.6447) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1371(0.1421) | Bit/dim 8.6030(8.5860) | Steps 41(41.00) | Grad Norm 0.3235(0.5686) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1370(0.1408) | Bit/dim 8.5346(8.5687) | Steps 41(41.00) | Grad Norm 0.3283(0.5080) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1369(0.1398) | Bit/dim 8.2340(8.5412) | Steps 41(41.00) | Grad Norm 0.2690(0.4591) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1370(0.1391) | Bit/dim 8.2288(8.5311) | Steps 41(41.00) | Grad Norm 0.3275(0.4537) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1370(0.1386) | Bit/dim 8.7157(8.4998) | Steps 41(41.00) | Grad Norm 0.9627(0.4824) | Total Time 1.00(1.00)
Iter 0170 | Time 0.1370(0.1382) | Bit/dim 8.2548(8.4470) | Steps 41(41.00) | Grad Norm 0.7866(0.5436) | Total Time 1.00(1.00)
Iter 0180 | Time 0.1370(0.1379) | Bit/dim 8.2190(8.4144) | Steps 41(41.00) | Grad Norm 1.6947(0.7779) | Total Time 1.00(1.00)
Iter 0190 | Time 0.1457(0.1386) | Bit/dim 8.0244(8.3295) | Steps 47(41.69) | Grad Norm 1.4230(0.9626) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2049(0.1480) | Bit/dim 7.9900(8.2375) | Steps 59(44.62) | Grad Norm 1.9234(1.1365) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2128(0.1650) | Bit/dim 7.8531(8.1315) | Steps 65(49.97) | Grad Norm 1.6407(1.3191) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2521(0.1894) | Bit/dim 7.6299(8.0180) | Steps 77(57.54) | Grad Norm 1.7923(1.5014) | Total Time 1.00(1.00)
Iter 0230 | Time 0.3188(0.2151) | Bit/dim 7.5674(7.9093) | Steps 95(65.64) | Grad Norm 1.5163(1.5278) | Total Time 1.00(1.00)
Iter 0240 | Time 0.3185(0.2414) | Bit/dim 7.5215(7.8028) | Steps 95(73.24) | Grad Norm 1.3814(1.5139) | Total Time 1.00(1.00)
Iter 0250 | Time 0.3643(0.2657) | Bit/dim 7.4528(7.7052) | Steps 107(80.71) | Grad Norm 1.2529(1.4832) | Total Time 1.00(1.00)
Iter 0260 | Time 0.3919(0.2871) | Bit/dim 7.4202(7.6193) | Steps 119(87.57) | Grad Norm 0.9620(1.3846) | Total Time 1.00(1.00)
Iter 0270 | Time 0.3920(0.3146) | Bit/dim 7.3279(7.5461) | Steps 119(95.82) | Grad Norm 0.7303(1.3146) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3920(0.3349) | Bit/dim 7.3242(7.4820) | Steps 119(101.91) | Grad Norm 0.7460(1.2115) | Total Time 1.00(1.00)
Iter 0290 | Time 0.4058(0.3519) | Bit/dim 7.2630(7.4306) | Steps 119(106.85) | Grad Norm 0.5973(1.0951) | Total Time 1.00(1.00)
Iter 0300 | Time 0.4297(0.3690) | Bit/dim 7.2589(7.3886) | Steps 125(111.33) | Grad Norm 0.5881(0.9778) | Total Time 1.00(1.00)
Iter 0310 | Time 0.4379(0.3850) | Bit/dim 7.1762(7.3461) | Steps 131(115.74) | Grad Norm 0.6130(0.9009) | Total Time 1.00(1.00)
Iter 0320 | Time 0.4445(0.3981) | Bit/dim 7.1754(7.3103) | Steps 131(118.95) | Grad Norm 0.6116(0.8058) | Total Time 1.00(1.00)
Iter 0330 | Time 0.4318(0.4080) | Bit/dim 7.2569(7.2871) | Steps 125(121.31) | Grad Norm 0.3725(0.7431) | Total Time 1.00(1.00)
Iter 0340 | Time 0.4310(0.4150) | Bit/dim 7.1753(7.2651) | Steps 125(123.03) | Grad Norm 0.4695(0.6633) | Total Time 1.00(1.00)
Iter 0350 | Time 0.4305(0.4191) | Bit/dim 7.1612(7.2442) | Steps 125(123.55) | Grad Norm 0.3413(0.5975) | Total Time 1.00(1.00)
Iter 0360 | Time 0.4222(0.4220) | Bit/dim 7.2055(7.2312) | Steps 125(124.09) | Grad Norm 0.3380(0.5402) | Total Time 1.00(1.00)
Iter 0370 | Time 0.4219(0.4220) | Bit/dim 7.1702(7.2130) | Steps 125(124.33) | Grad Norm 0.3072(0.4982) | Total Time 1.00(1.00)
Iter 0380 | Time 0.4298(0.4221) | Bit/dim 7.1532(7.1975) | Steps 131(124.69) | Grad Norm 0.3337(0.4712) | Total Time 1.00(1.00)
Iter 0390 | Time 0.4201(0.4225) | Bit/dim 7.2012(7.1821) | Steps 125(124.77) | Grad Norm 0.3340(0.4496) | Total Time 1.00(1.00)
Iter 0400 | Time 0.4200(0.4219) | Bit/dim 7.1849(7.1748) | Steps 125(124.83) | Grad Norm 0.3698(0.4224) | Total Time 1.00(1.00)
Iter 0410 | Time 0.4335(0.4238) | Bit/dim 7.1549(7.1650) | Steps 125(125.18) | Grad Norm 0.3099(0.4071) | Total Time 1.00(1.00)
Iter 0420 | Time 0.4200(0.4255) | Bit/dim 7.1393(7.1622) | Steps 125(125.60) | Grad Norm 0.3177(0.3894) | Total Time 1.00(1.00)
Iter 0430 | Time 0.4202(0.4241) | Bit/dim 7.1188(7.1612) | Steps 125(125.44) | Grad Norm 0.3736(0.3767) | Total Time 1.00(1.00)
Iter 0440 | Time 0.4202(0.4248) | Bit/dim 7.1268(7.1515) | Steps 125(125.80) | Grad Norm 0.3071(0.3744) | Total Time 1.00(1.00)
Iter 0450 | Time 0.4486(0.4267) | Bit/dim 7.0732(7.1399) | Steps 131(126.28) | Grad Norm 0.2595(0.3682) | Total Time 1.00(1.00)
Iter 0460 | Time 0.4275(0.4277) | Bit/dim 7.0608(7.1260) | Steps 131(126.76) | Grad Norm 0.4466(0.3790) | Total Time 1.00(1.00)
Iter 0470 | Time 0.4487(0.4332) | Bit/dim 7.1291(7.1185) | Steps 131(128.23) | Grad Norm 0.4307(0.3921) | Total Time 1.00(1.00)
Iter 0480 | Time 0.4199(0.4358) | Bit/dim 7.0707(7.1079) | Steps 125(128.77) | Grad Norm 0.4670(0.4010) | Total Time 1.00(1.00)
Iter 0490 | Time 0.4488(0.4388) | Bit/dim 7.0608(7.1030) | Steps 131(129.53) | Grad Norm 0.6445(0.4236) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 5.8976, Bit/dim 7.0683
Iter 0500 | Time 0.4753(0.4427) | Bit/dim 7.0053(7.0917) | Steps 137(130.57) | Grad Norm 0.6162(0.4442) | Total Time 1.00(1.00)
Iter 0510 | Time 0.4271(0.4444) | Bit/dim 7.0707(7.0851) | Steps 131(131.16) | Grad Norm 0.5654(0.4625) | Total Time 1.00(1.00)
Iter 0520 | Time 0.4557(0.4434) | Bit/dim 7.0757(7.0784) | Steps 137(131.18) | Grad Norm 0.4525(0.4580) | Total Time 1.00(1.00)
Iter 0530 | Time 0.4480(0.4430) | Bit/dim 6.9972(7.0658) | Steps 131(131.50) | Grad Norm 0.5110(0.4449) | Total Time 1.00(1.00)
Iter 0540 | Time 0.4555(0.4444) | Bit/dim 6.9508(7.0490) | Steps 137(132.34) | Grad Norm 0.4845(0.4397) | Total Time 1.00(1.00)
Iter 0550 | Time 0.4482(0.4471) | Bit/dim 7.0740(7.0418) | Steps 131(133.39) | Grad Norm 0.3448(0.4134) | Total Time 1.00(1.00)
Iter 0560 | Time 0.4551(0.4482) | Bit/dim 6.9797(7.0279) | Steps 137(133.83) | Grad Norm 0.5290(0.4306) | Total Time 1.00(1.00)
Iter 0570 | Time 0.4481(0.4447) | Bit/dim 7.0416(7.0203) | Steps 131(132.78) | Grad Norm 0.6643(0.4328) | Total Time 1.00(1.00)
Iter 0580 | Time 0.4322(0.4457) | Bit/dim 6.9801(7.0138) | Steps 125(131.84) | Grad Norm 0.3609(0.4239) | Total Time 1.00(1.00)
Iter 0590 | Time 0.4735(0.4467) | Bit/dim 6.9382(7.0080) | Steps 137(131.13) | Grad Norm 0.3203(0.4098) | Total Time 1.00(1.00)
Iter 0600 | Time 0.4361(0.4460) | Bit/dim 7.0462(7.0033) | Steps 125(130.16) | Grad Norm 0.3678(0.4209) | Total Time 1.00(1.00)
Iter 0610 | Time 0.4649(0.4505) | Bit/dim 7.0039(6.9933) | Steps 131(130.55) | Grad Norm 0.2970(0.4170) | Total Time 1.00(1.00)
Iter 0620 | Time 0.4269(0.4521) | Bit/dim 7.0086(6.9852) | Steps 125(130.50) | Grad Norm 0.4326(0.4333) | Total Time 1.00(1.00)
Iter 0630 | Time 0.4220(0.4505) | Bit/dim 6.9422(6.9844) | Steps 125(130.11) | Grad Norm 0.4080(0.4094) | Total Time 1.00(1.00)
Iter 0640 | Time 0.4482(0.4492) | Bit/dim 6.8658(6.9847) | Steps 131(130.02) | Grad Norm 0.7443(0.4043) | Total Time 1.00(1.00)
Iter 0650 | Time 0.4573(0.4463) | Bit/dim 6.9170(6.9854) | Steps 137(129.85) | Grad Norm 0.2976(0.4137) | Total Time 1.00(1.00)
Iter 0660 | Time 0.4599(0.4456) | Bit/dim 7.0224(6.9854) | Steps 131(129.67) | Grad Norm 0.3566(0.4178) | Total Time 1.00(1.00)
Iter 0670 | Time 0.4480(0.4437) | Bit/dim 6.9606(6.9856) | Steps 131(129.36) | Grad Norm 0.6773(0.4204) | Total Time 1.00(1.00)
Iter 0680 | Time 0.4202(0.4427) | Bit/dim 6.9689(6.9786) | Steps 125(129.45) | Grad Norm 0.2944(0.4306) | Total Time 1.00(1.00)
Iter 0690 | Time 0.4480(0.4422) | Bit/dim 6.9283(6.9769) | Steps 131(129.40) | Grad Norm 0.2948(0.4144) | Total Time 1.00(1.00)
Iter 0700 | Time 0.4478(0.4424) | Bit/dim 6.9815(6.9684) | Steps 131(129.67) | Grad Norm 0.4944(0.4340) | Total Time 1.00(1.00)
Iter 0710 | Time 0.4524(0.4419) | Bit/dim 7.0149(6.9692) | Steps 131(129.27) | Grad Norm 0.3989(0.4361) | Total Time 1.00(1.00)
Iter 0720 | Time 0.4529(0.4432) | Bit/dim 6.9262(6.9606) | Steps 131(129.42) | Grad Norm 0.5823(0.4771) | Total Time 1.00(1.00)
Iter 0730 | Time 0.4523(0.4442) | Bit/dim 6.9312(6.9518) | Steps 131(129.54) | Grad Norm 0.3524(0.4982) | Total Time 1.00(1.00)
Iter 0740 | Time 0.4240(0.4441) | Bit/dim 6.9333(6.9579) | Steps 125(129.43) | Grad Norm 0.1849(0.4596) | Total Time 1.00(1.00)
Iter 0750 | Time 0.4240(0.4418) | Bit/dim 6.9396(6.9651) | Steps 125(128.90) | Grad Norm 0.2679(0.4240) | Total Time 1.00(1.00)
Iter 0760 | Time 0.4527(0.4432) | Bit/dim 6.9275(6.9596) | Steps 131(129.16) | Grad Norm 0.3446(0.4072) | Total Time 1.00(1.00)
Iter 0770 | Time 0.4540(0.4435) | Bit/dim 6.9532(6.9560) | Steps 131(129.18) | Grad Norm 0.3356(0.3927) | Total Time 1.00(1.00)
Iter 0780 | Time 0.4724(0.4456) | Bit/dim 6.9653(6.9564) | Steps 131(129.49) | Grad Norm 0.7453(0.4159) | Total Time 1.00(1.00)
Iter 0790 | Time 0.4537(0.4459) | Bit/dim 6.8701(6.9521) | Steps 131(129.55) | Grad Norm 0.3937(0.3970) | Total Time 1.00(1.00)
Iter 0800 | Time 0.4529(0.4478) | Bit/dim 6.9333(6.9442) | Steps 131(129.93) | Grad Norm 0.3049(0.3853) | Total Time 1.00(1.00)
Iter 0810 | Time 0.4869(0.4562) | Bit/dim 6.9086(6.9441) | Steps 131(130.05) | Grad Norm 0.6921(0.4344) | Total Time 1.00(1.00)
Iter 0820 | Time 0.4738(0.4630) | Bit/dim 6.8001(6.9355) | Steps 131(130.30) | Grad Norm 0.7101(0.5230) | Total Time 1.00(1.00)
Iter 0830 | Time 0.4495(0.4606) | Bit/dim 7.0004(6.9443) | Steps 131(130.35) | Grad Norm 0.6224(0.6388) | Total Time 1.00(1.00)
Iter 0840 | Time 0.4498(0.4551) | Bit/dim 6.9266(6.9452) | Steps 131(130.01) | Grad Norm 0.5715(0.6143) | Total Time 1.00(1.00)
Iter 0850 | Time 0.4486(0.4520) | Bit/dim 6.9221(6.9480) | Steps 131(129.98) | Grad Norm 0.4234(0.5988) | Total Time 1.00(1.00)
Iter 0860 | Time 0.4481(0.4509) | Bit/dim 6.9435(6.9488) | Steps 131(130.08) | Grad Norm 0.4256(0.5943) | Total Time 1.00(1.00)
Iter 0870 | Time 0.4485(0.4503) | Bit/dim 6.9191(6.9407) | Steps 131(130.32) | Grad Norm 1.6143(0.6888) | Total Time 1.00(1.00)
Iter 0880 | Time 0.4498(0.4500) | Bit/dim 6.8780(6.9361) | Steps 131(130.50) | Grad Norm 0.8265(0.7745) | Total Time 1.00(1.00)
Iter 0890 | Time 0.4493(0.4498) | Bit/dim 6.9042(6.9346) | Steps 131(130.63) | Grad Norm 0.6634(0.8198) | Total Time 1.00(1.00)
Iter 0900 | Time 0.4496(0.4489) | Bit/dim 6.9341(6.9295) | Steps 131(130.58) | Grad Norm 2.6908(1.0850) | Total Time 1.00(1.00)
Iter 0910 | Time 0.4587(0.4492) | Bit/dim 6.9749(6.9290) | Steps 131(130.39) | Grad Norm 1.0375(1.1610) | Total Time 1.00(1.00)
Iter 0920 | Time 0.4618(0.4511) | Bit/dim 6.8770(6.9259) | Steps 131(130.22) | Grad Norm 0.8235(1.0581) | Total Time 1.00(1.00)
Iter 0930 | Time 0.4560(0.4532) | Bit/dim 6.9241(6.9265) | Steps 131(130.29) | Grad Norm 0.7947(1.0045) | Total Time 1.00(1.00)
Iter 0940 | Time 0.4559(0.4539) | Bit/dim 7.0051(6.9284) | Steps 125(129.64) | Grad Norm 1.0216(0.9850) | Total Time 1.00(1.00)
Iter 0950 | Time 0.4557(0.4437) | Bit/dim 6.9957(6.9323) | Steps 125(126.38) | Grad Norm 0.4625(0.8935) | Total Time 1.00(1.00)
Iter 0960 | Time 0.4178(0.4438) | Bit/dim 6.9174(6.9254) | Steps 119(125.86) | Grad Norm 1.8230(0.9053) | Total Time 1.00(1.00)
Iter 0970 | Time 0.4542(0.4442) | Bit/dim 6.8016(6.9124) | Steps 125(125.49) | Grad Norm 1.4389(0.9957) | Total Time 1.00(1.00)
Iter 0980 | Time 0.3820(0.4406) | Bit/dim 6.8805(6.9099) | Steps 113(124.52) | Grad Norm 0.4789(1.0019) | Total Time 1.00(1.00)
Iter 0990 | Time 0.3641(0.4320) | Bit/dim 6.8942(6.9030) | Steps 107(122.51) | Grad Norm 1.5871(1.0416) | Total Time 1.00(1.00)
validating...
Epoch 0002 | Time 5.4516, Bit/dim 6.8976
Iter 1000 | Time 0.3830(0.4241) | Bit/dim 6.8689(6.8954) | Steps 113(120.45) | Grad Norm 1.0700(1.0281) | Total Time 1.00(1.00)
Iter 1010 | Time 0.3856(0.4147) | Bit/dim 6.9181(6.8967) | Steps 113(118.50) | Grad Norm 0.8096(1.0175) | Total Time 1.00(1.00)
Iter 1020 | Time 0.3849(0.4071) | Bit/dim 6.8484(6.8926) | Steps 113(117.05) | Grad Norm 3.4947(1.3025) | Total Time 1.00(1.00)
Iter 1030 | Time 0.3850(0.3996) | Bit/dim 6.9113(6.8859) | Steps 113(115.49) | Grad Norm 1.9518(1.5160) | Total Time 1.00(1.00)
Iter 1040 | Time 0.3861(0.3955) | Bit/dim 6.8385(6.8856) | Steps 113(114.49) | Grad Norm 0.7988(1.5418) | Total Time 1.00(1.00)
Iter 1050 | Time 0.3776(0.3899) | Bit/dim 6.8531(6.8755) | Steps 107(112.35) | Grad Norm 1.5153(1.6039) | Total Time 1.00(1.00)
Iter 1060 | Time 0.3783(0.3867) | Bit/dim 6.8687(6.8714) | Steps 107(110.95) | Grad Norm 1.3581(1.5087) | Total Time 1.00(1.00)
Iter 1070 | Time 0.3817(0.3846) | Bit/dim 6.8818(6.8686) | Steps 107(109.76) | Grad Norm 0.4798(1.3768) | Total Time 1.00(1.00)
Iter 1080 | Time 0.3579(0.3810) | Bit/dim 6.9147(6.8647) | Steps 101(108.20) | Grad Norm 2.0860(1.4129) | Total Time 1.00(1.00)
Iter 1090 | Time 0.3843(0.3772) | Bit/dim 6.9050(6.8660) | Steps 107(106.92) | Grad Norm 1.9149(1.6562) | Total Time 1.00(1.00)
Iter 1100 | Time 0.3556(0.3724) | Bit/dim 6.8328(6.8650) | Steps 101(105.64) | Grad Norm 1.1166(1.5696) | Total Time 1.00(1.00)
Iter 1110 | Time 0.3612(0.3642) | Bit/dim 6.8150(6.8563) | Steps 101(103.62) | Grad Norm 0.9382(1.4580) | Total Time 1.00(1.00)
Iter 1120 | Time 0.2818(0.3525) | Bit/dim 6.8236(6.8579) | Steps 83(100.56) | Grad Norm 2.9367(1.4005) | Total Time 1.00(1.00)
Iter 1130 | Time 0.2956(0.3430) | Bit/dim 6.8530(6.8603) | Steps 89(98.50) | Grad Norm 1.2189(2.3354) | Total Time 1.00(1.00)
Iter 1140 | Time 0.2956(0.3380) | Bit/dim 6.8711(6.8641) | Steps 89(97.54) | Grad Norm 1.5211(2.1048) | Total Time 1.00(1.00)
Iter 1150 | Time 0.2989(0.3246) | Bit/dim 6.8858(6.8659) | Steps 89(93.90) | Grad Norm 0.7132(1.8070) | Total Time 1.00(1.00)
Iter 1160 | Time 0.2863(0.3128) | Bit/dim 6.7928(6.8544) | Steps 83(91.01) | Grad Norm 0.7679(1.5685) | Total Time 1.00(1.00)
Iter 1170 | Time 0.2903(0.3068) | Bit/dim 6.7980(6.8440) | Steps 83(88.91) | Grad Norm 1.0597(1.4007) | Total Time 1.00(1.00)
Iter 1180 | Time 0.2907(0.3024) | Bit/dim 6.8516(6.8377) | Steps 83(87.36) | Grad Norm 0.7373(1.2123) | Total Time 1.00(1.00)
Iter 1190 | Time 0.2819(0.2972) | Bit/dim 6.7607(6.8284) | Steps 83(86.06) | Grad Norm 1.1692(1.1092) | Total Time 1.00(1.00)
Iter 1200 | Time 0.2912(0.2949) | Bit/dim 6.8613(6.8308) | Steps 83(85.26) | Grad Norm 1.1925(1.2003) | Total Time 1.00(1.00)
Iter 1210 | Time 0.2904(0.2913) | Bit/dim 6.7537(6.8221) | Steps 83(84.36) | Grad Norm 0.6794(1.3248) | Total Time 1.00(1.00)
Iter 1220 | Time 0.2540(0.2876) | Bit/dim 6.7872(6.8177) | Steps 77(83.49) | Grad Norm 1.1729(1.5324) | Total Time 1.00(1.00)
Iter 1230 | Time 0.2550(0.2795) | Bit/dim 6.8425(6.8108) | Steps 77(81.78) | Grad Norm 1.4036(1.5868) | Total Time 1.00(1.00)
Iter 1240 | Time 0.2538(0.2730) | Bit/dim 6.8034(6.8084) | Steps 77(80.53) | Grad Norm 1.8150(1.4948) | Total Time 1.00(1.00)
Iter 1250 | Time 0.2593(0.2695) | Bit/dim 6.7975(6.8005) | Steps 77(79.60) | Grad Norm 1.4148(1.5069) | Total Time 1.00(1.00)
Iter 1260 | Time 0.2539(0.2660) | Bit/dim 6.7936(6.8032) | Steps 77(78.92) | Grad Norm 1.4034(1.5063) | Total Time 1.00(1.00)
Iter 1270 | Time 0.2560(0.2635) | Bit/dim 6.7639(6.8016) | Steps 77(78.41) | Grad Norm 0.6482(1.4532) | Total Time 1.00(1.00)
Iter 1280 | Time 0.2584(0.2620) | Bit/dim 6.8350(6.7979) | Steps 77(78.04) | Grad Norm 1.4903(1.4351) | Total Time 1.00(1.00)
Iter 1290 | Time 0.2548(0.2610) | Bit/dim 6.7707(6.7923) | Steps 77(77.77) | Grad Norm 0.6548(1.4281) | Total Time 1.00(1.00)
Iter 1300 | Time 0.2615(0.2604) | Bit/dim 6.7791(6.7863) | Steps 77(77.57) | Grad Norm 1.3598(1.5118) | Total Time 1.00(1.00)
Iter 1310 | Time 0.2617(0.2574) | Bit/dim 6.7111(6.7839) | Steps 77(76.79) | Grad Norm 0.8616(1.5693) | Total Time 1.00(1.00)
Iter 1320 | Time 0.2542(0.2533) | Bit/dim 6.7304(6.7834) | Steps 77(76.07) | Grad Norm 3.2068(2.1357) | Total Time 1.00(1.00)
Iter 1330 | Time 0.2332(0.2508) | Bit/dim 6.7514(6.7849) | Steps 71(75.47) | Grad Norm 1.1196(2.1124) | Total Time 1.00(1.00)
Iter 1340 | Time 0.2280(0.2452) | Bit/dim 6.8144(6.7876) | Steps 71(74.30) | Grad Norm 1.2960(1.9370) | Total Time 1.00(1.00)
Iter 1350 | Time 0.2270(0.2406) | Bit/dim 6.7147(6.7771) | Steps 71(73.43) | Grad Norm 0.9333(1.7999) | Total Time 1.00(1.00)
Iter 1360 | Time 0.2341(0.2382) | Bit/dim 6.7451(6.7743) | Steps 71(72.79) | Grad Norm 1.4891(1.7046) | Total Time 1.00(1.00)
Iter 1370 | Time 0.2273(0.2362) | Bit/dim 6.6662(6.7684) | Steps 71(72.32) | Grad Norm 0.8549(1.6669) | Total Time 1.00(1.00)
Iter 1380 | Time 0.2347(0.2342) | Bit/dim 6.7548(6.7715) | Steps 71(71.98) | Grad Norm 2.1810(1.6282) | Total Time 1.00(1.00)
Iter 1390 | Time 0.2270(0.2326) | Bit/dim 6.6859(6.7685) | Steps 71(71.72) | Grad Norm 1.4562(1.6223) | Total Time 1.00(1.00)
Iter 1400 | Time 0.2329(0.2309) | Bit/dim 6.7274(6.7711) | Steps 71(70.47) | Grad Norm 1.0853(1.6550) | Total Time 1.00(1.00)
Iter 1410 | Time 0.2257(0.2297) | Bit/dim 6.7844(6.7662) | Steps 71(69.70) | Grad Norm 2.6544(1.6698) | Total Time 1.00(1.00)
Iter 1420 | Time 0.2156(0.2269) | Bit/dim 6.7577(6.7647) | Steps 65(68.46) | Grad Norm 2.8802(1.7295) | Total Time 1.00(1.00)
Iter 1430 | Time 0.2180(0.2244) | Bit/dim 6.7577(6.7658) | Steps 65(67.86) | Grad Norm 1.8696(1.9216) | Total Time 1.00(1.00)
Iter 1440 | Time 0.2226(0.2240) | Bit/dim 6.7917(6.7649) | Steps 65(67.11) | Grad Norm 1.4255(1.7720) | Total Time 1.00(1.00)
Iter 1450 | Time 0.2200(0.2227) | Bit/dim 6.7559(6.7652) | Steps 65(66.56) | Grad Norm 1.0763(1.6108) | Total Time 1.00(1.00)
Iter 1460 | Time 0.2146(0.2204) | Bit/dim 6.7423(6.7678) | Steps 65(66.15) | Grad Norm 5.3744(2.2423) | Total Time 1.00(1.00)
Iter 1470 | Time 0.2149(0.2188) | Bit/dim 6.6908(6.7618) | Steps 65(65.85) | Grad Norm 2.3719(2.3004) | Total Time 1.00(1.00)
Iter 1480 | Time 0.2139(0.2183) | Bit/dim 6.7538(6.7576) | Steps 65(65.62) | Grad Norm 1.8376(2.1039) | Total Time 1.00(1.00)
Iter 1490 | Time 0.2150(0.2172) | Bit/dim 6.7442(6.7557) | Steps 65(65.46) | Grad Norm 1.7745(2.0334) | Total Time 1.00(1.00)
validating...
Epoch 0003 | Time 3.9098, Bit/dim 6.7468
Iter 1500 | Time 0.2241(0.2168) | Bit/dim 6.7469(6.7573) | Steps 65(65.34) | Grad Norm 1.2734(1.8463) | Total Time 1.00(1.00)
Iter 1510 | Time 0.2215(0.2177) | Bit/dim 6.7466(6.7519) | Steps 65(65.25) | Grad Norm 1.5651(1.7409) | Total Time 1.00(1.00)
Iter 1520 | Time 0.2181(0.2186) | Bit/dim 6.7397(6.7456) | Steps 65(65.18) | Grad Norm 0.8468(1.6193) | Total Time 1.00(1.00)
Iter 1530 | Time 0.2208(0.2193) | Bit/dim 6.7762(6.7466) | Steps 65(65.14) | Grad Norm 1.6580(1.5816) | Total Time 1.00(1.00)
Iter 1540 | Time 0.2259(0.2201) | Bit/dim 6.7710(6.7471) | Steps 65(65.10) | Grad Norm 2.0041(1.8267) | Total Time 1.00(1.00)
Iter 1550 | Time 0.2254(0.2209) | Bit/dim 6.7371(6.7392) | Steps 65(65.07) | Grad Norm 1.1165(1.8056) | Total Time 1.00(1.00)
Iter 1560 | Time 0.2241(0.2219) | Bit/dim 6.7242(6.7389) | Steps 65(65.05) | Grad Norm 1.7595(1.9187) | Total Time 1.00(1.00)
Iter 1570 | Time 0.2241(0.2226) | Bit/dim 6.7663(6.7381) | Steps 65(65.04) | Grad Norm 1.0944(1.8018) | Total Time 1.00(1.00)
Iter 1580 | Time 0.2232(0.2228) | Bit/dim 6.8015(6.7399) | Steps 65(65.03) | Grad Norm 3.9103(1.7699) | Total Time 1.00(1.00)
Iter 1590 | Time 0.2233(0.2229) | Bit/dim 6.7742(6.7362) | Steps 65(65.02) | Grad Norm 1.4723(2.0182) | Total Time 1.00(1.00)
Iter 1600 | Time 0.2207(0.2227) | Bit/dim 6.7520(6.7358) | Steps 65(65.02) | Grad Norm 1.4522(1.8643) | Total Time 1.00(1.00)
Iter 1610 | Time 0.2185(0.2223) | Bit/dim 6.7227(6.7333) | Steps 65(65.01) | Grad Norm 1.9917(1.9419) | Total Time 1.00(1.00)
Iter 1620 | Time 0.2209(0.2215) | Bit/dim 6.7573(6.7335) | Steps 65(65.01) | Grad Norm 2.1161(1.9078) | Total Time 1.00(1.00)
Iter 1630 | Time 0.2241(0.2219) | Bit/dim 6.7289(6.7283) | Steps 65(65.01) | Grad Norm 2.2443(2.0142) | Total Time 1.00(1.00)
Iter 1640 | Time 0.2241(0.2223) | Bit/dim 6.6999(6.7241) | Steps 65(65.00) | Grad Norm 1.6899(2.1465) | Total Time 1.00(1.00)
Iter 1650 | Time 0.2191(0.2221) | Bit/dim 6.6508(6.7230) | Steps 65(65.00) | Grad Norm 0.8043(1.8897) | Total Time 1.00(1.00)
Iter 1660 | Time 0.2189(0.2220) | Bit/dim 6.7409(6.7194) | Steps 65(65.15) | Grad Norm 0.9979(1.7762) | Total Time 1.00(1.00)
Iter 1670 | Time 0.2183(0.2210) | Bit/dim 6.7005(6.7168) | Steps 65(65.11) | Grad Norm 2.3393(1.7470) | Total Time 1.00(1.00)
Iter 1680 | Time 0.2238(0.2216) | Bit/dim 6.7368(6.7226) | Steps 65(65.24) | Grad Norm 6.0090(2.4845) | Total Time 1.00(1.00)
Iter 1690 | Time 0.2241(0.2214) | Bit/dim 6.7446(6.7234) | Steps 65(65.18) | Grad Norm 2.6712(2.6198) | Total Time 1.00(1.00)
Iter 1700 | Time 0.2209(0.2208) | Bit/dim 6.7341(6.7192) | Steps 65(65.13) | Grad Norm 3.3025(2.6772) | Total Time 1.00(1.00)
Iter 1710 | Time 0.2219(0.2212) | Bit/dim 6.7148(6.7219) | Steps 65(65.24) | Grad Norm 2.0622(2.6155) | Total Time 1.00(1.00)
Iter 1720 | Time 0.2257(0.2210) | Bit/dim 6.6524(6.7168) | Steps 65(65.18) | Grad Norm 1.5086(2.2971) | Total Time 1.00(1.00)
Iter 1730 | Time 0.2223(0.2267) | Bit/dim 6.7241(6.7145) | Steps 65(66.10) | Grad Norm 1.9098(2.1438) | Total Time 1.00(1.00)
Iter 1740 | Time 0.2245(0.2281) | Bit/dim 6.7671(6.7167) | Steps 65(66.32) | Grad Norm 2.0849(1.9999) | Total Time 1.00(1.00)
Iter 1750 | Time 0.2246(0.2302) | Bit/dim 6.7322(6.7147) | Steps 65(66.58) | Grad Norm 1.2718(1.7865) | Total Time 1.00(1.00)
Iter 1760 | Time 0.2256(0.2332) | Bit/dim 6.6960(6.7142) | Steps 65(67.08) | Grad Norm 1.2488(2.2483) | Total Time 1.00(1.00)
Iter 1770 | Time 0.2252(0.2303) | Bit/dim 6.6372(6.7077) | Steps 65(66.53) | Grad Norm 2.5425(2.2257) | Total Time 1.00(1.00)
Iter 1780 | Time 0.2504(0.2313) | Bit/dim 6.6570(6.7075) | Steps 71(66.81) | Grad Norm 2.2316(2.1568) | Total Time 1.00(1.00)
Iter 1790 | Time 0.2545(0.2373) | Bit/dim 6.7176(6.7087) | Steps 71(68.07) | Grad Norm 1.7999(2.2988) | Total Time 1.00(1.00)
Iter 1800 | Time 0.2578(0.2431) | Bit/dim 6.7270(6.7106) | Steps 77(70.12) | Grad Norm 2.3796(2.1068) | Total Time 1.00(1.00)
Iter 1810 | Time 0.2564(0.2476) | Bit/dim 6.7147(6.7053) | Steps 77(71.64) | Grad Norm 2.2839(2.1038) | Total Time 1.00(1.00)
Iter 1820 | Time 0.2503(0.2507) | Bit/dim 6.6754(6.6998) | Steps 71(72.70) | Grad Norm 2.9827(2.3504) | Total Time 1.00(1.00)
Iter 1830 | Time 0.2533(0.2513) | Bit/dim 6.7054(6.6973) | Steps 77(73.52) | Grad Norm 2.8532(2.4826) | Total Time 1.00(1.00)
Iter 1840 | Time 0.2541(0.2514) | Bit/dim 6.7486(6.7027) | Steps 77(73.99) | Grad Norm 1.8944(2.5159) | Total Time 1.00(1.00)
Iter 1850 | Time 0.2460(0.2511) | Bit/dim 6.6468(6.6937) | Steps 71(73.98) | Grad Norm 3.2132(2.6312) | Total Time 1.00(1.00)
Iter 1860 | Time 0.2530(0.2509) | Bit/dim 6.6936(6.6828) | Steps 77(74.00) | Grad Norm 1.5064(2.6103) | Total Time 1.00(1.00)
Iter 1870 | Time 0.2533(0.2516) | Bit/dim 6.7901(6.6873) | Steps 77(74.79) | Grad Norm 2.4127(2.3600) | Total Time 1.00(1.00)
Iter 1880 | Time 0.2561(0.2522) | Bit/dim 6.7716(6.6903) | Steps 77(75.37) | Grad Norm 4.5675(2.3915) | Total Time 1.00(1.00)
Iter 1890 | Time 0.2301(0.2479) | Bit/dim 6.6470(6.6936) | Steps 71(74.02) | Grad Norm 2.4004(2.9037) | Total Time 1.00(1.00)
Iter 1900 | Time 0.2629(0.2482) | Bit/dim 6.6847(6.6894) | Steps 77(74.22) | Grad Norm 1.7945(2.7278) | Total Time 1.00(1.00)
Iter 1910 | Time 0.2587(0.2515) | Bit/dim 6.7567(6.6931) | Steps 77(74.95) | Grad Norm 1.8050(2.7664) | Total Time 1.00(1.00)
Iter 1920 | Time 0.2598(0.2536) | Bit/dim 6.6538(6.6880) | Steps 77(75.35) | Grad Norm 2.5392(2.7027) | Total Time 1.00(1.00)
Iter 1930 | Time 0.2647(0.2563) | Bit/dim 6.6446(6.6782) | Steps 77(75.78) | Grad Norm 2.2852(2.6836) | Total Time 1.00(1.00)
Iter 1940 | Time 0.2663(0.2582) | Bit/dim 6.6935(6.6798) | Steps 77(75.96) | Grad Norm 1.0616(2.3320) | Total Time 1.00(1.00)
Iter 1950 | Time 0.2557(0.2581) | Bit/dim 6.7080(6.6789) | Steps 77(76.23) | Grad Norm 3.1239(2.3035) | Total Time 1.00(1.00)
Iter 1960 | Time 0.2561(0.2590) | Bit/dim 6.6895(6.6791) | Steps 77(76.43) | Grad Norm 2.4048(2.3430) | Total Time 1.00(1.00)
Iter 1970 | Time 0.2573(0.2591) | Bit/dim 6.6224(6.6760) | Steps 77(76.58) | Grad Norm 2.6512(2.3115) | Total Time 1.00(1.00)
Iter 1980 | Time 0.2615(0.2597) | Bit/dim 6.6827(6.6728) | Steps 77(76.69) | Grad Norm 1.5394(2.2673) | Total Time 1.00(1.00)
Iter 1990 | Time 0.2616(0.2595) | Bit/dim 6.6507(6.6735) | Steps 77(76.77) | Grad Norm 2.9279(2.1986) | Total Time 1.00(1.00)
validating...
Epoch 0004 | Time 4.5988, Bit/dim 6.6642
Iter 2000 | Time 0.2598(0.2591) | Bit/dim 6.6331(6.6736) | Steps 77(76.54) | Grad Norm 2.5440(2.6053) | Total Time 1.00(1.00)
Iter 2010 | Time 0.2594(0.2598) | Bit/dim 6.6073(6.6629) | Steps 77(76.66) | Grad Norm 3.6031(2.5280) | Total Time 1.00(1.00)
Iter 2020 | Time 0.2646(0.2609) | Bit/dim 6.6444(6.6565) | Steps 77(76.75) | Grad Norm 7.1276(2.8558) | Total Time 1.00(1.00)
Iter 2030 | Time 0.2552(0.2608) | Bit/dim 6.6145(6.6591) | Steps 77(76.68) | Grad Norm 3.1881(3.5159) | Total Time 1.00(1.00)
Iter 2040 | Time 0.2550(0.2598) | Bit/dim 6.6564(6.6572) | Steps 77(76.76) | Grad Norm 2.3190(3.2753) | Total Time 1.00(1.00)
Iter 2050 | Time 0.2556(0.2586) | Bit/dim 6.7533(6.6604) | Steps 77(76.82) | Grad Norm 2.9782(2.9915) | Total Time 1.00(1.00)
Iter 2060 | Time 0.2548(0.2577) | Bit/dim 6.6998(6.6584) | Steps 77(76.87) | Grad Norm 2.1533(2.8688) | Total Time 1.00(1.00)
Iter 2070 | Time 0.2556(0.2570) | Bit/dim 6.7120(6.6604) | Steps 77(76.90) | Grad Norm 2.0509(2.7698) | Total Time 1.00(1.00)
Iter 2080 | Time 0.2545(0.2566) | Bit/dim 6.6875(6.6561) | Steps 77(76.93) | Grad Norm 6.2421(2.9626) | Total Time 1.00(1.00)
Iter 2090 | Time 0.2474(0.2554) | Bit/dim 6.6930(6.6614) | Steps 71(76.26) | Grad Norm 3.8248(3.6022) | Total Time 1.00(1.00)
Iter 2100 | Time 0.2558(0.2551) | Bit/dim 6.6085(6.6580) | Steps 77(76.32) | Grad Norm 1.5904(3.2178) | Total Time 1.00(1.00)
Iter 2110 | Time 0.2551(0.2551) | Bit/dim 6.6529(6.6522) | Steps 77(76.50) | Grad Norm 0.8807(2.7567) | Total Time 1.00(1.00)
Iter 2120 | Time 0.2548(0.2550) | Bit/dim 6.6736(6.6500) | Steps 77(76.63) | Grad Norm 1.5571(2.4247) | Total Time 1.00(1.00)
Iter 2130 | Time 0.2546(0.2551) | Bit/dim 6.6728(6.6469) | Steps 77(76.73) | Grad Norm 1.4331(2.3713) | Total Time 1.00(1.00)
Iter 2140 | Time 0.2544(0.2551) | Bit/dim 6.5210(6.6366) | Steps 77(76.80) | Grad Norm 1.9700(2.2952) | Total Time 1.00(1.00)
Iter 2150 | Time 0.2557(0.2551) | Bit/dim 6.6437(6.6382) | Steps 77(76.85) | Grad Norm 2.6470(2.3897) | Total Time 1.00(1.00)
Iter 2160 | Time 0.2545(0.2551) | Bit/dim 6.6896(6.6338) | Steps 77(76.89) | Grad Norm 3.6149(2.3035) | Total Time 1.00(1.00)
Iter 2170 | Time 0.2544(0.2550) | Bit/dim 6.5762(6.6261) | Steps 77(76.92) | Grad Norm 1.5523(2.3427) | Total Time 1.00(1.00)
Iter 2180 | Time 0.2546(0.2550) | Bit/dim 6.6421(6.6198) | Steps 77(76.94) | Grad Norm 2.6394(2.6811) | Total Time 1.00(1.00)
Iter 2190 | Time 0.2545(0.2550) | Bit/dim 6.5769(6.6130) | Steps 77(76.96) | Grad Norm 2.4362(2.5292) | Total Time 1.00(1.00)
Iter 2200 | Time 0.2559(0.2550) | Bit/dim 6.5898(6.6109) | Steps 77(76.97) | Grad Norm 1.8512(2.5947) | Total Time 1.00(1.00)
Iter 2210 | Time 0.2552(0.2550) | Bit/dim 6.5656(6.6090) | Steps 77(76.98) | Grad Norm 4.2556(2.7950) | Total Time 1.00(1.00)
Iter 2220 | Time 0.2548(0.2550) | Bit/dim 6.6317(6.6107) | Steps 77(76.98) | Grad Norm 5.7494(3.4896) | Total Time 1.00(1.00)
Iter 2230 | Time 0.2545(0.2550) | Bit/dim 6.5558(6.6051) | Steps 77(76.99) | Grad Norm 3.7688(3.4132) | Total Time 1.00(1.00)
Iter 2240 | Time 0.2546(0.2550) | Bit/dim 6.6081(6.6097) | Steps 77(76.99) | Grad Norm 3.2500(3.3691) | Total Time 1.00(1.00)
Iter 2250 | Time 0.2549(0.2550) | Bit/dim 6.5983(6.6049) | Steps 77(76.99) | Grad Norm 4.0851(3.4333) | Total Time 1.00(1.00)
Iter 2260 | Time 0.2555(0.2555) | Bit/dim 6.6106(6.6078) | Steps 77(76.99) | Grad Norm 1.3946(3.3079) | Total Time 1.00(1.00)
Iter 2270 | Time 0.2546(0.2554) | Bit/dim 6.5763(6.6016) | Steps 77(77.00) | Grad Norm 3.3477(2.9904) | Total Time 1.00(1.00)
Iter 2280 | Time 0.2545(0.2553) | Bit/dim 6.6190(6.5983) | Steps 77(77.00) | Grad Norm 3.3154(3.0525) | Total Time 1.00(1.00)
Iter 2290 | Time 0.2557(0.2552) | Bit/dim 6.6699(6.5946) | Steps 77(77.00) | Grad Norm 2.9460(3.0375) | Total Time 1.00(1.00)
Iter 2300 | Time 0.2546(0.2552) | Bit/dim 6.6325(6.5949) | Steps 77(77.00) | Grad Norm 3.0326(3.0836) | Total Time 1.00(1.00)
Iter 2310 | Time 0.2555(0.2551) | Bit/dim 6.5541(6.5910) | Steps 77(77.00) | Grad Norm 2.2298(3.0462) | Total Time 1.00(1.00)
Iter 2320 | Time 0.2548(0.2551) | Bit/dim 6.6842(6.5956) | Steps 77(77.00) | Grad Norm 5.9096(3.3124) | Total Time 1.00(1.00)
Iter 2330 | Time 0.2547(0.2551) | Bit/dim 6.6675(6.5993) | Steps 77(77.00) | Grad Norm 4.9453(3.5999) | Total Time 1.00(1.00)
Iter 2340 | Time 0.2655(0.2561) | Bit/dim 6.6915(6.6005) | Steps 83(77.32) | Grad Norm 6.6055(3.9802) | Total Time 1.00(1.00)
Iter 2350 | Time 0.2645(0.2582) | Bit/dim 6.5424(6.5958) | Steps 77(77.23) | Grad Norm 3.0585(4.0507) | Total Time 1.00(1.00)
Iter 2360 | Time 0.2536(0.2579) | Bit/dim 6.5749(6.5936) | Steps 77(77.17) | Grad Norm 2.1952(3.8904) | Total Time 1.00(1.00)
Iter 2370 | Time 0.2531(0.2568) | Bit/dim 6.6272(6.5877) | Steps 77(77.13) | Grad Norm 2.5244(3.5373) | Total Time 1.00(1.00)
Iter 2380 | Time 0.2623(0.2566) | Bit/dim 6.6272(6.5828) | Steps 77(77.09) | Grad Norm 3.5825(3.2820) | Total Time 1.00(1.00)
Iter 2390 | Time 0.2521(0.2559) | Bit/dim 6.5727(6.5738) | Steps 77(77.07) | Grad Norm 1.0779(2.8246) | Total Time 1.00(1.00)
Iter 2400 | Time 0.2527(0.2549) | Bit/dim 6.5581(6.5697) | Steps 77(77.05) | Grad Norm 2.8749(2.5896) | Total Time 1.00(1.00)
Iter 2410 | Time 0.2517(0.2542) | Bit/dim 6.5191(6.5644) | Steps 77(77.04) | Grad Norm 4.9644(3.0563) | Total Time 1.00(1.00)
Iter 2420 | Time 0.2608(0.2540) | Bit/dim 6.5520(6.5663) | Steps 77(77.03) | Grad Norm 2.6796(3.0735) | Total Time 1.00(1.00)
Iter 2430 | Time 0.2619(0.2558) | Bit/dim 6.5614(6.5679) | Steps 77(77.02) | Grad Norm 2.3535(2.7850) | Total Time 1.00(1.00)
Iter 2440 | Time 0.2611(0.2575) | Bit/dim 6.5063(6.5730) | Steps 77(77.02) | Grad Norm 5.4048(3.4863) | Total Time 1.00(1.00)
Iter 2450 | Time 0.2529(0.2571) | Bit/dim 6.5742(6.5686) | Steps 77(77.01) | Grad Norm 3.6273(3.5081) | Total Time 1.00(1.00)
Iter 2460 | Time 0.2515(0.2558) | Bit/dim 6.6336(6.5671) | Steps 77(77.01) | Grad Norm 3.6495(3.5207) | Total Time 1.00(1.00)
Iter 2470 | Time 0.2515(0.2549) | Bit/dim 6.5870(6.5635) | Steps 77(77.18) | Grad Norm 4.7536(3.3307) | Total Time 1.00(1.00)
Iter 2480 | Time 0.2532(0.2550) | Bit/dim 6.5752(6.5668) | Steps 77(77.80) | Grad Norm 4.7218(3.4562) | Total Time 1.00(1.00)
Iter 2490 | Time 0.2519(0.2542) | Bit/dim 6.4890(6.5622) | Steps 77(77.59) | Grad Norm 0.9293(3.1240) | Total Time 1.00(1.00)
validating...
Epoch 0005 | Time 4.4259, Bit/dim 6.5548
Iter 2500 | Time 0.2524(0.2542) | Bit/dim 6.4945(6.5595) | Steps 77(77.90) | Grad Norm 3.2386(2.8628) | Total Time 1.00(1.00)
Iter 2510 | Time 0.2628(0.2554) | Bit/dim 6.5062(6.5487) | Steps 77(77.83) | Grad Norm 3.2058(3.0963) | Total Time 1.00(1.00)
Iter 2520 | Time 0.2620(0.2577) | Bit/dim 6.5162(6.5500) | Steps 77(78.11) | Grad Norm 2.8122(3.0829) | Total Time 1.00(1.00)
Iter 2530 | Time 0.2618(0.2588) | Bit/dim 6.5985(6.5518) | Steps 77(77.82) | Grad Norm 2.6252(2.9248) | Total Time 1.00(1.00)
Iter 2540 | Time 0.2520(0.2573) | Bit/dim 6.4778(6.5508) | Steps 77(77.60) | Grad Norm 5.7216(3.5550) | Total Time 1.00(1.00)
Iter 2550 | Time 0.2529(0.2565) | Bit/dim 6.5890(6.5557) | Steps 77(77.88) | Grad Norm 5.1072(3.8832) | Total Time 1.00(1.00)
Iter 2560 | Time 0.2520(0.2556) | Bit/dim 6.5323(6.5534) | Steps 77(77.82) | Grad Norm 2.3017(3.8067) | Total Time 1.00(1.00)
Iter 2570 | Time 0.2515(0.2551) | Bit/dim 6.6116(6.5598) | Steps 77(77.93) | Grad Norm 2.8169(3.7646) | Total Time 1.00(1.00)
Iter 2580 | Time 0.2600(0.2552) | Bit/dim 6.5889(6.5604) | Steps 83(78.49) | Grad Norm 3.1696(3.5625) | Total Time 1.00(1.00)
Iter 2590 | Time 0.2585(0.2553) | Bit/dim 6.5691(6.5531) | Steps 83(78.91) | Grad Norm 3.1046(3.4225) | Total Time 1.00(1.00)
Iter 2600 | Time 0.2596(0.2557) | Bit/dim 6.5725(6.5455) | Steps 83(79.52) | Grad Norm 2.3180(3.2494) | Total Time 1.00(1.00)
Iter 2610 | Time 0.2521(0.2555) | Bit/dim 6.5816(6.5497) | Steps 77(79.49) | Grad Norm 2.7417(3.1100) | Total Time 1.00(1.00)
Iter 2620 | Time 0.2585(0.2559) | Bit/dim 6.5402(6.5381) | Steps 83(79.98) | Grad Norm 5.1137(3.0767) | Total Time 1.00(1.00)
Iter 2630 | Time 0.2595(0.2561) | Bit/dim 6.5322(6.5364) | Steps 83(80.29) | Grad Norm 4.0646(3.6304) | Total Time 1.00(1.00)
Iter 2640 | Time 0.2584(0.2559) | Bit/dim 6.5534(6.5368) | Steps 83(80.10) | Grad Norm 2.9928(3.5306) | Total Time 1.00(1.00)
Iter 2650 | Time 0.2520(0.2560) | Bit/dim 6.4998(6.5372) | Steps 77(80.24) | Grad Norm 2.2562(3.2040) | Total Time 1.00(1.00)
Iter 2660 | Time 0.2515(0.2563) | Bit/dim 6.5261(6.5336) | Steps 77(80.64) | Grad Norm 4.4264(3.2864) | Total Time 1.00(1.00)
Iter 2670 | Time 0.2519(0.2562) | Bit/dim 6.4738(6.5321) | Steps 77(80.62) | Grad Norm 2.8450(3.3830) | Total Time 1.00(1.00)
Iter 2680 | Time 0.2585(0.2565) | Bit/dim 6.5581(6.5271) | Steps 83(80.93) | Grad Norm 3.5026(3.2759) | Total Time 1.00(1.00)
Iter 2690 | Time 0.2527(0.2565) | Bit/dim 6.4749(6.5220) | Steps 77(80.86) | Grad Norm 4.5869(3.4267) | Total Time 1.00(1.00)
Iter 2700 | Time 0.2519(0.2563) | Bit/dim 6.4942(6.5222) | Steps 77(80.60) | Grad Norm 5.1229(3.6067) | Total Time 1.00(1.00)
Iter 2710 | Time 0.2586(0.2568) | Bit/dim 6.5817(6.5257) | Steps 83(81.08) | Grad Norm 2.8309(3.6203) | Total Time 1.00(1.00)
Iter 2720 | Time 0.2597(0.2569) | Bit/dim 6.5499(6.5253) | Steps 83(81.12) | Grad Norm 4.4756(3.9299) | Total Time 1.00(1.00)
Iter 2730 | Time 0.2594(0.2573) | Bit/dim 6.5213(6.5221) | Steps 83(81.47) | Grad Norm 4.2215(3.9289) | Total Time 1.00(1.00)
Iter 2740 | Time 0.2592(0.2573) | Bit/dim 6.5511(6.5228) | Steps 83(81.44) | Grad Norm 3.4074(3.7927) | Total Time 1.00(1.00)
Iter 2750 | Time 0.2518(0.2574) | Bit/dim 6.4715(6.5168) | Steps 77(81.50) | Grad Norm 3.4121(3.6343) | Total Time 1.00(1.00)
Iter 2760 | Time 0.2583(0.2578) | Bit/dim 6.5118(6.5176) | Steps 83(81.89) | Grad Norm 3.4447(3.4621) | Total Time 1.00(1.00)
Iter 2770 | Time 0.2630(0.2582) | Bit/dim 6.5254(6.5262) | Steps 83(82.18) | Grad Norm 5.0287(4.0485) | Total Time 1.00(1.00)
Iter 2780 | Time 0.2593(0.2582) | Bit/dim 6.5196(6.5236) | Steps 83(82.25) | Grad Norm 6.2731(4.1324) | Total Time 1.00(1.00)
Iter 2790 | Time 0.2516(0.2577) | Bit/dim 6.4986(6.5192) | Steps 77(81.83) | Grad Norm 3.7683(4.0473) | Total Time 1.00(1.00)
Iter 2800 | Time 0.2519(0.2564) | Bit/dim 6.5245(6.5142) | Steps 77(80.73) | Grad Norm 2.8992(3.9772) | Total Time 1.00(1.00)
Iter 2810 | Time 0.2532(0.2569) | Bit/dim 6.5335(6.5170) | Steps 77(80.06) | Grad Norm 3.3929(4.0282) | Total Time 1.00(1.00)
Iter 2820 | Time 0.2586(0.2568) | Bit/dim 6.5691(6.5202) | Steps 83(80.20) | Grad Norm 2.8133(3.7169) | Total Time 1.00(1.00)
Iter 2830 | Time 0.2542(0.2572) | Bit/dim 6.4981(6.5199) | Steps 77(80.30) | Grad Norm 5.4493(4.0069) | Total Time 1.00(1.00)
Iter 2840 | Time 0.2567(0.2586) | Bit/dim 6.4978(6.5193) | Steps 77(80.23) | Grad Norm 3.8410(3.9630) | Total Time 1.00(1.00)
Iter 2850 | Time 0.2719(0.2609) | Bit/dim 6.5859(6.5209) | Steps 83(80.52) | Grad Norm 1.7850(3.6585) | Total Time 1.00(1.00)
Iter 2860 | Time 0.2626(0.2605) | Bit/dim 6.5382(6.5200) | Steps 83(80.53) | Grad Norm 3.4715(3.4091) | Total Time 1.00(1.00)
Iter 2870 | Time 0.2587(0.2598) | Bit/dim 6.5452(6.5146) | Steps 83(80.85) | Grad Norm 4.2840(3.3337) | Total Time 1.00(1.00)
Iter 2880 | Time 0.2596(0.2595) | Bit/dim 6.4539(6.5113) | Steps 83(81.25) | Grad Norm 3.1002(3.5680) | Total Time 1.00(1.00)
Iter 2890 | Time 0.2666(0.2605) | Bit/dim 6.5286(6.5110) | Steps 83(81.71) | Grad Norm 2.0080(3.4110) | Total Time 1.00(1.00)
Iter 2900 | Time 0.2591(0.2609) | Bit/dim 6.4893(6.5040) | Steps 83(82.05) | Grad Norm 3.3013(3.2773) | Total Time 1.00(1.00)
Iter 2910 | Time 0.2613(0.2607) | Bit/dim 6.4896(6.5066) | Steps 83(82.30) | Grad Norm 3.5813(3.7148) | Total Time 1.00(1.00)
Iter 2920 | Time 0.2730(0.2639) | Bit/dim 6.4788(6.5070) | Steps 83(82.48) | Grad Norm 4.3347(3.7062) | Total Time 1.00(1.00)
Iter 2930 | Time 0.2619(0.2643) | Bit/dim 6.5068(6.5048) | Steps 83(82.62) | Grad Norm 7.5415(4.1921) | Total Time 1.00(1.00)
Iter 2940 | Time 0.2629(0.2657) | Bit/dim 6.5471(6.5064) | Steps 83(82.72) | Grad Norm 2.6317(4.2450) | Total Time 1.00(1.00)
Iter 2950 | Time 0.2747(0.2677) | Bit/dim 6.4557(6.5050) | Steps 83(82.79) | Grad Norm 2.8439(3.9410) | Total Time 1.00(1.00)
Iter 2960 | Time 0.2861(0.2703) | Bit/dim 6.4695(6.4988) | Steps 83(82.67) | Grad Norm 3.2250(4.0986) | Total Time 1.00(1.00)
Iter 2970 | Time 0.2671(0.2696) | Bit/dim 6.5289(6.4990) | Steps 83(82.27) | Grad Norm 7.7857(4.4255) | Total Time 1.00(1.00)
Iter 2980 | Time 0.2699(0.2678) | Bit/dim 6.4794(6.4936) | Steps 83(82.00) | Grad Norm 5.7269(4.7495) | Total Time 1.00(1.00)
Iter 2990 | Time 0.2614(0.2651) | Bit/dim 6.4718(6.4887) | Steps 83(81.35) | Grad Norm 2.1734(4.3428) | Total Time 1.00(1.00)
validating...
Epoch 0006 | Time 4.9907, Bit/dim 6.4865
Iter 3000 | Time 0.2664(0.2650) | Bit/dim 6.4835(6.4835) | Steps 83(81.15) | Grad Norm 2.1443(3.8956) | Total Time 1.00(1.00)
Iter 3010 | Time 0.2695(0.2661) | Bit/dim 6.4908(6.4868) | Steps 83(81.64) | Grad Norm 1.6434(3.5888) | Total Time 1.00(1.00)
Iter 3020 | Time 0.2698(0.2686) | Bit/dim 6.5794(6.4906) | Steps 83(81.99) | Grad Norm 3.1076(3.4885) | Total Time 1.00(1.00)
Iter 3030 | Time 0.2868(0.2713) | Bit/dim 6.5270(6.4958) | Steps 83(82.11) | Grad Norm 4.0345(3.4189) | Total Time 1.00(1.00)
Iter 3040 | Time 0.2609(0.2701) | Bit/dim 6.4351(6.4834) | Steps 77(82.02) | Grad Norm 2.6596(3.3474) | Total Time 1.00(1.00)
Iter 3050 | Time 0.2715(0.2692) | Bit/dim 6.4363(6.4818) | Steps 83(82.28) | Grad Norm 3.2535(3.2161) | Total Time 1.00(1.00)
Iter 3060 | Time 0.2729(0.2710) | Bit/dim 6.5287(6.4895) | Steps 83(82.47) | Grad Norm 5.1851(3.2894) | Total Time 1.00(1.00)
Iter 3070 | Time 0.2638(0.2689) | Bit/dim 6.5387(6.4909) | Steps 83(82.61) | Grad Norm 3.3633(3.2705) | Total Time 1.00(1.00)
Iter 3080 | Time 0.2714(0.2690) | Bit/dim 6.4163(6.4893) | Steps 83(82.71) | Grad Norm 2.5331(3.4436) | Total Time 1.00(1.00)
Iter 3090 | Time 0.2652(0.2689) | Bit/dim 6.4310(6.4864) | Steps 83(82.79) | Grad Norm 2.8793(3.4606) | Total Time 1.00(1.00)
Iter 3100 | Time 0.2765(0.2680) | Bit/dim 6.4806(6.4798) | Steps 83(82.84) | Grad Norm 3.8178(3.6700) | Total Time 1.00(1.00)
Iter 3110 | Time 0.2620(0.2669) | Bit/dim 6.5291(6.4811) | Steps 83(82.88) | Grad Norm 5.7947(4.1418) | Total Time 1.00(1.00)
Iter 3120 | Time 0.2717(0.2671) | Bit/dim 6.5274(6.4851) | Steps 83(82.75) | Grad Norm 5.4678(4.5396) | Total Time 1.00(1.00)
Iter 3130 | Time 0.2705(0.2682) | Bit/dim 6.5293(6.4838) | Steps 83(82.66) | Grad Norm 4.2330(4.7575) | Total Time 1.00(1.00)
Iter 3140 | Time 0.2605(0.2681) | Bit/dim 6.4474(6.4786) | Steps 83(82.75) | Grad Norm 3.0618(4.4648) | Total Time 1.00(1.00)
Iter 3150 | Time 0.2583(0.2659) | Bit/dim 6.4963(6.4774) | Steps 83(82.82) | Grad Norm 2.3028(4.0071) | Total Time 1.00(1.00)
Iter 3160 | Time 0.2669(0.2652) | Bit/dim 6.4421(6.4729) | Steps 83(82.86) | Grad Norm 3.3673(3.7466) | Total Time 1.00(1.00)
Iter 3170 | Time 0.2634(0.2678) | Bit/dim 6.4954(6.4721) | Steps 83(82.90) | Grad Norm 3.0124(3.8911) | Total Time 1.00(1.00)
Iter 3180 | Time 0.2714(0.2688) | Bit/dim 6.4463(6.4721) | Steps 83(82.93) | Grad Norm 7.2299(3.8556) | Total Time 1.00(1.00)
Iter 3190 | Time 0.2635(0.2712) | Bit/dim 6.4758(6.4676) | Steps 83(82.95) | Grad Norm 4.0253(3.9482) | Total Time 1.00(1.00)
Iter 3200 | Time 0.2904(0.2735) | Bit/dim 6.4778(6.4697) | Steps 83(82.96) | Grad Norm 2.0900(3.6836) | Total Time 1.00(1.00)
Iter 3210 | Time 0.2712(0.2783) | Bit/dim 6.4853(6.4701) | Steps 83(82.97) | Grad Norm 3.2112(3.7999) | Total Time 1.00(1.00)
Iter 3220 | Time 0.2695(0.2804) | Bit/dim 6.5258(6.4674) | Steps 83(82.98) | Grad Norm 2.8068(3.6420) | Total Time 1.00(1.00)
Iter 3230 | Time 0.2581(0.2800) | Bit/dim 6.4741(6.4699) | Steps 83(82.98) | Grad Norm 8.7342(4.6032) | Total Time 1.00(1.00)
Iter 3240 | Time 0.2655(0.2791) | Bit/dim 6.4924(6.4657) | Steps 83(82.99) | Grad Norm 5.3705(4.7153) | Total Time 1.00(1.00)
Iter 3250 | Time 0.2904(0.2765) | Bit/dim 6.3979(6.4641) | Steps 83(82.85) | Grad Norm 3.4462(4.4674) | Total Time 1.00(1.00)
Iter 3260 | Time 0.2707(0.2771) | Bit/dim 6.4074(6.4583) | Steps 83(82.89) | Grad Norm 3.9450(4.0596) | Total Time 1.00(1.00)
Iter 3270 | Time 0.2825(0.2762) | Bit/dim 6.5118(6.4564) | Steps 83(82.92) | Grad Norm 4.2976(3.9546) | Total Time 1.00(1.00)
Iter 3280 | Time 0.3017(0.2780) | Bit/dim 6.4559(6.4545) | Steps 83(82.94) | Grad Norm 2.3056(3.7632) | Total Time 1.00(1.00)
Iter 3290 | Time 0.2712(0.2759) | Bit/dim 6.4578(6.4531) | Steps 83(82.95) | Grad Norm 3.4415(3.5320) | Total Time 1.00(1.00)
Iter 3300 | Time 0.2707(0.2750) | Bit/dim 6.4357(6.4539) | Steps 83(82.97) | Grad Norm 3.5969(3.4366) | Total Time 1.00(1.00)
Iter 3310 | Time 0.2718(0.2745) | Bit/dim 6.4846(6.4630) | Steps 83(82.98) | Grad Norm 5.1729(3.7470) | Total Time 1.00(1.00)
Iter 3320 | Time 0.2777(0.2772) | Bit/dim 6.4821(6.4629) | Steps 83(82.98) | Grad Norm 4.0261(3.8966) | Total Time 1.00(1.00)
Iter 3330 | Time 0.2631(0.2760) | Bit/dim 6.4212(6.4564) | Steps 83(82.99) | Grad Norm 2.7894(3.7298) | Total Time 1.00(1.00)
Iter 3340 | Time 0.2944(0.2787) | Bit/dim 6.4572(6.4601) | Steps 83(82.99) | Grad Norm 3.8878(4.0805) | Total Time 1.00(1.00)
Iter 3350 | Time 0.2689(0.2785) | Bit/dim 6.4984(6.4539) | Steps 83(82.99) | Grad Norm 7.3614(4.4052) | Total Time 1.00(1.00)
Iter 3360 | Time 0.2770(0.2794) | Bit/dim 6.5148(6.4507) | Steps 83(82.99) | Grad Norm 4.1420(4.8446) | Total Time 1.00(1.00)
Iter 3370 | Time 0.2674(0.2778) | Bit/dim 6.4412(6.4510) | Steps 83(83.00) | Grad Norm 4.3901(4.9765) | Total Time 1.00(1.00)
Iter 3380 | Time 0.2762(0.2765) | Bit/dim 6.4604(6.4524) | Steps 83(83.00) | Grad Norm 3.3941(4.8988) | Total Time 1.00(1.00)
Iter 3390 | Time 0.2719(0.2732) | Bit/dim 6.4139(6.4513) | Steps 83(83.00) | Grad Norm 1.9043(4.4202) | Total Time 1.00(1.00)
Iter 3400 | Time 0.2686(0.2727) | Bit/dim 6.4495(6.4516) | Steps 83(83.00) | Grad Norm 1.2370(3.8888) | Total Time 1.00(1.00)
Iter 3410 | Time 0.2724(0.2713) | Bit/dim 6.4636(6.4517) | Steps 83(83.00) | Grad Norm 2.5532(3.5131) | Total Time 1.00(1.00)
Iter 3420 | Time 0.2681(0.2709) | Bit/dim 6.4063(6.4483) | Steps 83(83.00) | Grad Norm 2.8087(3.3441) | Total Time 1.00(1.00)
Iter 3430 | Time 0.2673(0.2706) | Bit/dim 6.4597(6.4468) | Steps 83(83.00) | Grad Norm 2.4199(3.1962) | Total Time 1.00(1.00)
Iter 3440 | Time 0.2635(0.2691) | Bit/dim 6.4280(6.4398) | Steps 83(83.00) | Grad Norm 1.8429(2.8679) | Total Time 1.00(1.00)
Iter 3450 | Time 0.2673(0.2687) | Bit/dim 6.4739(6.4460) | Steps 83(83.00) | Grad Norm 8.1873(3.5730) | Total Time 1.00(1.00)
Iter 3460 | Time 0.2678(0.2679) | Bit/dim 6.3887(6.4467) | Steps 83(83.00) | Grad Norm 4.8048(3.9393) | Total Time 1.00(1.00)
Iter 3470 | Time 0.2637(0.2673) | Bit/dim 6.3862(6.4473) | Steps 83(83.00) | Grad Norm 3.6111(4.2581) | Total Time 1.00(1.00)
Iter 3480 | Time 0.2663(0.2669) | Bit/dim 6.3854(6.4404) | Steps 83(83.00) | Grad Norm 3.9504(4.3743) | Total Time 1.00(1.00)
Iter 3490 | Time 0.2710(0.2705) | Bit/dim 6.4207(6.4446) | Steps 83(83.00) | Grad Norm 3.3488(4.4079) | Total Time 1.00(1.00)
validating...
Epoch 0007 | Time 5.0174, Bit/dim 6.4501
Iter 3500 | Time 0.2747(0.2710) | Bit/dim 6.4764(6.4492) | Steps 83(83.00) | Grad Norm 5.9392(4.8225) | Total Time 1.00(1.00)
Iter 3510 | Time 0.2706(0.2714) | Bit/dim 6.4935(6.4514) | Steps 83(83.00) | Grad Norm 3.4605(4.7394) | Total Time 1.00(1.00)
Iter 3520 | Time 0.2754(0.2707) | Bit/dim 6.4223(6.4467) | Steps 83(83.00) | Grad Norm 4.3922(4.5579) | Total Time 1.00(1.00)
Iter 3530 | Time 0.2734(0.2717) | Bit/dim 6.4411(6.4445) | Steps 83(83.00) | Grad Norm 7.4571(4.6089) | Total Time 1.00(1.00)
Iter 3540 | Time 0.2598(0.2694) | Bit/dim 6.5350(6.4499) | Steps 83(83.00) | Grad Norm 4.8574(5.1393) | Total Time 1.00(1.00)
Iter 3550 | Time 0.2599(0.2668) | Bit/dim 6.3972(6.4462) | Steps 83(83.00) | Grad Norm 2.8595(4.9374) | Total Time 1.00(1.00)
Iter 3560 | Time 0.2690(0.2652) | Bit/dim 6.4834(6.4442) | Steps 83(83.00) | Grad Norm 2.9710(4.4317) | Total Time 1.00(1.00)
Iter 3570 | Time 0.2700(0.2662) | Bit/dim 6.4937(6.4411) | Steps 83(83.00) | Grad Norm 2.9657(4.0202) | Total Time 1.00(1.00)
Iter 3580 | Time 0.2710(0.2675) | Bit/dim 6.5201(6.4420) | Steps 83(83.00) | Grad Norm 2.3582(3.5598) | Total Time 1.00(1.00)
Iter 3590 | Time 0.2668(0.2677) | Bit/dim 6.4027(6.4461) | Steps 83(83.00) | Grad Norm 3.3158(3.4051) | Total Time 1.00(1.00)
Iter 3600 | Time 0.2751(0.2694) | Bit/dim 6.4302(6.4451) | Steps 83(83.00) | Grad Norm 3.4067(3.2906) | Total Time 1.00(1.00)
Iter 3610 | Time 0.2658(0.2696) | Bit/dim 6.3687(6.4367) | Steps 83(83.00) | Grad Norm 2.3333(3.1800) | Total Time 1.00(1.00)
Iter 3620 | Time 0.2738(0.2697) | Bit/dim 6.4919(6.4384) | Steps 83(83.00) | Grad Norm 2.1548(3.0462) | Total Time 1.00(1.00)
Iter 3630 | Time 0.2676(0.2707) | Bit/dim 6.4912(6.4376) | Steps 83(83.00) | Grad Norm 3.2273(3.5457) | Total Time 1.00(1.00)
Iter 3640 | Time 0.2723(0.2705) | Bit/dim 6.4893(6.4357) | Steps 83(83.00) | Grad Norm 3.8804(3.9616) | Total Time 1.00(1.00)
Iter 3650 | Time 0.2713(0.2708) | Bit/dim 6.4032(6.4338) | Steps 83(83.00) | Grad Norm 1.9165(4.0149) | Total Time 1.00(1.00)
Iter 3660 | Time 0.2616(0.2697) | Bit/dim 6.4281(6.4340) | Steps 83(83.00) | Grad Norm 3.4109(4.2483) | Total Time 1.00(1.00)
Iter 3670 | Time 0.2698(0.2691) | Bit/dim 6.4629(6.4347) | Steps 83(83.00) | Grad Norm 2.5060(4.0650) | Total Time 1.00(1.00)
Iter 3680 | Time 0.2698(0.2695) | Bit/dim 6.3514(6.4244) | Steps 83(83.00) | Grad Norm 2.8446(3.8976) | Total Time 1.00(1.00)
Iter 3690 | Time 0.2790(0.2688) | Bit/dim 6.3918(6.4219) | Steps 83(83.00) | Grad Norm 2.6720(3.6602) | Total Time 1.00(1.00)
Iter 3700 | Time 0.3014(0.2689) | Bit/dim 6.4380(6.4216) | Steps 95(83.53) | Grad Norm 5.9479(3.8787) | Total Time 1.00(1.00)
Iter 3710 | Time 0.2604(0.2671) | Bit/dim 6.4543(6.4237) | Steps 83(83.39) | Grad Norm 3.3272(4.1507) | Total Time 1.00(1.00)
Iter 3720 | Time 0.2658(0.2677) | Bit/dim 6.4550(6.4266) | Steps 83(83.29) | Grad Norm 3.0191(4.0742) | Total Time 1.00(1.00)
Iter 3730 | Time 0.2654(0.2674) | Bit/dim 6.4212(6.4266) | Steps 83(83.21) | Grad Norm 4.5636(4.2115) | Total Time 1.00(1.00)
Iter 3740 | Time 0.2709(0.2681) | Bit/dim 6.4234(6.4275) | Steps 83(83.16) | Grad Norm 4.7463(4.3004) | Total Time 1.00(1.00)
Iter 3750 | Time 0.2631(0.2704) | Bit/dim 6.4890(6.4243) | Steps 83(83.12) | Grad Norm 4.8546(4.4318) | Total Time 1.00(1.00)
Iter 3760 | Time 0.2617(0.2702) | Bit/dim 6.4298(6.4223) | Steps 83(83.23) | Grad Norm 3.9754(4.4280) | Total Time 1.00(1.00)
Iter 3770 | Time 0.2674(0.2690) | Bit/dim 6.4835(6.4250) | Steps 83(83.17) | Grad Norm 4.9213(4.2720) | Total Time 1.00(1.00)
Iter 3780 | Time 0.2646(0.2685) | Bit/dim 6.4414(6.4281) | Steps 83(83.27) | Grad Norm 7.7847(5.0819) | Total Time 1.00(1.00)
Iter 3790 | Time 0.2632(0.2675) | Bit/dim 6.3925(6.4308) | Steps 83(83.20) | Grad Norm 4.9969(5.2986) | Total Time 1.00(1.00)
Iter 3800 | Time 0.2644(0.2670) | Bit/dim 6.3784(6.4308) | Steps 83(83.15) | Grad Norm 2.3398(4.8054) | Total Time 1.00(1.00)
Iter 3810 | Time 0.2748(0.2695) | Bit/dim 6.3847(6.4289) | Steps 83(83.44) | Grad Norm 4.2313(4.7094) | Total Time 1.00(1.00)
Iter 3820 | Time 0.2638(0.2690) | Bit/dim 6.4549(6.4256) | Steps 83(83.63) | Grad Norm 3.0276(4.4806) | Total Time 1.00(1.00)
Iter 3830 | Time 0.2673(0.2696) | Bit/dim 6.3745(6.4210) | Steps 83(83.64) | Grad Norm 2.4944(4.0697) | Total Time 1.00(1.00)
Iter 3840 | Time 0.2706(0.2712) | Bit/dim 6.3861(6.4166) | Steps 83(84.11) | Grad Norm 3.3040(3.8517) | Total Time 1.00(1.00)
Iter 3850 | Time 0.3091(0.2778) | Bit/dim 6.4117(6.4170) | Steps 95(85.57) | Grad Norm 4.6531(3.9929) | Total Time 1.00(1.00)
Iter 3860 | Time 0.2842(0.2792) | Bit/dim 6.3908(6.4176) | Steps 89(85.83) | Grad Norm 4.2912(4.1403) | Total Time 1.00(1.00)
Iter 3870 | Time 0.2733(0.2875) | Bit/dim 6.4360(6.4274) | Steps 83(85.26) | Grad Norm 4.4412(4.1841) | Total Time 1.00(1.00)
Iter 3880 | Time 0.2584(0.2833) | Bit/dim 6.3777(6.4257) | Steps 83(84.99) | Grad Norm 3.3716(4.2638) | Total Time 1.00(1.00)
Iter 3890 | Time 0.2598(0.2791) | Bit/dim 6.4012(6.4226) | Steps 83(84.91) | Grad Norm 2.0873(4.0070) | Total Time 1.00(1.00)
Iter 3900 | Time 0.2665(0.2778) | Bit/dim 6.4460(6.4249) | Steps 89(85.53) | Grad Norm 5.4797(4.0844) | Total Time 1.00(1.00)
Iter 3910 | Time 0.2584(0.2739) | Bit/dim 6.4097(6.4193) | Steps 83(85.19) | Grad Norm 4.2599(3.9647) | Total Time 1.00(1.00)
Iter 3920 | Time 0.2975(0.2755) | Bit/dim 6.4257(6.4164) | Steps 95(86.18) | Grad Norm 5.1077(4.3230) | Total Time 1.00(1.00)
Iter 3930 | Time 0.2663(0.2712) | Bit/dim 6.3861(6.4138) | Steps 89(85.52) | Grad Norm 4.2763(4.2654) | Total Time 1.00(1.00)
Iter 3940 | Time 0.2581(0.2717) | Bit/dim 6.3961(6.4143) | Steps 83(86.07) | Grad Norm 4.5013(4.2064) | Total Time 1.00(1.00)
Iter 3950 | Time 0.2584(0.2703) | Bit/dim 6.4675(6.4187) | Steps 83(85.91) | Grad Norm 3.7302(4.5196) | Total Time 1.00(1.00)
Iter 3960 | Time 0.2661(0.2703) | Bit/dim 6.4680(6.4232) | Steps 89(86.25) | Grad Norm 10.5641(5.5172) | Total Time 1.00(1.00)
Iter 3970 | Time 0.3090(0.2723) | Bit/dim 6.3909(6.4245) | Steps 83(85.54) | Grad Norm 5.0338(5.9539) | Total Time 1.00(1.00)
Iter 3980 | Time 0.2699(0.2717) | Bit/dim 6.4567(6.4220) | Steps 83(84.88) | Grad Norm 1.9931(5.8535) | Total Time 1.00(1.00)
Iter 3990 | Time 0.3075(0.2727) | Bit/dim 6.4254(6.4132) | Steps 95(85.07) | Grad Norm 2.2964(5.1407) | Total Time 1.00(1.00)
validating...
Epoch 0008 | Time 5.1793, Bit/dim 6.4082
Iter 4000 | Time 0.2800(0.2782) | Bit/dim 6.3514(6.4055) | Steps 89(86.65) | Grad Norm 2.1180(4.4992) | Total Time 1.00(1.00)
Iter 4010 | Time 0.3090(0.2849) | Bit/dim 6.3446(6.4051) | Steps 95(88.67) | Grad Norm 0.9247(3.9476) | Total Time 1.00(1.00)
Iter 4020 | Time 0.2993(0.2857) | Bit/dim 6.4413(6.4004) | Steps 89(89.26) | Grad Norm 3.6253(3.7132) | Total Time 1.00(1.00)
Iter 4030 | Time 0.3026(0.2891) | Bit/dim 6.4069(6.4030) | Steps 95(90.20) | Grad Norm 5.5739(3.7305) | Total Time 1.00(1.00)
Iter 4040 | Time 0.3070(0.2936) | Bit/dim 6.3198(6.3996) | Steps 95(91.46) | Grad Norm 4.5036(3.9916) | Total Time 1.00(1.00)
Iter 4050 | Time 0.3021(0.2934) | Bit/dim 6.4443(6.4033) | Steps 95(91.49) | Grad Norm 5.8207(4.3566) | Total Time 1.00(1.00)
Iter 4060 | Time 0.3108(0.2915) | Bit/dim 6.4278(6.4013) | Steps 95(90.70) | Grad Norm 4.3898(4.3422) | Total Time 1.00(1.00)
Iter 4070 | Time 0.2734(0.2910) | Bit/dim 6.4732(6.4106) | Steps 83(90.50) | Grad Norm 9.1667(5.0048) | Total Time 1.00(1.00)
Iter 4080 | Time 0.2672(0.2871) | Bit/dim 6.4033(6.4208) | Steps 83(89.12) | Grad Norm 6.4902(5.7704) | Total Time 1.00(1.00)
Iter 4090 | Time 0.2761(0.2832) | Bit/dim 6.3919(6.4228) | Steps 83(87.82) | Grad Norm 3.4323(5.6968) | Total Time 1.00(1.00)
Iter 4100 | Time 0.2631(0.2814) | Bit/dim 6.3940(6.4202) | Steps 83(87.20) | Grad Norm 3.1926(5.1142) | Total Time 1.00(1.00)
Iter 4110 | Time 0.3032(0.2859) | Bit/dim 6.4454(6.4110) | Steps 95(88.93) | Grad Norm 4.1922(4.7252) | Total Time 1.00(1.00)
Iter 4120 | Time 0.3035(0.2902) | Bit/dim 6.4245(6.4105) | Steps 95(90.23) | Grad Norm 4.7310(4.6704) | Total Time 1.00(1.00)
Iter 4130 | Time 0.2817(0.2931) | Bit/dim 6.4085(6.4029) | Steps 89(91.14) | Grad Norm 3.8267(4.5381) | Total Time 1.00(1.00)
Iter 4140 | Time 0.2755(0.2926) | Bit/dim 6.3610(6.4022) | Steps 89(91.23) | Grad Norm 3.7506(4.3969) | Total Time 1.00(1.00)
Iter 4150 | Time 0.3077(0.2939) | Bit/dim 6.4473(6.4026) | Steps 95(91.59) | Grad Norm 3.5017(4.1338) | Total Time 1.00(1.00)
Iter 4160 | Time 0.2751(0.2929) | Bit/dim 6.4231(6.4026) | Steps 89(91.71) | Grad Norm 4.9567(4.3566) | Total Time 1.00(1.00)
Iter 4170 | Time 0.3061(0.2930) | Bit/dim 6.4615(6.4097) | Steps 95(91.62) | Grad Norm 8.4800(4.9724) | Total Time 1.00(1.00)
Iter 4180 | Time 0.3026(0.2901) | Bit/dim 6.4307(6.4139) | Steps 95(90.50) | Grad Norm 5.1031(4.9241) | Total Time 1.00(1.00)
Iter 4190 | Time 0.3076(0.2915) | Bit/dim 6.4271(6.4100) | Steps 95(90.68) | Grad Norm 3.7271(4.7258) | Total Time 1.00(1.00)
Iter 4200 | Time 0.3067(0.2956) | Bit/dim 6.3898(6.4073) | Steps 95(91.81) | Grad Norm 4.5336(4.4877) | Total Time 1.00(1.00)
Iter 4210 | Time 0.3124(0.2997) | Bit/dim 6.4199(6.4035) | Steps 95(92.65) | Grad Norm 3.5291(4.2073) | Total Time 1.00(1.00)
Iter 4220 | Time 0.3027(0.3018) | Bit/dim 6.3894(6.4031) | Steps 89(92.94) | Grad Norm 3.0752(3.7956) | Total Time 1.00(1.00)
Iter 4230 | Time 0.3086(0.3033) | Bit/dim 6.3714(6.3991) | Steps 95(93.30) | Grad Norm 5.3169(3.6774) | Total Time 1.00(1.00)
Iter 4240 | Time 0.3118(0.3054) | Bit/dim 6.4109(6.4014) | Steps 95(93.75) | Grad Norm 2.5205(3.6510) | Total Time 1.00(1.00)
Iter 4250 | Time 0.3105(0.3070) | Bit/dim 6.4689(6.4036) | Steps 95(94.08) | Grad Norm 3.3066(3.4969) | Total Time 1.00(1.00)
Iter 4260 | Time 0.3116(0.3082) | Bit/dim 6.4218(6.4048) | Steps 95(94.32) | Grad Norm 4.9783(3.5908) | Total Time 1.00(1.00)
Iter 4270 | Time 0.3122(0.3063) | Bit/dim 6.3855(6.4072) | Steps 95(93.82) | Grad Norm 5.1467(3.7312) | Total Time 1.00(1.00)
Iter 4280 | Time 0.3109(0.3048) | Bit/dim 6.4959(6.4127) | Steps 95(93.26) | Grad Norm 5.6671(4.0268) | Total Time 1.00(1.00)
Iter 4290 | Time 0.3047(0.3017) | Bit/dim 6.4034(6.4051) | Steps 95(92.94) | Grad Norm 4.2934(4.1251) | Total Time 1.00(1.00)
Iter 4300 | Time 0.3074(0.3027) | Bit/dim 6.3953(6.4000) | Steps 95(93.48) | Grad Norm 2.2173(4.2403) | Total Time 1.00(1.00)
Iter 4310 | Time 0.2633(0.3022) | Bit/dim 6.3857(6.3979) | Steps 83(93.52) | Grad Norm 5.5002(4.3049) | Total Time 1.00(1.00)
Iter 4320 | Time 0.3072(0.3000) | Bit/dim 6.3722(6.4000) | Steps 95(92.95) | Grad Norm 3.1065(4.4211) | Total Time 1.00(1.00)
Iter 4330 | Time 0.2981(0.2984) | Bit/dim 6.4567(6.4009) | Steps 89(92.66) | Grad Norm 3.9286(4.5650) | Total Time 1.00(1.00)
Iter 4340 | Time 0.3091(0.2979) | Bit/dim 6.4478(6.3982) | Steps 95(92.66) | Grad Norm 5.2263(5.0291) | Total Time 1.00(1.00)
Iter 4350 | Time 0.2687(0.2927) | Bit/dim 6.3697(6.3958) | Steps 83(91.22) | Grad Norm 7.8324(5.3424) | Total Time 1.00(1.00)
Iter 4360 | Time 0.2758(0.2910) | Bit/dim 6.4562(6.4013) | Steps 89(90.91) | Grad Norm 7.2239(6.3200) | Total Time 1.00(1.00)
Iter 4370 | Time 0.2673(0.2872) | Bit/dim 6.3834(6.3960) | Steps 83(89.73) | Grad Norm 5.5141(6.2663) | Total Time 1.00(1.00)
Iter 4380 | Time 0.3121(0.2904) | Bit/dim 6.4659(6.4003) | Steps 95(90.25) | Grad Norm 4.3932(5.7018) | Total Time 1.00(1.00)
Iter 4390 | Time 0.3118(0.2960) | Bit/dim 6.4092(6.4008) | Steps 95(91.50) | Grad Norm 2.7476(5.0489) | Total Time 1.00(1.00)
Iter 4400 | Time 0.3118(0.3001) | Bit/dim 6.3536(6.3933) | Steps 95(92.42) | Grad Norm 5.6584(4.6950) | Total Time 1.00(1.00)
Iter 4410 | Time 0.3121(0.3031) | Bit/dim 6.4205(6.3943) | Steps 95(93.10) | Grad Norm 5.5380(4.6166) | Total Time 1.00(1.00)
Iter 4420 | Time 0.2971(0.3042) | Bit/dim 6.3806(6.3897) | Steps 95(93.60) | Grad Norm 4.0260(4.3755) | Total Time 1.00(1.00)
Iter 4430 | Time 0.2968(0.3023) | Bit/dim 6.3572(6.3870) | Steps 95(93.96) | Grad Norm 4.8329(4.1454) | Total Time 1.00(1.00)
Iter 4440 | Time 0.2970(0.3009) | Bit/dim 6.3947(6.3928) | Steps 95(94.24) | Grad Norm 2.6883(3.9733) | Total Time 1.00(1.00)
Iter 4450 | Time 0.2971(0.2999) | Bit/dim 6.4541(6.3936) | Steps 95(94.44) | Grad Norm 5.5758(3.8632) | Total Time 1.00(1.00)
Iter 4460 | Time 0.3098(0.3008) | Bit/dim 6.4176(6.3922) | Steps 95(94.43) | Grad Norm 5.7479(4.2475) | Total Time 1.00(1.00)
Iter 4470 | Time 0.3043(0.3030) | Bit/dim 6.3737(6.3900) | Steps 95(94.58) | Grad Norm 4.7216(4.4554) | Total Time 1.00(1.00)
Iter 4480 | Time 0.3098(0.3035) | Bit/dim 6.3830(6.3882) | Steps 95(94.39) | Grad Norm 4.3062(4.7000) | Total Time 1.00(1.00)
Iter 4490 | Time 0.3088(0.3050) | Bit/dim 6.4240(6.3873) | Steps 95(94.55) | Grad Norm 4.5463(4.8002) | Total Time 1.00(1.00)
validating...
Epoch 0009 | Time 5.2245, Bit/dim 6.3958
Iter 4500 | Time 0.3005(0.3031) | Bit/dim 6.3824(6.3896) | Steps 95(94.35) | Grad Norm 5.2076(5.1346) | Total Time 1.00(1.00)
Iter 4510 | Time 0.2973(0.3010) | Bit/dim 6.4003(6.3892) | Steps 95(94.37) | Grad Norm 4.7601(4.9319) | Total Time 1.00(1.00)
Iter 4520 | Time 0.2974(0.3000) | Bit/dim 6.3344(6.3855) | Steps 95(94.54) | Grad Norm 4.2369(4.5774) | Total Time 1.00(1.00)
Iter 4530 | Time 0.2976(0.2994) | Bit/dim 6.3046(6.3817) | Steps 95(94.66) | Grad Norm 4.5684(4.7787) | Total Time 1.00(1.00)
Iter 4540 | Time 0.3118(0.2995) | Bit/dim 6.3673(6.3873) | Steps 95(94.40) | Grad Norm 3.2048(4.7605) | Total Time 1.00(1.00)
Iter 4550 | Time 0.3116(0.3020) | Bit/dim 6.3926(6.3844) | Steps 95(94.42) | Grad Norm 2.8757(4.7939) | Total Time 1.00(1.00)
Iter 4560 | Time 0.3119(0.3029) | Bit/dim 6.3220(6.3809) | Steps 95(94.29) | Grad Norm 4.3077(4.7451) | Total Time 1.00(1.00)
Iter 4570 | Time 0.3120(0.3052) | Bit/dim 6.4421(6.3805) | Steps 95(94.48) | Grad Norm 6.3183(4.6076) | Total Time 1.00(1.00)
Iter 4580 | Time 0.3120(0.3057) | Bit/dim 6.3697(6.3810) | Steps 95(94.44) | Grad Norm 4.3554(4.7619) | Total Time 1.00(1.00)
Iter 4590 | Time 0.3110(0.3065) | Bit/dim 6.3348(6.3738) | Steps 95(94.44) | Grad Norm 3.0383(4.9245) | Total Time 1.00(1.00)
Iter 4600 | Time 0.3018(0.3072) | Bit/dim 6.3553(6.3745) | Steps 95(94.59) | Grad Norm 8.4366(4.9563) | Total Time 1.00(1.00)
Iter 4610 | Time 0.2969(0.3029) | Bit/dim 6.4254(6.3860) | Steps 95(94.38) | Grad Norm 5.3060(5.2290) | Total Time 1.00(1.00)
Iter 4620 | Time 0.2970(0.3006) | Bit/dim 6.4266(6.3820) | Steps 95(94.41) | Grad Norm 3.4568(5.0390) | Total Time 1.00(1.00)
Iter 4630 | Time 0.2970(0.2997) | Bit/dim 6.3825(6.3848) | Steps 95(94.56) | Grad Norm 7.3783(5.0646) | Total Time 1.00(1.00)
Iter 4640 | Time 0.2971(0.2989) | Bit/dim 6.2826(6.3808) | Steps 95(94.68) | Grad Norm 4.1860(5.1582) | Total Time 1.00(1.00)
Iter 4650 | Time 0.2967(0.2984) | Bit/dim 6.3847(6.3783) | Steps 95(94.76) | Grad Norm 6.6919(5.0098) | Total Time 1.00(1.00)
Iter 4660 | Time 0.2968(0.2965) | Bit/dim 6.2742(6.3776) | Steps 95(94.53) | Grad Norm 5.2047(5.3398) | Total Time 1.00(1.00)
Iter 4670 | Time 0.2660(0.2957) | Bit/dim 6.4363(6.3813) | Steps 89(94.47) | Grad Norm 3.1437(5.1091) | Total Time 1.00(1.00)
Iter 4680 | Time 0.2968(0.2960) | Bit/dim 6.3509(6.3804) | Steps 95(94.61) | Grad Norm 3.4176(4.6931) | Total Time 1.00(1.00)
Iter 4690 | Time 0.2968(0.2962) | Bit/dim 6.3743(6.3847) | Steps 95(94.71) | Grad Norm 3.1349(4.4254) | Total Time 1.00(1.00)
Iter 4700 | Time 0.2969(0.2964) | Bit/dim 6.4021(6.3794) | Steps 95(94.79) | Grad Norm 7.3966(4.5721) | Total Time 1.00(1.00)
Iter 4710 | Time 0.2971(0.2965) | Bit/dim 6.4139(6.3875) | Steps 95(94.84) | Grad Norm 4.7475(4.7223) | Total Time 1.00(1.00)
Iter 4720 | Time 0.2971(0.2967) | Bit/dim 6.3743(6.3847) | Steps 95(94.89) | Grad Norm 5.3220(4.6583) | Total Time 1.00(1.00)
Iter 4730 | Time 0.2969(0.2959) | Bit/dim 6.3175(6.3827) | Steps 95(94.75) | Grad Norm 5.7409(4.9240) | Total Time 1.00(1.00)
Iter 4740 | Time 0.2968(0.2961) | Bit/dim 6.4373(6.3840) | Steps 95(94.82) | Grad Norm 7.6066(5.0579) | Total Time 1.00(1.00)
Iter 4750 | Time 0.2967(0.2949) | Bit/dim 6.3346(6.3814) | Steps 95(94.58) | Grad Norm 6.4005(5.3907) | Total Time 1.00(1.00)
Iter 4760 | Time 0.2970(0.2942) | Bit/dim 6.3488(6.3824) | Steps 95(94.40) | Grad Norm 5.1957(5.7193) | Total Time 1.00(1.00)
Iter 4770 | Time 0.2968(0.2942) | Bit/dim 6.3488(6.3850) | Steps 95(94.42) | Grad Norm 3.7426(5.2642) | Total Time 1.00(1.00)
Iter 4780 | Time 0.2968(0.2948) | Bit/dim 6.3949(6.3806) | Steps 95(94.57) | Grad Norm 5.7994(5.0240) | Total Time 1.00(1.00)
Iter 4790 | Time 0.2970(0.2954) | Bit/dim 6.3686(6.3831) | Steps 95(94.69) | Grad Norm 3.8873(4.8973) | Total Time 1.00(1.00)
Iter 4800 | Time 0.2968(0.2958) | Bit/dim 6.3279(6.3797) | Steps 95(94.77) | Grad Norm 2.3420(4.4591) | Total Time 1.00(1.00)
Iter 4810 | Time 0.2968(0.2960) | Bit/dim 6.3629(6.3762) | Steps 95(94.83) | Grad Norm 2.9000(3.9927) | Total Time 1.00(1.00)
Iter 4820 | Time 0.2967(0.2963) | Bit/dim 6.3795(6.3814) | Steps 95(94.87) | Grad Norm 6.7902(4.3790) | Total Time 1.00(1.00)
Iter 4830 | Time 0.2969(0.2965) | Bit/dim 6.4074(6.3829) | Steps 95(94.91) | Grad Norm 4.5663(4.4637) | Total Time 1.00(1.00)
Iter 4840 | Time 0.2970(0.2958) | Bit/dim 6.3832(6.3819) | Steps 95(94.78) | Grad Norm 6.4988(4.6431) | Total Time 1.00(1.00)
Iter 4850 | Time 0.2969(0.2961) | Bit/dim 6.3444(6.3820) | Steps 95(94.84) | Grad Norm 4.6194(4.6520) | Total Time 1.00(1.00)
Iter 4860 | Time 0.2967(0.2963) | Bit/dim 6.3098(6.3731) | Steps 95(94.88) | Grad Norm 2.7004(4.2862) | Total Time 1.00(1.00)
Iter 4870 | Time 0.2970(0.2964) | Bit/dim 6.3925(6.3749) | Steps 95(94.91) | Grad Norm 3.9800(4.5980) | Total Time 1.00(1.00)
Iter 4880 | Time 0.2966(0.2966) | Bit/dim 6.3382(6.3749) | Steps 95(94.93) | Grad Norm 5.9127(4.7443) | Total Time 1.00(1.00)
Iter 4890 | Time 0.2967(0.2967) | Bit/dim 6.3603(6.3707) | Steps 95(94.95) | Grad Norm 5.1195(4.6603) | Total Time 1.00(1.00)
Iter 4900 | Time 0.2967(0.2967) | Bit/dim 6.4201(6.3719) | Steps 95(94.96) | Grad Norm 4.5512(4.5306) | Total Time 1.00(1.00)
Iter 4910 | Time 0.3240(0.2989) | Bit/dim 6.3909(6.3696) | Steps 95(94.97) | Grad Norm 4.8221(4.7853) | Total Time 1.00(1.00)
Iter 4920 | Time 0.3234(0.3038) | Bit/dim 6.3333(6.3691) | Steps 95(94.68) | Grad Norm 4.6059(4.9751) | Total Time 1.00(1.00)
Iter 4930 | Time 0.3232(0.3091) | Bit/dim 6.3785(6.3714) | Steps 95(94.76) | Grad Norm 4.2333(5.4056) | Total Time 1.00(1.00)
Iter 4940 | Time 0.3081(0.3097) | Bit/dim 6.4124(6.3684) | Steps 95(94.82) | Grad Norm 8.3762(5.3912) | Total Time 1.00(1.00)
Iter 4950 | Time 0.3070(0.3086) | Bit/dim 6.4183(6.3734) | Steps 95(94.87) | Grad Norm 6.0342(5.6144) | Total Time 1.00(1.00)
Iter 4960 | Time 0.3054(0.3076) | Bit/dim 6.3994(6.3737) | Steps 95(94.90) | Grad Norm 3.4813(5.8053) | Total Time 1.00(1.00)
Iter 4970 | Time 0.3026(0.3073) | Bit/dim 6.3723(6.3713) | Steps 95(94.93) | Grad Norm 3.1390(5.4513) | Total Time 1.00(1.00)
Iter 4980 | Time 0.3029(0.3070) | Bit/dim 6.3429(6.3673) | Steps 95(94.95) | Grad Norm 4.3617(5.0709) | Total Time 1.00(1.00)
Iter 4990 | Time 0.3100(0.3067) | Bit/dim 6.3477(6.3654) | Steps 95(94.96) | Grad Norm 9.7802(5.2001) | Total Time 1.00(1.00)
validating...
Epoch 0010 | Time 5.5314, Bit/dim 6.3879
Iter 5000 | Time 0.2714(0.3030) | Bit/dim 6.4185(6.3703) | Steps 89(94.17) | Grad Norm 8.4144(6.1878) | Total Time 1.00(1.00)
Iter 5010 | Time 0.2782(0.2993) | Bit/dim 6.4115(6.3762) | Steps 89(93.76) | Grad Norm 6.6618(6.2853) | Total Time 1.00(1.00)
Iter 5020 | Time 0.3131(0.3003) | Bit/dim 6.2501(6.3733) | Steps 95(94.08) | Grad Norm 3.1886(6.1750) | Total Time 1.00(1.00)
Iter 5030 | Time 0.3121(0.3031) | Bit/dim 6.3798(6.3790) | Steps 95(94.32) | Grad Norm 4.3366(5.9623) | Total Time 1.00(1.00)
Iter 5040 | Time 0.3116(0.3054) | Bit/dim 6.3685(6.3793) | Steps 95(94.50) | Grad Norm 4.1983(5.5751) | Total Time 1.00(1.00)
Iter 5050 | Time 0.3023(0.3061) | Bit/dim 6.3004(6.3774) | Steps 95(94.63) | Grad Norm 4.9256(5.3429) | Total Time 1.00(1.00)
Iter 5060 | Time 0.2989(0.3047) | Bit/dim 6.3267(6.3770) | Steps 95(94.73) | Grad Norm 5.2601(5.4309) | Total Time 1.00(1.00)
Iter 5070 | Time 0.2985(0.3032) | Bit/dim 6.3948(6.3752) | Steps 95(94.80) | Grad Norm 5.3583(5.2322) | Total Time 1.00(1.00)
Iter 5080 | Time 0.2987(0.3019) | Bit/dim 6.3422(6.3688) | Steps 95(94.85) | Grad Norm 2.6293(4.9268) | Total Time 1.00(1.00)
Iter 5090 | Time 0.2986(0.3011) | Bit/dim 6.3820(6.3639) | Steps 95(94.89) | Grad Norm 3.0230(4.5376) | Total Time 1.00(1.00)
Iter 5100 | Time 0.2985(0.3004) | Bit/dim 6.4124(6.3668) | Steps 95(94.92) | Grad Norm 4.1964(4.2306) | Total Time 1.00(1.00)
Iter 5110 | Time 0.2984(0.2999) | Bit/dim 6.3056(6.3561) | Steps 95(94.94) | Grad Norm 3.4938(4.2052) | Total Time 1.00(1.00)
Iter 5120 | Time 0.2985(0.2995) | Bit/dim 6.2894(6.3527) | Steps 95(94.96) | Grad Norm 5.7370(4.5193) | Total Time 1.00(1.00)
Iter 5130 | Time 0.2985(0.2993) | Bit/dim 6.3835(6.3522) | Steps 95(94.97) | Grad Norm 5.8875(4.5928) | Total Time 1.00(1.00)
Iter 5140 | Time 0.2988(0.2991) | Bit/dim 6.4198(6.3541) | Steps 95(94.98) | Grad Norm 4.5526(4.7281) | Total Time 1.00(1.00)
Iter 5150 | Time 0.2985(0.2989) | Bit/dim 6.4225(6.3544) | Steps 95(94.98) | Grad Norm 5.1432(4.5863) | Total Time 1.00(1.00)
Iter 5160 | Time 0.2982(0.2988) | Bit/dim 6.4079(6.3600) | Steps 95(94.99) | Grad Norm 5.1352(4.9648) | Total Time 1.00(1.00)
Iter 5170 | Time 0.2985(0.2987) | Bit/dim 6.2765(6.3633) | Steps 95(94.99) | Grad Norm 4.1478(5.2024) | Total Time 1.00(1.00)
Iter 5180 | Time 0.2991(0.2987) | Bit/dim 6.3778(6.3581) | Steps 95(94.99) | Grad Norm 6.0855(5.5149) | Total Time 1.00(1.00)
Iter 5190 | Time 0.2984(0.2978) | Bit/dim 6.3515(6.3562) | Steps 95(94.83) | Grad Norm 13.5482(5.9275) | Total Time 1.00(1.00)
Iter 5200 | Time 0.2984(0.2958) | Bit/dim 6.4112(6.3629) | Steps 95(94.44) | Grad Norm 6.8982(6.4669) | Total Time 1.00(1.00)
Iter 5210 | Time 0.2984(0.2965) | Bit/dim 6.3537(6.3563) | Steps 95(94.59) | Grad Norm 4.1260(6.0420) | Total Time 1.00(1.00)
Iter 5220 | Time 0.2985(0.2971) | Bit/dim 6.3430(6.3570) | Steps 95(94.69) | Grad Norm 2.9203(5.4720) | Total Time 1.00(1.00)
Iter 5230 | Time 0.2985(0.2974) | Bit/dim 6.2733(6.3561) | Steps 95(94.77) | Grad Norm 3.4666(4.9295) | Total Time 1.00(1.00)
Iter 5240 | Time 0.2986(0.2977) | Bit/dim 6.3383(6.3536) | Steps 95(94.83) | Grad Norm 1.9374(4.4865) | Total Time 1.00(1.00)
Iter 5250 | Time 0.2985(0.2980) | Bit/dim 6.3238(6.3546) | Steps 95(94.88) | Grad Norm 2.4932(4.2180) | Total Time 1.00(1.00)
Iter 5260 | Time 0.2987(0.2982) | Bit/dim 6.3428(6.3557) | Steps 95(94.91) | Grad Norm 5.0153(4.0764) | Total Time 1.00(1.00)
Iter 5270 | Time 0.2987(0.2983) | Bit/dim 6.3260(6.3566) | Steps 95(94.93) | Grad Norm 4.6673(4.2256) | Total Time 1.00(1.00)
Iter 5280 | Time 0.3026(0.2985) | Bit/dim 6.3679(6.3622) | Steps 95(94.95) | Grad Norm 10.0381(4.7331) | Total Time 1.00(1.00)
Iter 5290 | Time 0.3030(0.2971) | Bit/dim 6.3655(6.3573) | Steps 95(94.66) | Grad Norm 5.7763(4.9496) | Total Time 1.00(1.00)
Iter 5300 | Time 0.2986(0.2977) | Bit/dim 6.3413(6.3538) | Steps 95(94.75) | Grad Norm 2.5734(4.8512) | Total Time 1.00(1.00)
Iter 5310 | Time 0.2987(0.2981) | Bit/dim 6.3446(6.3541) | Steps 95(94.81) | Grad Norm 5.0020(4.8027) | Total Time 1.00(1.00)
Iter 5320 | Time 0.3053(0.2986) | Bit/dim 6.3026(6.3505) | Steps 95(94.86) | Grad Norm 5.3733(4.7357) | Total Time 1.00(1.00)
Iter 5330 | Time 0.2998(0.3004) | Bit/dim 6.3279(6.3529) | Steps 95(94.90) | Grad Norm 2.8042(4.6772) | Total Time 1.00(1.00)
Iter 5340 | Time 0.3102(0.3024) | Bit/dim 6.3275(6.3508) | Steps 95(94.93) | Grad Norm 4.4448(4.8457) | Total Time 1.00(1.00)
Iter 5350 | Time 0.2999(0.3029) | Bit/dim 6.3119(6.3475) | Steps 95(94.95) | Grad Norm 4.2446(5.0439) | Total Time 1.00(1.00)
Iter 5360 | Time 0.3093(0.3044) | Bit/dim 6.2907(6.3417) | Steps 95(94.96) | Grad Norm 4.3806(4.7395) | Total Time 1.00(1.00)
Iter 5370 | Time 0.3083(0.3070) | Bit/dim 6.3767(6.3455) | Steps 95(94.97) | Grad Norm 3.8663(5.0527) | Total Time 1.00(1.00)
Iter 5380 | Time 0.3113(0.3084) | Bit/dim 6.3854(6.3472) | Steps 95(94.98) | Grad Norm 2.3593(4.9683) | Total Time 1.00(1.00)
Iter 5390 | Time 0.3063(0.3107) | Bit/dim 6.3526(6.3488) | Steps 95(94.98) | Grad Norm 5.0596(5.0877) | Total Time 1.00(1.00)
Iter 5400 | Time 0.3175(0.3108) | Bit/dim 6.4050(6.3452) | Steps 95(94.99) | Grad Norm 5.4565(5.1656) | Total Time 1.00(1.00)
Iter 5410 | Time 0.3477(0.3151) | Bit/dim 6.3407(6.3427) | Steps 95(94.99) | Grad Norm 5.6562(5.1747) | Total Time 1.00(1.00)
Iter 5420 | Time 0.3143(0.3145) | Bit/dim 6.2436(6.3352) | Steps 95(94.99) | Grad Norm 5.6799(4.9987) | Total Time 1.00(1.00)
Iter 5430 | Time 0.3116(0.3136) | Bit/dim 6.3327(6.3402) | Steps 95(95.00) | Grad Norm 8.8071(5.2158) | Total Time 1.00(1.00)
Iter 5440 | Time 0.3069(0.3111) | Bit/dim 6.3162(6.3466) | Steps 95(94.85) | Grad Norm 4.1777(5.7924) | Total Time 1.00(1.00)
Iter 5450 | Time 0.3021(0.3091) | Bit/dim 6.3888(6.3465) | Steps 95(94.72) | Grad Norm 11.7249(6.5420) | Total Time 1.00(1.00)
Iter 5460 | Time 0.2762(0.3026) | Bit/dim 6.3511(6.3561) | Steps 89(93.68) | Grad Norm 6.1124(6.9813) | Total Time 1.00(1.00)
Iter 5470 | Time 0.3113(0.3040) | Bit/dim 6.3362(6.3569) | Steps 95(94.02) | Grad Norm 4.1868(6.5655) | Total Time 1.00(1.00)
Iter 5480 | Time 0.3027(0.3057) | Bit/dim 6.3720(6.3567) | Steps 95(94.28) | Grad Norm 3.4112(5.8377) | Total Time 1.00(1.00)
Iter 5490 | Time 0.3025(0.3066) | Bit/dim 6.3584(6.3563) | Steps 95(94.47) | Grad Norm 3.8354(5.1546) | Total Time 1.00(1.00)
validating...
Epoch 0011 | Time 5.2322, Bit/dim 6.3421
Iter 5500 | Time 0.3031(0.3056) | Bit/dim 6.3481(6.3507) | Steps 95(94.61) | Grad Norm 3.7798(4.7786) | Total Time 1.00(1.00)
Iter 5510 | Time 0.2970(0.3035) | Bit/dim 6.3988(6.3523) | Steps 95(94.71) | Grad Norm 4.2707(4.8787) | Total Time 1.00(1.00)
Iter 5520 | Time 0.2967(0.3020) | Bit/dim 6.3356(6.3482) | Steps 95(94.79) | Grad Norm 3.0888(4.5130) | Total Time 1.00(1.00)
Iter 5530 | Time 0.2973(0.3007) | Bit/dim 6.3300(6.3413) | Steps 95(94.84) | Grad Norm 3.1930(4.2115) | Total Time 1.00(1.00)
Iter 5540 | Time 0.2969(0.2999) | Bit/dim 6.3044(6.3381) | Steps 95(94.88) | Grad Norm 3.8515(4.4158) | Total Time 1.00(1.00)
Iter 5550 | Time 0.2972(0.2991) | Bit/dim 6.3633(6.3348) | Steps 95(94.91) | Grad Norm 5.0871(4.5558) | Total Time 1.00(1.00)
Iter 5560 | Time 0.2969(0.2987) | Bit/dim 6.2906(6.3361) | Steps 95(94.94) | Grad Norm 7.3371(4.8566) | Total Time 1.00(1.00)
Iter 5570 | Time 0.2970(0.2983) | Bit/dim 6.2905(6.3356) | Steps 95(94.95) | Grad Norm 5.7142(5.1865) | Total Time 1.00(1.00)
Iter 5580 | Time 0.2970(0.2982) | Bit/dim 6.3203(6.3317) | Steps 95(94.97) | Grad Norm 2.9586(4.9696) | Total Time 1.00(1.00)
Iter 5590 | Time 0.2975(0.2980) | Bit/dim 6.3090(6.3267) | Steps 95(94.97) | Grad Norm 5.0295(4.8823) | Total Time 1.00(1.00)
Iter 5600 | Time 0.2974(0.2979) | Bit/dim 6.3239(6.3305) | Steps 95(94.98) | Grad Norm 7.1156(5.0758) | Total Time 1.00(1.00)
Iter 5610 | Time 0.2971(0.2978) | Bit/dim 6.3342(6.3343) | Steps 95(94.99) | Grad Norm 3.9695(5.1569) | Total Time 1.00(1.00)
Iter 5620 | Time 0.2967(0.2976) | Bit/dim 6.3475(6.3352) | Steps 95(94.99) | Grad Norm 5.4066(5.1078) | Total Time 1.00(1.00)
Iter 5630 | Time 0.3031(0.2978) | Bit/dim 6.3889(6.3359) | Steps 95(94.99) | Grad Norm 4.2282(4.9772) | Total Time 1.00(1.00)
Iter 5640 | Time 0.2970(0.2978) | Bit/dim 6.3615(6.3376) | Steps 95(94.99) | Grad Norm 5.7925(5.1630) | Total Time 1.00(1.00)
Iter 5650 | Time 0.2971(0.2989) | Bit/dim 6.3971(6.3342) | Steps 95(95.00) | Grad Norm 7.3455(5.3166) | Total Time 1.00(1.00)
Iter 5660 | Time 0.2967(0.2984) | Bit/dim 6.3469(6.3350) | Steps 95(95.00) | Grad Norm 5.8261(5.3434) | Total Time 1.00(1.00)
Iter 5670 | Time 0.2971(0.2981) | Bit/dim 6.3522(6.3390) | Steps 95(95.00) | Grad Norm 6.1460(5.5372) | Total Time 1.00(1.00)
Iter 5680 | Time 0.2971(0.2978) | Bit/dim 6.3455(6.3394) | Steps 95(95.00) | Grad Norm 2.8498(5.4373) | Total Time 1.00(1.00)
Iter 5690 | Time 0.2968(0.2976) | Bit/dim 6.3157(6.3394) | Steps 95(95.00) | Grad Norm 3.5793(5.3641) | Total Time 1.00(1.00)
Iter 5700 | Time 0.2969(0.2975) | Bit/dim 6.3537(6.3380) | Steps 95(95.00) | Grad Norm 5.5079(5.4576) | Total Time 1.00(1.00)
Iter 5710 | Time 0.2975(0.2974) | Bit/dim 6.3651(6.3360) | Steps 95(95.00) | Grad Norm 5.4321(5.2575) | Total Time 1.00(1.00)
Iter 5720 | Time 0.2970(0.2973) | Bit/dim 6.3532(6.3366) | Steps 95(95.00) | Grad Norm 3.9115(4.7970) | Total Time 1.00(1.00)
Iter 5730 | Time 0.2971(0.2973) | Bit/dim 6.3963(6.3386) | Steps 95(95.00) | Grad Norm 6.3107(4.6210) | Total Time 1.00(1.00)
Iter 5740 | Time 0.2969(0.2973) | Bit/dim 6.2870(6.3343) | Steps 95(95.00) | Grad Norm 5.3209(5.0379) | Total Time 1.00(1.00)
Iter 5750 | Time 0.2971(0.2974) | Bit/dim 6.3186(6.3299) | Steps 95(95.00) | Grad Norm 5.0298(5.2120) | Total Time 1.00(1.00)
Iter 5760 | Time 0.2972(0.2974) | Bit/dim 6.3277(6.3315) | Steps 95(95.00) | Grad Norm 5.4434(5.5237) | Total Time 1.00(1.00)
Iter 5770 | Time 0.2973(0.2974) | Bit/dim 6.3851(6.3290) | Steps 95(95.00) | Grad Norm 4.8970(5.4438) | Total Time 1.00(1.00)
Iter 5780 | Time 0.3076(0.3000) | Bit/dim 6.3408(6.3250) | Steps 95(95.00) | Grad Norm 6.6363(5.1460) | Total Time 1.00(1.00)
Iter 5790 | Time 0.3084(0.3020) | Bit/dim 6.3435(6.3305) | Steps 95(95.00) | Grad Norm 4.1115(5.3885) | Total Time 1.00(1.00)
Iter 5800 | Time 0.3097(0.3037) | Bit/dim 6.2450(6.3281) | Steps 95(95.00) | Grad Norm 6.8719(5.9025) | Total Time 1.00(1.00)
Iter 5810 | Time 0.3066(0.3046) | Bit/dim 6.3855(6.3306) | Steps 95(95.00) | Grad Norm 7.3535(6.1264) | Total Time 1.00(1.00)
Iter 5820 | Time 0.3083(0.3055) | Bit/dim 6.3985(6.3281) | Steps 95(95.00) | Grad Norm 5.4390(6.1679) | Total Time 1.00(1.00)
Iter 5830 | Time 0.2969(0.3036) | Bit/dim 6.3116(6.3283) | Steps 95(95.00) | Grad Norm 7.4433(6.3564) | Total Time 1.00(1.00)
Iter 5840 | Time 0.2967(0.3019) | Bit/dim 6.3196(6.3308) | Steps 95(95.00) | Grad Norm 7.1499(6.5504) | Total Time 1.00(1.00)
Iter 5850 | Time 0.2969(0.3005) | Bit/dim 6.3302(6.3292) | Steps 95(95.00) | Grad Norm 4.6003(6.0933) | Total Time 1.00(1.00)
Iter 5860 | Time 0.2969(0.2996) | Bit/dim 6.3912(6.3327) | Steps 95(95.00) | Grad Norm 4.4725(5.5563) | Total Time 1.00(1.00)
Iter 5870 | Time 0.2965(0.2989) | Bit/dim 6.2493(6.3296) | Steps 95(95.00) | Grad Norm 3.5538(5.0360) | Total Time 1.00(1.00)
Iter 5880 | Time 0.2970(0.2984) | Bit/dim 6.2652(6.3198) | Steps 95(95.00) | Grad Norm 2.4446(4.5727) | Total Time 1.00(1.00)
Iter 5890 | Time 0.2965(0.2980) | Bit/dim 6.3379(6.3266) | Steps 95(95.00) | Grad Norm 6.2712(4.5777) | Total Time 1.00(1.00)
Iter 5900 | Time 0.2969(0.2977) | Bit/dim 6.2710(6.3240) | Steps 95(95.00) | Grad Norm 5.9647(4.8775) | Total Time 1.00(1.00)
Iter 5910 | Time 0.2966(0.2977) | Bit/dim 6.3032(6.3289) | Steps 95(95.00) | Grad Norm 7.4247(4.8062) | Total Time 1.00(1.00)
Iter 5920 | Time 0.3086(0.3004) | Bit/dim 6.3563(6.3332) | Steps 95(95.00) | Grad Norm 8.3526(5.2292) | Total Time 1.00(1.00)
Iter 5930 | Time 0.2658(0.2993) | Bit/dim 6.3628(6.3349) | Steps 89(94.82) | Grad Norm 8.2394(6.2176) | Total Time 1.00(1.00)
Iter 5940 | Time 0.2984(0.2973) | Bit/dim 6.2746(6.3324) | Steps 95(94.58) | Grad Norm 4.4696(5.9518) | Total Time 1.00(1.00)
Iter 5950 | Time 0.2967(0.2972) | Bit/dim 6.3576(6.3332) | Steps 95(94.69) | Grad Norm 3.4318(5.3789) | Total Time 1.00(1.00)
Iter 5960 | Time 0.2969(0.2971) | Bit/dim 6.2597(6.3272) | Steps 95(94.77) | Grad Norm 2.6930(4.8241) | Total Time 1.00(1.00)
Iter 5970 | Time 0.2966(0.2970) | Bit/dim 6.3072(6.3223) | Steps 95(94.83) | Grad Norm 4.1816(4.7784) | Total Time 1.00(1.00)
Iter 5980 | Time 0.2967(0.2970) | Bit/dim 6.2794(6.3202) | Steps 95(94.88) | Grad Norm 6.7547(4.9036) | Total Time 1.00(1.00)
Iter 5990 | Time 0.2965(0.2985) | Bit/dim 6.3776(6.3186) | Steps 95(94.91) | Grad Norm 3.7160(4.9630) | Total Time 1.00(1.00)
validating...
Epoch 0012 | Time 5.5720, Bit/dim 6.3198
Iter 6000 | Time 0.3062(0.2990) | Bit/dim 6.3368(6.3192) | Steps 95(94.93) | Grad Norm 4.6985(5.0452) | Total Time 1.00(1.00)
Iter 6010 | Time 0.2973(0.2985) | Bit/dim 6.2518(6.3139) | Steps 95(94.95) | Grad Norm 2.2118(4.6923) | Total Time 1.00(1.00)
Iter 6020 | Time 0.2971(0.2982) | Bit/dim 6.2858(6.3128) | Steps 95(94.96) | Grad Norm 3.2980(4.1899) | Total Time 1.00(1.00)
Iter 6030 | Time 0.2971(0.2979) | Bit/dim 6.3394(6.3118) | Steps 95(94.97) | Grad Norm 7.1204(4.3869) | Total Time 1.00(1.00)
Iter 6040 | Time 0.2973(0.2979) | Bit/dim 6.3130(6.3115) | Steps 95(94.98) | Grad Norm 7.2566(4.8356) | Total Time 1.00(1.00)
Iter 6050 | Time 0.3023(0.2994) | Bit/dim 6.3831(6.3153) | Steps 95(94.99) | Grad Norm 7.2487(5.9715) | Total Time 1.00(1.00)
Iter 6060 | Time 0.3125(0.3014) | Bit/dim 6.3679(6.3194) | Steps 95(94.99) | Grad Norm 6.9268(6.1329) | Total Time 1.00(1.00)
Iter 6070 | Time 0.3071(0.3024) | Bit/dim 6.3651(6.3192) | Steps 95(94.99) | Grad Norm 5.4964(6.1094) | Total Time 1.00(1.00)
Iter 6080 | Time 0.3124(0.3046) | Bit/dim 6.3592(6.3203) | Steps 95(94.99) | Grad Norm 4.2989(5.7271) | Total Time 1.00(1.00)
Iter 6090 | Time 0.3109(0.3059) | Bit/dim 6.3132(6.3114) | Steps 95(95.00) | Grad Norm 6.0002(5.8180) | Total Time 1.00(1.00)
Iter 6100 | Time 0.2975(0.3039) | Bit/dim 6.3273(6.3117) | Steps 95(95.00) | Grad Norm 7.9358(5.8764) | Total Time 1.00(1.00)
Iter 6110 | Time 0.3111(0.3046) | Bit/dim 6.2628(6.3076) | Steps 95(95.00) | Grad Norm 5.2788(5.7282) | Total Time 1.00(1.00)
Iter 6120 | Time 0.3113(0.3063) | Bit/dim 6.2775(6.3120) | Steps 95(95.00) | Grad Norm 4.8256(5.4285) | Total Time 1.00(1.00)
Iter 6130 | Time 0.3110(0.3071) | Bit/dim 6.3090(6.3143) | Steps 95(95.00) | Grad Norm 5.7296(5.3071) | Total Time 1.00(1.00)
Iter 6140 | Time 0.3112(0.3072) | Bit/dim 6.1828(6.3121) | Steps 95(95.00) | Grad Norm 3.3969(5.1635) | Total Time 1.00(1.00)
Iter 6150 | Time 0.3079(0.3076) | Bit/dim 6.2823(6.3080) | Steps 95(95.00) | Grad Norm 4.0856(4.8595) | Total Time 1.00(1.00)
Iter 6160 | Time 0.3198(0.3086) | Bit/dim 6.3706(6.3075) | Steps 95(95.00) | Grad Norm 5.0007(4.8644) | Total Time 1.00(1.00)
Iter 6170 | Time 0.3166(0.3111) | Bit/dim 6.2368(6.3010) | Steps 95(95.00) | Grad Norm 4.8905(4.8211) | Total Time 1.00(1.00)
Iter 6180 | Time 0.3174(0.3117) | Bit/dim 6.3124(6.3069) | Steps 95(95.00) | Grad Norm 7.3241(5.4247) | Total Time 1.00(1.00)
Iter 6190 | Time 0.3163(0.3123) | Bit/dim 6.3068(6.3063) | Steps 95(95.00) | Grad Norm 6.2400(5.5916) | Total Time 1.00(1.00)
Iter 6200 | Time 0.3489(0.3121) | Bit/dim 6.2410(6.3102) | Steps 95(95.00) | Grad Norm 4.3248(5.2495) | Total Time 1.00(1.00)
Iter 6210 | Time 0.2966(0.3091) | Bit/dim 6.3573(6.3122) | Steps 95(95.00) | Grad Norm 7.1593(5.0391) | Total Time 1.00(1.00)
Iter 6220 | Time 0.3062(0.3108) | Bit/dim 6.2965(6.3110) | Steps 95(95.00) | Grad Norm 4.6421(5.5038) | Total Time 1.00(1.00)
Iter 6230 | Time 0.3027(0.3109) | Bit/dim 6.3542(6.3179) | Steps 95(95.00) | Grad Norm 7.0210(5.8168) | Total Time 1.00(1.00)
Iter 6240 | Time 0.3016(0.3083) | Bit/dim 6.3347(6.3138) | Steps 95(95.00) | Grad Norm 8.6533(6.1639) | Total Time 1.00(1.00)
Iter 6250 | Time 0.3042(0.3118) | Bit/dim 6.3380(6.3100) | Steps 95(95.00) | Grad Norm 6.7046(6.3963) | Total Time 1.00(1.00)
Iter 6260 | Time 0.3191(0.3153) | Bit/dim 6.2214(6.3087) | Steps 95(95.00) | Grad Norm 4.1304(6.0440) | Total Time 1.00(1.00)
Iter 6270 | Time 0.3297(0.3209) | Bit/dim 6.3641(6.3063) | Steps 95(95.00) | Grad Norm 4.7023(5.9710) | Total Time 1.00(1.00)
Iter 6280 | Time 0.3056(0.3208) | Bit/dim 6.3401(6.3037) | Steps 95(95.00) | Grad Norm 7.5970(5.7554) | Total Time 1.00(1.00)
Iter 6290 | Time 0.3121(0.3185) | Bit/dim 6.3294(6.2973) | Steps 95(95.00) | Grad Norm 3.8246(5.5021) | Total Time 1.00(1.00)
Iter 6300 | Time 0.3081(0.3159) | Bit/dim 6.3270(6.2994) | Steps 95(95.00) | Grad Norm 3.7853(4.9620) | Total Time 1.00(1.00)
Iter 6310 | Time 0.3108(0.3147) | Bit/dim 6.3367(6.3056) | Steps 95(95.00) | Grad Norm 5.7328(5.2117) | Total Time 1.00(1.00)
Iter 6320 | Time 0.3061(0.3166) | Bit/dim 6.2898(6.3045) | Steps 95(95.00) | Grad Norm 6.0785(5.3296) | Total Time 1.00(1.00)
Iter 6330 | Time 0.3117(0.3169) | Bit/dim 6.3360(6.3083) | Steps 95(95.00) | Grad Norm 4.9005(5.1177) | Total Time 1.00(1.00)
Iter 6340 | Time 0.3164(0.3153) | Bit/dim 6.3095(6.3096) | Steps 95(95.00) | Grad Norm 7.1010(5.3825) | Total Time 1.00(1.00)
Iter 6350 | Time 0.3098(0.3167) | Bit/dim 6.3036(6.3102) | Steps 95(95.00) | Grad Norm 5.8645(5.6310) | Total Time 1.00(1.00)
Iter 6360 | Time 0.3568(0.3251) | Bit/dim 6.3245(6.3138) | Steps 95(95.00) | Grad Norm 8.6898(5.9954) | Total Time 1.00(1.00)
Iter 6370 | Time 0.3555(0.3256) | Bit/dim 6.3859(6.3255) | Steps 95(94.38) | Grad Norm 7.7417(6.7810) | Total Time 1.00(1.00)
Iter 6380 | Time 0.3423(0.3244) | Bit/dim 6.2620(6.3222) | Steps 95(94.54) | Grad Norm 6.3476(6.8824) | Total Time 1.00(1.00)
Iter 6390 | Time 0.3005(0.3203) | Bit/dim 6.2802(6.3188) | Steps 95(94.51) | Grad Norm 5.4394(7.0541) | Total Time 1.00(1.00)
Iter 6400 | Time 0.3123(0.3180) | Bit/dim 6.3288(6.3145) | Steps 95(94.64) | Grad Norm 4.1468(6.4116) | Total Time 1.00(1.00)
Iter 6410 | Time 0.3178(0.3184) | Bit/dim 6.3009(6.3091) | Steps 95(94.74) | Grad Norm 4.1692(5.8940) | Total Time 1.00(1.00)
Iter 6420 | Time 0.3025(0.3186) | Bit/dim 6.3162(6.3120) | Steps 95(94.80) | Grad Norm 2.9714(5.2764) | Total Time 1.00(1.00)
Iter 6430 | Time 0.3051(0.3169) | Bit/dim 6.3888(6.3108) | Steps 95(94.86) | Grad Norm 4.9524(4.9368) | Total Time 1.00(1.00)
Iter 6440 | Time 0.2998(0.3137) | Bit/dim 6.2844(6.3062) | Steps 95(94.89) | Grad Norm 3.3739(4.5960) | Total Time 1.00(1.00)
Iter 6450 | Time 0.3115(0.3134) | Bit/dim 6.3494(6.3051) | Steps 95(94.92) | Grad Norm 4.1317(4.4000) | Total Time 1.00(1.00)
Iter 6460 | Time 0.2972(0.3108) | Bit/dim 6.3222(6.3058) | Steps 95(94.77) | Grad Norm 10.0234(5.0519) | Total Time 1.00(1.00)
Iter 6470 | Time 0.2969(0.3065) | Bit/dim 6.2992(6.3082) | Steps 95(94.51) | Grad Norm 6.2934(5.7695) | Total Time 1.00(1.00)
Iter 6480 | Time 0.2970(0.3041) | Bit/dim 6.2040(6.3059) | Steps 95(94.64) | Grad Norm 3.8183(5.9778) | Total Time 1.00(1.00)
Iter 6490 | Time 0.2970(0.3025) | Bit/dim 6.3315(6.3044) | Steps 95(94.73) | Grad Norm 4.6539(5.6216) | Total Time 1.00(1.00)
validating...
Epoch 0013 | Time 5.2565, Bit/dim 6.2977
Iter 6500 | Time 0.3095(0.3016) | Bit/dim 6.3091(6.3054) | Steps 95(94.80) | Grad Norm 7.2623(5.4871) | Total Time 1.00(1.00)
Iter 6510 | Time 0.3119(0.3023) | Bit/dim 6.3246(6.3038) | Steps 95(94.85) | Grad Norm 5.2820(5.3869) | Total Time 1.00(1.00)
Iter 6520 | Time 0.3095(0.3066) | Bit/dim 6.2996(6.3010) | Steps 95(94.89) | Grad Norm 3.5298(5.3910) | Total Time 1.00(1.00)
Iter 6530 | Time 0.3504(0.3093) | Bit/dim 6.2847(6.3007) | Steps 95(94.92) | Grad Norm 5.2571(5.6039) | Total Time 1.00(1.00)
Iter 6540 | Time 0.3097(0.3126) | Bit/dim 6.2801(6.3010) | Steps 95(94.94) | Grad Norm 5.7563(5.6811) | Total Time 1.00(1.00)
Iter 6550 | Time 0.3158(0.3116) | Bit/dim 6.2862(6.2979) | Steps 95(94.96) | Grad Norm 5.5388(5.8486) | Total Time 1.00(1.00)
Iter 6560 | Time 0.3171(0.3102) | Bit/dim 6.3226(6.3016) | Steps 95(94.97) | Grad Norm 5.7954(5.8147) | Total Time 1.00(1.00)
Iter 6570 | Time 0.3014(0.3080) | Bit/dim 6.2761(6.2939) | Steps 95(94.98) | Grad Norm 3.6502(5.3749) | Total Time 1.00(1.00)
Iter 6580 | Time 0.2997(0.3059) | Bit/dim 6.2746(6.2917) | Steps 95(94.98) | Grad Norm 3.6963(4.9941) | Total Time 1.00(1.00)
Iter 6590 | Time 0.3372(0.3100) | Bit/dim 6.3389(6.2882) | Steps 95(94.99) | Grad Norm 8.0267(5.2241) | Total Time 1.00(1.00)
Iter 6600 | Time 0.3024(0.3137) | Bit/dim 6.2939(6.2920) | Steps 95(94.99) | Grad Norm 5.2321(5.7621) | Total Time 1.00(1.00)
Iter 6610 | Time 0.3131(0.3160) | Bit/dim 6.3042(6.2919) | Steps 95(94.99) | Grad Norm 7.2625(6.2421) | Total Time 1.00(1.00)
Iter 6620 | Time 0.3423(0.3149) | Bit/dim 6.2425(6.2904) | Steps 95(94.99) | Grad Norm 5.2453(6.2078) | Total Time 1.00(1.00)
Iter 6630 | Time 0.3537(0.3201) | Bit/dim 6.3106(6.2821) | Steps 95(95.00) | Grad Norm 4.2413(5.6625) | Total Time 1.00(1.00)
Iter 6640 | Time 0.3086(0.3182) | Bit/dim 6.2380(6.2850) | Steps 95(94.84) | Grad Norm 4.3684(5.9395) | Total Time 1.00(1.00)
Iter 6650 | Time 0.3473(0.3200) | Bit/dim 6.3396(6.2876) | Steps 95(94.88) | Grad Norm 4.2582(6.0048) | Total Time 1.00(1.00)
Iter 6660 | Time 0.3068(0.3170) | Bit/dim 6.2850(6.2886) | Steps 95(94.91) | Grad Norm 5.1493(5.8022) | Total Time 1.00(1.00)
Iter 6670 | Time 0.3100(0.3178) | Bit/dim 6.2968(6.2838) | Steps 95(94.93) | Grad Norm 7.3578(5.6204) | Total Time 1.00(1.00)
Iter 6680 | Time 0.3554(0.3194) | Bit/dim 6.3897(6.2922) | Steps 95(94.63) | Grad Norm 11.1860(6.7336) | Total Time 1.00(1.00)
Iter 6690 | Time 0.3053(0.3191) | Bit/dim 6.2576(6.2918) | Steps 95(94.58) | Grad Norm 7.6983(7.0924) | Total Time 1.00(1.00)
Iter 6700 | Time 0.3499(0.3190) | Bit/dim 6.2877(6.2916) | Steps 95(94.69) | Grad Norm 4.6900(6.2915) | Total Time 1.00(1.00)
Iter 6710 | Time 0.3510(0.3226) | Bit/dim 6.2741(6.2896) | Steps 95(94.77) | Grad Norm 3.1172(5.5463) | Total Time 1.00(1.00)
Iter 6720 | Time 0.2983(0.3221) | Bit/dim 6.2645(6.2853) | Steps 95(94.83) | Grad Norm 4.9584(4.9935) | Total Time 1.00(1.00)
Iter 6730 | Time 0.2989(0.3184) | Bit/dim 6.3454(6.2864) | Steps 95(94.88) | Grad Norm 4.8202(4.7744) | Total Time 1.00(1.00)
Iter 6740 | Time 0.3034(0.3136) | Bit/dim 6.2281(6.2810) | Steps 95(94.91) | Grad Norm 5.1165(4.8377) | Total Time 1.00(1.00)
Iter 6750 | Time 0.3023(0.3133) | Bit/dim 6.2728(6.2904) | Steps 95(94.93) | Grad Norm 5.8021(4.7515) | Total Time 1.00(1.00)
Iter 6760 | Time 0.3444(0.3154) | Bit/dim 6.2437(6.2876) | Steps 95(94.95) | Grad Norm 3.7510(5.1154) | Total Time 1.00(1.00)
Iter 6770 | Time 0.3110(0.3150) | Bit/dim 6.3116(6.2870) | Steps 95(94.96) | Grad Norm 5.3520(5.1748) | Total Time 1.00(1.00)
Iter 6780 | Time 0.3457(0.3186) | Bit/dim 6.2680(6.2854) | Steps 95(94.97) | Grad Norm 5.7117(5.1238) | Total Time 1.00(1.00)
Iter 6790 | Time 0.3096(0.3221) | Bit/dim 6.3184(6.2886) | Steps 95(94.98) | Grad Norm 8.5300(5.7108) | Total Time 1.00(1.00)
Iter 6800 | Time 0.3007(0.3173) | Bit/dim 6.2620(6.2902) | Steps 95(94.99) | Grad Norm 5.5336(6.2449) | Total Time 1.00(1.00)
Iter 6810 | Time 0.3072(0.3164) | Bit/dim 6.2631(6.2858) | Steps 95(94.99) | Grad Norm 3.5462(6.1694) | Total Time 1.00(1.00)
Iter 6820 | Time 0.3326(0.3180) | Bit/dim 6.2622(6.2825) | Steps 95(94.99) | Grad Norm 4.0549(5.8779) | Total Time 1.00(1.00)
Iter 6830 | Time 0.3130(0.3247) | Bit/dim 6.2780(6.2824) | Steps 95(94.99) | Grad Norm 4.2327(5.3826) | Total Time 1.00(1.00)
Iter 6840 | Time 0.3049(0.3217) | Bit/dim 6.2708(6.2879) | Steps 95(95.16) | Grad Norm 8.8967(6.2683) | Total Time 1.00(1.00)
Iter 6850 | Time 0.3163(0.3226) | Bit/dim 6.3135(6.2894) | Steps 95(95.11) | Grad Norm 6.7906(6.1868) | Total Time 1.00(1.00)
Iter 6860 | Time 0.3004(0.3171) | Bit/dim 6.2756(6.2872) | Steps 95(95.08) | Grad Norm 4.1499(5.6971) | Total Time 1.00(1.00)
Iter 6870 | Time 0.3255(0.3180) | Bit/dim 6.3444(6.2825) | Steps 95(95.06) | Grad Norm 9.1350(5.6867) | Total Time 1.00(1.00)
Iter 6880 | Time 0.3075(0.3190) | Bit/dim 6.2793(6.2786) | Steps 95(95.05) | Grad Norm 6.2238(5.9118) | Total Time 1.00(1.00)
Iter 6890 | Time 0.3112(0.3181) | Bit/dim 6.2870(6.2827) | Steps 95(95.03) | Grad Norm 6.3904(6.3435) | Total Time 1.00(1.00)
Iter 6900 | Time 0.3049(0.3166) | Bit/dim 6.2868(6.2862) | Steps 95(95.02) | Grad Norm 7.5130(6.6398) | Total Time 1.00(1.00)
Iter 6910 | Time 0.3137(0.3155) | Bit/dim 6.3070(6.2868) | Steps 95(95.02) | Grad Norm 6.8107(6.9454) | Total Time 1.00(1.00)
Iter 6920 | Time 0.3151(0.3157) | Bit/dim 6.2826(6.2898) | Steps 95(95.01) | Grad Norm 5.0396(6.5921) | Total Time 1.00(1.00)
Iter 6930 | Time 0.3051(0.3144) | Bit/dim 6.3324(6.2859) | Steps 95(95.01) | Grad Norm 4.7046(5.9082) | Total Time 1.00(1.00)
Iter 6940 | Time 0.3107(0.3124) | Bit/dim 6.3153(6.2854) | Steps 95(95.01) | Grad Norm 3.4762(5.2236) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.3280(0.3280) | Bit/dim 29.3880(29.3880) | Steps 41(41.00) | Grad Norm 16.3903(16.3903) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1419(0.2794) | Bit/dim 29.6014(29.4335) | Steps 41(41.00) | Grad Norm 16.7843(16.4576) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1418(0.2436) | Bit/dim 29.5445(29.4470) | Steps 41(41.00) | Grad Norm 16.6466(16.4897) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1425(0.2168) | Bit/dim 29.2135(29.3968) | Steps 41(41.00) | Grad Norm 16.3750(16.4349) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1441(0.1973) | Bit/dim 28.8846(29.3312) | Steps 41(41.00) | Grad Norm 15.9741(16.3960) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1472(0.1834) | Bit/dim 28.6536(29.2190) | Steps 41(41.00) | Grad Norm 16.0509(16.3375) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1378(0.1721) | Bit/dim 28.2225(29.0383) | Steps 41(41.00) | Grad Norm 15.8797(16.2657) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1390(0.1640) | Bit/dim 28.0065(28.8123) | Steps 41(41.00) | Grad Norm 16.4972(16.2632) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1425(0.1587) | Bit/dim 27.4353(28.5138) | Steps 41(41.00) | Grad Norm 16.8692(16.3615) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1378(0.1544) | Bit/dim 26.5557(28.1122) | Steps 41(41.00) | Grad Norm 18.1619(16.7111) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1437(0.1511) | Bit/dim 25.5288(27.5728) | Steps 41(41.00) | Grad Norm 20.2412(17.4280) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1365(0.1484) | Bit/dim 24.5413(26.8827) | Steps 41(41.00) | Grad Norm 24.1637(18.7558) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1532(0.1470) | Bit/dim 23.4006(26.0752) | Steps 41(41.00) | Grad Norm 28.4400(20.7778) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1431(0.1463) | Bit/dim 22.5667(25.2556) | Steps 41(41.58) | Grad Norm 30.5486(23.2340) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1474(0.1454) | Bit/dim 21.4947(24.3600) | Steps 41(41.43) | Grad Norm 30.2514(25.1592) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1205(0.1421) | Bit/dim 19.8473(23.3679) | Steps 41(41.32) | Grad Norm 33.7124(26.9846) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1256(0.1373) | Bit/dim 17.9660(22.1720) | Steps 41(41.23) | Grad Norm 38.2799(29.4409) | Total Time 1.00(1.00)
Iter 0170 | Time 0.1445(0.1374) | Bit/dim 16.2140(20.7736) | Steps 47(42.17) | Grad Norm 38.6109(31.9603) | Total Time 1.00(1.00)
Iter 0180 | Time 0.1789(0.1373) | Bit/dim 13.7811(19.1939) | Steps 59(43.58) | Grad Norm 40.3009(33.9535) | Total Time 1.00(1.00)
Iter 0190 | Time 0.2025(0.1528) | Bit/dim 12.6350(17.6255) | Steps 65(48.78) | Grad Norm 37.7201(35.1651) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2108(0.1682) | Bit/dim 11.2631(16.1070) | Steps 71(54.04) | Grad Norm 33.4137(35.1747) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2612(0.1927) | Bit/dim 10.4372(14.6775) | Steps 83(61.64) | Grad Norm 27.8331(33.9146) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2694(0.2116) | Bit/dim 9.5080(13.3803) | Steps 89(67.94) | Grad Norm 22.6864(31.6329) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2693(0.2268) | Bit/dim 8.7188(12.2523) | Steps 89(73.47) | Grad Norm 18.3033(28.6334) | Total Time 1.00(1.00)
Iter 0240 | Time 0.3291(0.2448) | Bit/dim 8.3439(11.2357) | Steps 107(79.41) | Grad Norm 14.8426(25.3877) | Total Time 1.00(1.00)
Iter 0250 | Time 0.3275(0.2665) | Bit/dim 7.7817(10.3565) | Steps 107(86.65) | Grad Norm 12.7051(22.2822) | Total Time 1.00(1.00)
Iter 0260 | Time 0.3273(0.2811) | Bit/dim 7.3752(9.6008) | Steps 107(91.66) | Grad Norm 11.3284(19.5605) | Total Time 1.00(1.00)
Iter 0270 | Time 0.3034(0.2871) | Bit/dim 7.0307(8.9608) | Steps 101(94.11) | Grad Norm 10.7400(17.3040) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3113(0.2935) | Bit/dim 6.4033(8.3634) | Steps 107(97.49) | Grad Norm 10.3144(15.5178) | Total Time 1.00(1.00)
Iter 0290 | Time 0.3114(0.2982) | Bit/dim 6.3829(7.8664) | Steps 107(99.99) | Grad Norm 10.1387(14.1332) | Total Time 1.00(1.00)
Iter 0300 | Time 0.3130(0.3011) | Bit/dim 6.0015(7.4238) | Steps 107(101.67) | Grad Norm 9.0965(12.9566) | Total Time 1.00(1.00)
Iter 0310 | Time 0.2887(0.3036) | Bit/dim 5.8374(7.0462) | Steps 101(102.85) | Grad Norm 8.6771(11.9070) | Total Time 1.00(1.00)
Iter 0320 | Time 0.2888(0.3063) | Bit/dim 5.8299(6.7146) | Steps 101(103.93) | Grad Norm 8.8855(11.0819) | Total Time 1.00(1.00)
Iter 0330 | Time 0.3394(0.3150) | Bit/dim 5.5139(6.3939) | Steps 113(106.31) | Grad Norm 8.1837(10.3767) | Total Time 1.00(1.00)
Iter 0340 | Time 0.3396(0.3215) | Bit/dim 5.1990(6.1048) | Steps 113(108.07) | Grad Norm 7.9746(9.7573) | Total Time 1.00(1.00)
Iter 0350 | Time 0.3398(0.3263) | Bit/dim 5.0559(5.8424) | Steps 113(109.36) | Grad Norm 7.9646(9.2665) | Total Time 1.00(1.00)
Iter 0360 | Time 0.3397(0.3298) | Bit/dim 4.9461(5.6022) | Steps 113(110.32) | Grad Norm 7.7950(8.8831) | Total Time 1.00(1.00)
Iter 0370 | Time 0.3399(0.3324) | Bit/dim 4.6408(5.3693) | Steps 113(111.02) | Grad Norm 7.4980(8.5652) | Total Time 1.00(1.00)
Iter 0380 | Time 0.3396(0.3346) | Bit/dim 4.4259(5.1606) | Steps 113(111.54) | Grad Norm 7.5120(8.2760) | Total Time 1.00(1.00)
Iter 0390 | Time 0.3397(0.3346) | Bit/dim 4.2925(4.9537) | Steps 113(111.59) | Grad Norm 7.1479(8.0151) | Total Time 1.00(1.00)
Iter 0400 | Time 0.3181(0.3329) | Bit/dim 4.1343(4.7580) | Steps 107(111.16) | Grad Norm 7.4141(7.7956) | Total Time 1.00(1.00)
Iter 0410 | Time 0.3183(0.3297) | Bit/dim 3.9480(4.5726) | Steps 107(110.21) | Grad Norm 7.6713(7.6523) | Total Time 1.00(1.00)
Iter 0420 | Time 0.3176(0.3265) | Bit/dim 3.8815(4.3858) | Steps 107(109.37) | Grad Norm 6.6750(7.5199) | Total Time 1.00(1.00)
Iter 0430 | Time 0.3202(0.3250) | Bit/dim 3.6598(4.2062) | Steps 107(108.92) | Grad Norm 7.1557(7.3612) | Total Time 1.00(1.00)
Iter 0440 | Time 0.3577(0.3253) | Bit/dim 3.4860(4.0311) | Steps 113(108.73) | Grad Norm 5.7534(7.1017) | Total Time 1.00(1.00)
Iter 0450 | Time 0.3152(0.3262) | Bit/dim 3.4763(3.8712) | Steps 107(108.57) | Grad Norm 5.7524(6.8097) | Total Time 1.00(1.00)
Iter 0460 | Time 0.3402(0.3271) | Bit/dim 3.2793(3.7201) | Steps 113(108.82) | Grad Norm 5.6371(6.4720) | Total Time 1.00(1.00)
Iter 0470 | Time 0.3400(0.3288) | Bit/dim 3.0826(3.5956) | Steps 113(109.48) | Grad Norm 4.8826(6.1818) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3399(0.3310) | Bit/dim 3.1487(3.4833) | Steps 113(110.24) | Grad Norm 4.2345(5.7587) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3404(0.3334) | Bit/dim 3.0080(3.3778) | Steps 113(110.96) | Grad Norm 4.5585(5.3652) | Total Time 1.00(1.00)
Iter 0500 | Time 0.3415(0.3367) | Bit/dim 3.0418(3.2892) | Steps 113(111.50) | Grad Norm 3.7031(5.0036) | Total Time 1.00(1.00)
Iter 0510 | Time 0.3415(0.3380) | Bit/dim 2.7922(3.1961) | Steps 113(111.89) | Grad Norm 4.4143(4.7222) | Total Time 1.00(1.00)
Iter 0520 | Time 0.3176(0.3371) | Bit/dim 2.9096(3.1238) | Steps 107(111.70) | Grad Norm 2.8995(4.5360) | Total Time 1.00(1.00)
Iter 0530 | Time 0.3270(0.3382) | Bit/dim 2.7816(3.0563) | Steps 107(111.41) | Grad Norm 3.2713(4.3690) | Total Time 1.00(1.00)
Iter 0540 | Time 0.3168(0.3347) | Bit/dim 2.8678(2.9904) | Steps 107(110.72) | Grad Norm 3.0387(4.0084) | Total Time 1.00(1.00)
Iter 0550 | Time 0.3172(0.3301) | Bit/dim 2.7694(2.9382) | Steps 107(109.74) | Grad Norm 2.7687(3.8056) | Total Time 1.00(1.00)
Iter 0560 | Time 0.3190(0.3280) | Bit/dim 2.7648(2.8793) | Steps 107(109.02) | Grad Norm 4.1654(3.8013) | Total Time 1.00(1.00)
Iter 0570 | Time 0.3186(0.3256) | Bit/dim 2.6926(2.8442) | Steps 107(108.49) | Grad Norm 4.0644(4.0300) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3195(0.3245) | Bit/dim 2.6771(2.7979) | Steps 107(108.10) | Grad Norm 5.4898(4.4051) | Total Time 1.00(1.00)
Iter 0590 | Time 0.3258(0.3236) | Bit/dim 2.5840(2.7732) | Steps 107(107.81) | Grad Norm 3.1598(4.0705) | Total Time 1.00(1.00)
validating...
Epoch 0000 | Time 4.3734, Bit/dim 2.6279
Iter 0600 | Time 0.3226(0.3247) | Bit/dim 2.6810(2.7419) | Steps 107(107.60) | Grad Norm 6.0389(4.0923) | Total Time 1.00(1.00)
Iter 0610 | Time 0.3208(0.3234) | Bit/dim 2.5850(2.7119) | Steps 107(107.44) | Grad Norm 2.7477(3.9358) | Total Time 1.00(1.00)
Iter 0620 | Time 0.3198(0.3225) | Bit/dim 2.6116(2.6881) | Steps 107(107.32) | Grad Norm 3.4545(3.9030) | Total Time 1.00(1.00)
Iter 0630 | Time 0.3195(0.3218) | Bit/dim 2.5449(2.6609) | Steps 107(107.24) | Grad Norm 2.9844(3.7777) | Total Time 1.00(1.00)
Iter 0640 | Time 0.3195(0.3212) | Bit/dim 2.5491(2.6349) | Steps 107(107.18) | Grad Norm 1.9280(3.5809) | Total Time 1.00(1.00)
Iter 0650 | Time 0.3196(0.3207) | Bit/dim 2.4531(2.6139) | Steps 107(107.13) | Grad Norm 3.7492(3.3732) | Total Time 1.00(1.00)
Iter 0660 | Time 0.3244(0.3211) | Bit/dim 2.4662(2.5890) | Steps 107(107.10) | Grad Norm 3.4753(3.2466) | Total Time 1.00(1.00)
Iter 0670 | Time 0.3210(0.3210) | Bit/dim 2.4873(2.5708) | Steps 107(107.07) | Grad Norm 5.8150(3.5136) | Total Time 1.00(1.00)
Iter 0680 | Time 0.3206(0.3209) | Bit/dim 2.5791(2.5566) | Steps 107(107.05) | Grad Norm 5.0340(3.4212) | Total Time 1.00(1.00)
Iter 0690 | Time 0.3205(0.3208) | Bit/dim 2.5093(2.5407) | Steps 107(107.04) | Grad Norm 4.5245(3.6303) | Total Time 1.00(1.00)
Iter 0700 | Time 0.3204(0.3208) | Bit/dim 2.4602(2.5227) | Steps 107(107.03) | Grad Norm 4.3875(3.5764) | Total Time 1.00(1.00)
Iter 0710 | Time 0.3204(0.3208) | Bit/dim 2.4425(2.5044) | Steps 107(107.02) | Grad Norm 3.6222(4.0560) | Total Time 1.00(1.00)
Iter 0720 | Time 0.3207(0.3208) | Bit/dim 2.5195(2.4933) | Steps 107(107.02) | Grad Norm 4.2780(4.3486) | Total Time 1.00(1.00)
Iter 0730 | Time 0.3203(0.3207) | Bit/dim 2.4441(2.4826) | Steps 107(107.01) | Grad Norm 8.4030(4.6989) | Total Time 1.00(1.00)
Iter 0740 | Time 0.3321(0.3224) | Bit/dim 2.3759(2.4645) | Steps 107(107.01) | Grad Norm 3.6048(4.7920) | Total Time 1.00(1.00)
Iter 0750 | Time 0.3188(0.3240) | Bit/dim 2.4230(2.4531) | Steps 107(107.01) | Grad Norm 2.3960(4.1297) | Total Time 1.00(1.00)
Iter 0760 | Time 0.3189(0.3227) | Bit/dim 2.4508(2.4488) | Steps 107(107.00) | Grad Norm 5.1493(4.1038) | Total Time 1.00(1.00)
Iter 0770 | Time 0.2949(0.3210) | Bit/dim 2.3390(2.4336) | Steps 101(106.82) | Grad Norm 5.4653(4.5353) | Total Time 1.00(1.00)
Iter 0780 | Time 0.2949(0.3176) | Bit/dim 2.3831(2.4252) | Steps 101(106.09) | Grad Norm 2.6695(4.2260) | Total Time 1.00(1.00)
Iter 0790 | Time 0.2950(0.3129) | Bit/dim 2.3334(2.4065) | Steps 101(105.06) | Grad Norm 7.5229(4.2883) | Total Time 1.00(1.00)
Iter 0800 | Time 0.2947(0.3082) | Bit/dim 2.3480(2.3959) | Steps 101(104.00) | Grad Norm 4.8554(4.5059) | Total Time 1.00(1.00)
Iter 0810 | Time 0.2985(0.3054) | Bit/dim 2.3392(2.3910) | Steps 101(103.36) | Grad Norm 5.4182(4.5349) | Total Time 1.00(1.00)
Iter 0820 | Time 0.3032(0.3053) | Bit/dim 2.3583(2.3756) | Steps 101(102.74) | Grad Norm 5.6360(4.5148) | Total Time 1.00(1.00)
Iter 0830 | Time 0.2929(0.3027) | Bit/dim 2.3771(2.3707) | Steps 101(102.28) | Grad Norm 4.0444(4.2728) | Total Time 1.00(1.00)
Iter 0840 | Time 0.2928(0.3001) | Bit/dim 2.4323(2.3635) | Steps 101(101.95) | Grad Norm 2.9590(3.9907) | Total Time 1.00(1.00)
Iter 0850 | Time 0.2930(0.2996) | Bit/dim 2.3210(2.3558) | Steps 101(102.02) | Grad Norm 8.8387(4.8858) | Total Time 1.00(1.00)
Iter 0860 | Time 0.2927(0.2996) | Bit/dim 2.2745(2.3461) | Steps 101(102.18) | Grad Norm 11.2033(6.5314) | Total Time 1.00(1.00)
Iter 0870 | Time 0.2929(0.2998) | Bit/dim 2.3264(2.3363) | Steps 101(102.35) | Grad Norm 5.9956(6.7163) | Total Time 1.00(1.00)
Iter 0880 | Time 0.3170(0.3022) | Bit/dim 2.3299(2.3232) | Steps 107(102.97) | Grad Norm 7.2095(6.0036) | Total Time 1.00(1.00)
Iter 0890 | Time 0.2928(0.3012) | Bit/dim 2.2175(2.3094) | Steps 101(102.60) | Grad Norm 6.5750(5.5406) | Total Time 1.00(1.00)
Iter 0900 | Time 0.3178(0.3040) | Bit/dim 2.2786(2.3007) | Steps 107(102.81) | Grad Norm 3.4805(5.0901) | Total Time 1.00(1.00)
Iter 0910 | Time 0.2965(0.3046) | Bit/dim 2.2096(2.2890) | Steps 101(103.11) | Grad Norm 2.2245(4.6615) | Total Time 1.00(1.00)
Iter 0920 | Time 0.3193(0.3054) | Bit/dim 2.2696(2.2802) | Steps 107(103.36) | Grad Norm 3.9961(4.3144) | Total Time 1.00(1.00)
Iter 0930 | Time 0.2951(0.3047) | Bit/dim 2.2207(2.2720) | Steps 101(103.23) | Grad Norm 5.6484(4.2377) | Total Time 1.00(1.00)
Iter 0940 | Time 0.2953(0.3048) | Bit/dim 2.2912(2.2679) | Steps 101(103.30) | Grad Norm 6.0709(4.2092) | Total Time 1.00(1.00)
Iter 0950 | Time 0.3202(0.3049) | Bit/dim 2.2897(2.2585) | Steps 107(103.32) | Grad Norm 4.9278(4.2226) | Total Time 1.00(1.00)
Iter 0960 | Time 0.3214(0.3069) | Bit/dim 2.1781(2.2470) | Steps 107(103.82) | Grad Norm 3.6742(4.3710) | Total Time 1.00(1.00)
Iter 0970 | Time 0.3195(0.3090) | Bit/dim 2.1600(2.2368) | Steps 107(104.36) | Grad Norm 11.3846(5.1317) | Total Time 1.00(1.00)
Iter 0980 | Time 0.3193(0.3112) | Bit/dim 2.1731(2.2312) | Steps 107(104.91) | Grad Norm 4.0378(5.3075) | Total Time 1.00(1.00)
Iter 0990 | Time 0.3194(0.3122) | Bit/dim 2.1708(2.2288) | Steps 107(105.16) | Grad Norm 7.3040(5.2618) | Total Time 1.00(1.00)
Iter 1000 | Time 0.3195(0.3122) | Bit/dim 2.1897(2.2217) | Steps 107(105.19) | Grad Norm 5.1065(4.8867) | Total Time 1.00(1.00)
Iter 1010 | Time 0.3194(0.3142) | Bit/dim 2.1111(2.2156) | Steps 107(105.67) | Grad Norm 4.1685(4.7648) | Total Time 1.00(1.00)
Iter 1020 | Time 0.3195(0.3144) | Bit/dim 2.1878(2.2100) | Steps 107(105.73) | Grad Norm 2.5061(4.5926) | Total Time 1.00(1.00)
Iter 1030 | Time 0.3198(0.3158) | Bit/dim 2.2255(2.2090) | Steps 107(106.06) | Grad Norm 8.1131(5.4056) | Total Time 1.00(1.00)
Iter 1040 | Time 0.3194(0.3156) | Bit/dim 2.1773(2.1999) | Steps 107(106.00) | Grad Norm 4.1140(5.3301) | Total Time 1.00(1.00)
Iter 1050 | Time 0.3196(0.3161) | Bit/dim 2.1317(2.1940) | Steps 107(106.13) | Grad Norm 4.1153(5.3167) | Total Time 1.00(1.00)
Iter 1060 | Time 0.3197(0.3170) | Bit/dim 2.1034(2.1855) | Steps 107(106.36) | Grad Norm 4.3664(4.8819) | Total Time 1.00(1.00)
Iter 1070 | Time 0.3195(0.3177) | Bit/dim 2.2378(2.1840) | Steps 107(106.53) | Grad Norm 2.8894(4.7658) | Total Time 1.00(1.00)
Iter 1080 | Time 0.3234(0.3183) | Bit/dim 2.1816(2.1794) | Steps 107(106.65) | Grad Norm 3.9336(4.3382) | Total Time 1.00(1.00)
Iter 1090 | Time 0.3335(0.3226) | Bit/dim 2.1629(2.1751) | Steps 107(106.74) | Grad Norm 6.8183(4.5581) | Total Time 1.00(1.00)
Iter 1100 | Time 0.3346(0.3257) | Bit/dim 2.1383(2.1709) | Steps 107(106.81) | Grad Norm 5.0323(4.6495) | Total Time 1.00(1.00)
Iter 1110 | Time 0.3208(0.3254) | Bit/dim 2.1490(2.1644) | Steps 107(106.86) | Grad Norm 2.6398(4.3094) | Total Time 1.00(1.00)
Iter 1120 | Time 0.3207(0.3242) | Bit/dim 2.0914(2.1547) | Steps 107(106.90) | Grad Norm 5.8627(4.0233) | Total Time 1.00(1.00)
Iter 1130 | Time 0.3314(0.3246) | Bit/dim 2.0899(2.1517) | Steps 107(106.92) | Grad Norm 1.4144(3.6473) | Total Time 1.00(1.00)
Iter 1140 | Time 0.3206(0.3242) | Bit/dim 2.1097(2.1466) | Steps 107(106.94) | Grad Norm 1.2418(3.8439) | Total Time 1.00(1.00)
Iter 1150 | Time 0.3204(0.3239) | Bit/dim 2.0799(2.1402) | Steps 107(107.12) | Grad Norm 3.1079(3.7472) | Total Time 1.00(1.00)
Iter 1160 | Time 0.3205(0.3230) | Bit/dim 2.0441(2.1306) | Steps 107(107.09) | Grad Norm 3.7445(4.0163) | Total Time 1.00(1.00)
Iter 1170 | Time 0.3312(0.3237) | Bit/dim 2.1744(2.1303) | Steps 107(107.06) | Grad Norm 5.5918(3.9454) | Total Time 1.00(1.00)
Iter 1180 | Time 0.3311(0.3256) | Bit/dim 2.1164(2.1266) | Steps 107(107.05) | Grad Norm 4.3081(4.1307) | Total Time 1.00(1.00)
Iter 1190 | Time 0.3288(0.3270) | Bit/dim 2.1227(2.1236) | Steps 107(107.03) | Grad Norm 6.0155(4.3236) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 4.8344, Bit/dim 2.1050
Iter 1200 | Time 0.3279(0.3291) | Bit/dim 2.1564(2.1208) | Steps 107(107.37) | Grad Norm 4.4245(4.2302) | Total Time 1.00(1.00)
Iter 1210 | Time 0.3335(0.3293) | Bit/dim 2.1258(2.1215) | Steps 107(107.41) | Grad Norm 6.5091(4.6312) | Total Time 1.00(1.00)
Iter 1220 | Time 0.3182(0.3284) | Bit/dim 2.1563(2.1206) | Steps 107(107.30) | Grad Norm 3.0789(4.6788) | Total Time 1.00(1.00)
Iter 1230 | Time 0.3290(0.3262) | Bit/dim 2.0639(2.1111) | Steps 107(107.22) | Grad Norm 7.7022(5.0524) | Total Time 1.00(1.00)
Iter 1240 | Time 0.3334(0.3278) | Bit/dim 2.0982(2.1055) | Steps 107(107.31) | Grad Norm 3.0232(4.8859) | Total Time 1.00(1.00)
Iter 1250 | Time 0.3272(0.3296) | Bit/dim 2.0963(2.1035) | Steps 107(107.39) | Grad Norm 2.4431(4.4461) | Total Time 1.00(1.00)
Iter 1260 | Time 0.3183(0.3291) | Bit/dim 2.1182(2.1013) | Steps 107(107.29) | Grad Norm 2.6406(4.2027) | Total Time 1.00(1.00)
Iter 1270 | Time 0.3183(0.3266) | Bit/dim 2.0971(2.1019) | Steps 107(107.37) | Grad Norm 2.8542(3.9760) | Total Time 1.00(1.00)
Iter 1280 | Time 0.3252(0.3254) | Bit/dim 1.9630(2.0971) | Steps 113(107.74) | Grad Norm 3.7680(3.9668) | Total Time 1.00(1.00)
Iter 1290 | Time 0.3397(0.3251) | Bit/dim 2.0461(2.0913) | Steps 113(108.20) | Grad Norm 2.3018(3.6814) | Total Time 1.00(1.00)
Iter 1300 | Time 0.3184(0.3242) | Bit/dim 2.0949(2.0896) | Steps 107(108.46) | Grad Norm 4.0182(3.6374) | Total Time 1.00(1.00)
Iter 1310 | Time 0.3182(0.3249) | Bit/dim 2.0974(2.0898) | Steps 107(109.28) | Grad Norm 3.0924(3.3779) | Total Time 1.00(1.00)
Iter 1320 | Time 0.3418(0.3274) | Bit/dim 2.0532(2.0849) | Steps 113(109.94) | Grad Norm 2.2831(3.3297) | Total Time 1.00(1.00)
Iter 1330 | Time 0.3252(0.3287) | Bit/dim 2.1127(2.0823) | Steps 113(110.44) | Grad Norm 3.5622(3.4833) | Total Time 1.00(1.00)
Iter 1340 | Time 0.3252(0.3275) | Bit/dim 2.0983(2.0877) | Steps 113(110.81) | Grad Norm 4.8204(3.4748) | Total Time 1.00(1.00)
Iter 1350 | Time 0.3251(0.3275) | Bit/dim 2.0290(2.0837) | Steps 113(111.54) | Grad Norm 4.0972(3.5742) | Total Time 1.00(1.00)
Iter 1360 | Time 0.3257(0.3276) | Bit/dim 2.0597(2.0834) | Steps 113(112.06) | Grad Norm 5.9692(3.8595) | Total Time 1.00(1.00)
Iter 1370 | Time 0.3374(0.3280) | Bit/dim 2.0284(2.0768) | Steps 113(112.13) | Grad Norm 4.3050(3.8612) | Total Time 1.00(1.00)
Iter 1380 | Time 0.3403(0.3321) | Bit/dim 2.1063(2.0737) | Steps 113(112.53) | Grad Norm 3.1178(3.5569) | Total Time 1.00(1.00)
Iter 1390 | Time 0.3419(0.3337) | Bit/dim 2.0964(2.0740) | Steps 113(112.65) | Grad Norm 3.8816(3.3345) | Total Time 1.00(1.00)
Iter 1400 | Time 0.3254(0.3329) | Bit/dim 2.0524(2.0692) | Steps 113(112.58) | Grad Norm 3.8585(3.3815) | Total Time 1.00(1.00)
Iter 1410 | Time 0.3251(0.3310) | Bit/dim 2.0058(2.0680) | Steps 113(112.69) | Grad Norm 3.7807(3.4550) | Total Time 1.00(1.00)
Iter 1420 | Time 0.3249(0.3296) | Bit/dim 2.0403(2.0663) | Steps 113(112.77) | Grad Norm 6.4831(3.5478) | Total Time 1.00(1.00)
Iter 1430 | Time 0.3255(0.3287) | Bit/dim 2.0949(2.0692) | Steps 113(112.83) | Grad Norm 2.6840(3.5565) | Total Time 1.00(1.00)
Iter 1440 | Time 0.3411(0.3294) | Bit/dim 2.0656(2.0657) | Steps 113(112.88) | Grad Norm 5.5585(3.7899) | Total Time 1.00(1.00)
Iter 1450 | Time 0.3250(0.3284) | Bit/dim 2.0620(2.0667) | Steps 113(112.91) | Grad Norm 3.2437(3.7058) | Total Time 1.00(1.00)
Iter 1460 | Time 0.3311(0.3277) | Bit/dim 2.0182(2.0643) | Steps 113(112.93) | Grad Norm 3.5467(3.5392) | Total Time 1.00(1.00)
Iter 1470 | Time 0.3249(0.3270) | Bit/dim 2.0395(2.0656) | Steps 113(112.95) | Grad Norm 3.0794(3.3423) | Total Time 1.00(1.00)
Iter 1480 | Time 0.3248(0.3265) | Bit/dim 2.0629(2.0624) | Steps 113(112.96) | Grad Norm 2.7291(3.1923) | Total Time 1.00(1.00)
Iter 1490 | Time 0.3249(0.3262) | Bit/dim 2.0929(2.0570) | Steps 113(112.97) | Grad Norm 2.2375(2.9564) | Total Time 1.00(1.00)
Iter 1500 | Time 0.3377(0.3290) | Bit/dim 2.0341(2.0546) | Steps 113(112.98) | Grad Norm 4.4440(2.9346) | Total Time 1.00(1.00)
Iter 1510 | Time 0.3378(0.3314) | Bit/dim 2.0481(2.0529) | Steps 113(112.99) | Grad Norm 2.0492(3.0315) | Total Time 1.00(1.00)
Iter 1520 | Time 0.3272(0.3309) | Bit/dim 2.0449(2.0533) | Steps 113(112.99) | Grad Norm 2.8204(3.1264) | Total Time 1.00(1.00)
Iter 1530 | Time 0.3273(0.3302) | Bit/dim 2.0329(2.0512) | Steps 113(113.16) | Grad Norm 2.3174(3.1172) | Total Time 1.00(1.00)
Iter 1540 | Time 0.3274(0.3295) | Bit/dim 2.0512(2.0518) | Steps 113(113.12) | Grad Norm 2.8681(3.0143) | Total Time 1.00(1.00)
Iter 1550 | Time 0.3273(0.3289) | Bit/dim 2.0645(2.0491) | Steps 113(113.09) | Grad Norm 3.8233(3.1090) | Total Time 1.00(1.00)
Iter 1560 | Time 0.3275(0.3285) | Bit/dim 2.0472(2.0513) | Steps 113(113.06) | Grad Norm 2.5923(3.3505) | Total Time 1.00(1.00)
Iter 1570 | Time 0.3274(0.3282) | Bit/dim 2.0137(2.0521) | Steps 113(113.05) | Grad Norm 2.5576(3.3390) | Total Time 1.00(1.00)
Iter 1580 | Time 0.3272(0.3280) | Bit/dim 2.0415(2.0451) | Steps 113(113.03) | Grad Norm 3.3260(3.1818) | Total Time 1.00(1.00)
Iter 1590 | Time 0.3272(0.3279) | Bit/dim 2.0205(2.0419) | Steps 113(113.03) | Grad Norm 1.5746(2.8274) | Total Time 1.00(1.00)
Iter 1600 | Time 0.3272(0.3278) | Bit/dim 2.0495(2.0435) | Steps 113(113.02) | Grad Norm 3.9833(2.8917) | Total Time 1.00(1.00)
Iter 1610 | Time 0.3397(0.3302) | Bit/dim 2.0527(2.0417) | Steps 113(113.18) | Grad Norm 2.6903(2.7560) | Total Time 1.00(1.00)
Iter 1620 | Time 0.3249(0.3325) | Bit/dim 2.0062(2.0373) | Steps 113(113.30) | Grad Norm 1.9344(2.6198) | Total Time 1.00(1.00)
Iter 1630 | Time 0.3378(0.3336) | Bit/dim 2.0362(2.0362) | Steps 113(113.22) | Grad Norm 2.7888(2.4897) | Total Time 1.00(1.00)
Iter 1640 | Time 0.3328(0.3317) | Bit/dim 2.0543(2.0382) | Steps 119(113.35) | Grad Norm 2.9744(2.4835) | Total Time 1.00(1.00)
Iter 1650 | Time 0.3385(0.3339) | Bit/dim 2.0229(2.0383) | Steps 113(113.56) | Grad Norm 3.8096(2.6042) | Total Time 1.00(1.00)
Iter 1660 | Time 0.3252(0.3327) | Bit/dim 2.0326(2.0374) | Steps 113(113.56) | Grad Norm 2.0284(2.5791) | Total Time 1.00(1.00)
Iter 1670 | Time 0.3249(0.3307) | Bit/dim 2.0498(2.0367) | Steps 113(113.41) | Grad Norm 2.7660(2.6847) | Total Time 1.00(1.00)
Iter 1680 | Time 0.3248(0.3296) | Bit/dim 2.0556(2.0374) | Steps 113(113.60) | Grad Norm 1.6011(2.6265) | Total Time 1.00(1.00)
Iter 1690 | Time 0.3250(0.3286) | Bit/dim 2.0475(2.0363) | Steps 113(113.60) | Grad Norm 3.8308(2.7392) | Total Time 1.00(1.00)
Iter 1700 | Time 0.3332(0.3292) | Bit/dim 1.9933(2.0314) | Steps 119(114.23) | Grad Norm 2.7011(3.0108) | Total Time 1.00(1.00)
Iter 1710 | Time 0.3247(0.3281) | Bit/dim 2.0983(2.0327) | Steps 113(113.91) | Grad Norm 1.9207(2.9183) | Total Time 1.00(1.00)
Iter 1720 | Time 0.3250(0.3275) | Bit/dim 2.0588(2.0330) | Steps 113(113.82) | Grad Norm 5.7667(3.0519) | Total Time 1.00(1.00)
Iter 1730 | Time 0.3249(0.3268) | Bit/dim 2.0291(2.0322) | Steps 113(113.61) | Grad Norm 4.1839(3.4909) | Total Time 1.00(1.00)
Iter 1740 | Time 0.3251(0.3269) | Bit/dim 2.0248(2.0329) | Steps 113(113.88) | Grad Norm 3.1825(3.3681) | Total Time 1.00(1.00)
Iter 1750 | Time 0.3249(0.3270) | Bit/dim 1.9941(2.0278) | Steps 113(114.10) | Grad Norm 2.3216(3.2270) | Total Time 1.00(1.00)
Iter 1760 | Time 0.3250(0.3278) | Bit/dim 2.0092(2.0213) | Steps 113(113.96) | Grad Norm 2.6247(3.1246) | Total Time 1.00(1.00)
Iter 1770 | Time 0.3249(0.3273) | Bit/dim 2.0180(2.0194) | Steps 113(113.87) | Grad Norm 2.9455(2.9670) | Total Time 1.00(1.00)
Iter 1780 | Time 0.3247(0.3271) | Bit/dim 1.9957(2.0203) | Steps 113(113.94) | Grad Norm 2.1470(2.8350) | Total Time 1.00(1.00)
Iter 1790 | Time 0.3253(0.3270) | Bit/dim 1.9695(2.0186) | Steps 113(113.83) | Grad Norm 2.5470(2.6749) | Total Time 1.00(1.00)
validating...
Epoch 0002 | Time 4.8648, Bit/dim 2.0109
Iter 1800 | Time 0.3409(0.3279) | Bit/dim 2.0747(2.0206) | Steps 113(114.23) | Grad Norm 2.6099(2.7076) | Total Time 1.00(1.00)
Iter 1810 | Time 0.3271(0.3290) | Bit/dim 1.9920(2.0193) | Steps 113(114.36) | Grad Norm 2.4835(2.6526) | Total Time 1.00(1.00)
Iter 1820 | Time 0.3274(0.3286) | Bit/dim 2.0668(2.0190) | Steps 113(114.00) | Grad Norm 1.2634(2.4671) | Total Time 1.00(1.00)
Iter 1830 | Time 0.3273(0.3285) | Bit/dim 1.9612(2.0186) | Steps 113(113.88) | Grad Norm 2.4985(2.3710) | Total Time 1.00(1.00)
Iter 1840 | Time 0.3273(0.3299) | Bit/dim 2.0367(2.0112) | Steps 113(114.44) | Grad Norm 2.6672(2.3812) | Total Time 1.00(1.00)
Iter 1850 | Time 0.3357(0.3311) | Bit/dim 2.0375(2.0130) | Steps 119(114.75) | Grad Norm 2.4814(2.5638) | Total Time 1.00(1.00)
Iter 1860 | Time 0.3359(0.3335) | Bit/dim 1.9754(2.0141) | Steps 119(115.71) | Grad Norm 4.0628(2.7263) | Total Time 1.00(1.00)
Iter 1870 | Time 0.3374(0.3356) | Bit/dim 2.0183(2.0150) | Steps 119(116.42) | Grad Norm 3.0600(2.9884) | Total Time 1.00(1.00)
Iter 1880 | Time 0.3375(0.3343) | Bit/dim 2.0679(2.0155) | Steps 119(115.85) | Grad Norm 3.0264(2.9755) | Total Time 1.00(1.00)
Iter 1890 | Time 0.3828(0.3388) | Bit/dim 1.9452(2.0164) | Steps 125(116.09) | Grad Norm 2.0678(2.7438) | Total Time 1.00(1.00)
Iter 1900 | Time 0.3560(0.3384) | Bit/dim 1.9997(2.0128) | Steps 119(115.75) | Grad Norm 3.1122(2.6692) | Total Time 1.00(1.00)
Iter 1910 | Time 0.3350(0.3397) | Bit/dim 2.0714(2.0117) | Steps 113(116.26) | Grad Norm 2.5652(2.6316) | Total Time 1.00(1.00)
Iter 1920 | Time 0.3807(0.3396) | Bit/dim 2.0178(2.0150) | Steps 125(116.23) | Grad Norm 2.9951(2.6723) | Total Time 1.00(1.00)
Iter 1930 | Time 0.3783(0.3436) | Bit/dim 2.0091(2.0153) | Steps 125(116.53) | Grad Norm 2.8511(2.6011) | Total Time 1.00(1.00)
Iter 1940 | Time 0.3755(0.3467) | Bit/dim 2.0153(2.0121) | Steps 125(116.70) | Grad Norm 2.8352(2.5562) | Total Time 1.00(1.00)
Iter 1950 | Time 0.3271(0.3448) | Bit/dim 2.0264(2.0105) | Steps 113(116.50) | Grad Norm 2.3324(2.4910) | Total Time 1.00(1.00)
Iter 1960 | Time 0.3514(0.3438) | Bit/dim 1.9514(2.0082) | Steps 119(116.80) | Grad Norm 1.8751(2.4567) | Total Time 1.00(1.00)
Iter 1970 | Time 0.3436(0.3471) | Bit/dim 2.0541(2.0080) | Steps 113(116.90) | Grad Norm 1.8341(2.5957) | Total Time 1.00(1.00)
Iter 1980 | Time 0.3273(0.3471) | Bit/dim 2.0264(2.0083) | Steps 113(117.20) | Grad Norm 1.8090(2.4716) | Total Time 1.00(1.00)
Iter 1990 | Time 0.3324(0.3506) | Bit/dim 1.9931(2.0051) | Steps 113(118.42) | Grad Norm 3.2757(2.5246) | Total Time 1.00(1.00)
Iter 2000 | Time 0.3661(0.3548) | Bit/dim 2.0032(2.0023) | Steps 125(119.53) | Grad Norm 1.6982(2.5336) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.3032(0.3032) | Bit/dim 30.4492(30.4492) | Steps 41(41.00) | Grad Norm 18.4004(18.4004) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1258(0.2567) | Bit/dim 30.2797(30.4581) | Steps 41(41.00) | Grad Norm 18.1990(18.4008) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1252(0.2225) | Bit/dim 30.5164(30.4594) | Steps 41(41.00) | Grad Norm 18.3588(18.4031) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1206(0.1961) | Bit/dim 30.1078(30.3921) | Steps 41(41.00) | Grad Norm 17.8453(18.3176) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1206(0.1764) | Bit/dim 29.6670(30.2724) | Steps 41(41.00) | Grad Norm 17.5414(18.1856) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1211(0.1619) | Bit/dim 29.5042(30.0959) | Steps 41(41.00) | Grad Norm 17.7279(18.0229) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1204(0.1511) | Bit/dim 28.9761(29.8569) | Steps 41(41.00) | Grad Norm 17.4508(17.8827) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1206(0.1432) | Bit/dim 28.4421(29.5307) | Steps 41(41.00) | Grad Norm 17.9020(17.7909) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1204(0.1374) | Bit/dim 27.4851(29.1199) | Steps 41(41.00) | Grad Norm 18.4092(17.9047) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1206(0.1332) | Bit/dim 26.4772(28.5521) | Steps 41(41.00) | Grad Norm 20.4770(18.3288) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1205(0.1302) | Bit/dim 24.9287(27.7876) | Steps 41(41.00) | Grad Norm 23.3377(19.2868) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1255(0.1293) | Bit/dim 23.1254(26.7868) | Steps 41(41.00) | Grad Norm 26.2313(20.8839) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1256(0.1283) | Bit/dim 21.3340(25.5526) | Steps 41(41.00) | Grad Norm 29.3945(22.9369) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1477(0.1287) | Bit/dim 19.3865(24.1667) | Steps 47(41.18) | Grad Norm 31.8752(25.0194) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1502(0.1346) | Bit/dim 17.6136(22.6874) | Steps 47(42.71) | Grad Norm 31.8339(26.6665) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1516(0.1389) | Bit/dim 16.5940(21.2030) | Steps 47(43.99) | Grad Norm 29.4685(27.6062) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1596(0.1426) | Bit/dim 15.6685(19.8420) | Steps 53(45.13) | Grad Norm 27.6267(27.8615) | Total Time 1.00(1.00)
Iter 0170 | Time 0.2071(0.1576) | Bit/dim 14.7902(18.6099) | Steps 65(49.79) | Grad Norm 25.0996(27.4625) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Iter 0000 | Time 0.3012(0.3012) | Bit/dim 29.7988(29.7988) | Steps 41(41.00) | Grad Norm 16.2978(16.2978) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1212(0.2544) | Bit/dim 29.5106(29.7575) | Steps 41(41.00) | Grad Norm 16.0212(16.2927) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1212(0.2196) | Bit/dim 29.5528(29.7102) | Steps 41(41.00) | Grad Norm 16.1606(16.2852) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1224(0.1939) | Bit/dim 29.4205(29.6410) | Steps 41(41.00) | Grad Norm 16.1862(16.2678) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1232(0.1750) | Bit/dim 29.2357(29.5330) | Steps 41(41.00) | Grad Norm 16.2673(16.2065) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1213(0.1610) | Bit/dim 28.9053(29.3982) | Steps 41(41.00) | Grad Norm 16.0182(16.1768) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1212(0.1506) | Bit/dim 28.3429(29.2067) | Steps 41(41.00) | Grad Norm 15.9877(16.1707) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1213(0.1431) | Bit/dim 27.9703(28.9652) | Steps 41(41.00) | Grad Norm 16.8747(16.3014) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1221(0.1376) | Bit/dim 27.4442(28.6463) | Steps 41(41.00) | Grad Norm 18.4272(16.6595) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1268(0.1336) | Bit/dim 26.3670(28.1739) | Steps 41(41.00) | Grad Norm 21.2410(17.4873) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1263(0.1317) | Bit/dim 24.4796(27.4379) | Steps 41(41.00) | Grad Norm 24.9402(19.1112) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1323(0.1303) | Bit/dim 22.0149(26.3136) | Steps 41(41.00) | Grad Norm 30.6579(21.6276) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1516(0.1350) | Bit/dim 19.7098(24.8190) | Steps 47(42.30) | Grad Norm 33.0245(24.5049) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1506(0.1392) | Bit/dim 17.4644(23.1077) | Steps 47(43.53) | Grad Norm 30.8852(26.7212) | Total Time 1.00(1.00)
Iter 0140 | Time 0.2106(0.1440) | Bit/dim 16.0632(21.4071) | Steps 65(44.98) | Grad Norm 23.8337(26.6248) | Total Time 1.00(1.00)
Iter 0150 | Time 0.2119(0.1619) | Bit/dim 14.7003(19.7997) | Steps 65(50.24) | Grad Norm 19.7625(25.3034) | Total Time 1.00(1.00)
Iter 0160 | Time 0.2097(0.1751) | Bit/dim 13.8443(18.3787) | Steps 65(54.11) | Grad Norm 14.7800(22.9544) | Total Time 1.00(1.00)
Iter 0170 | Time 0.2080(0.1847) | Bit/dim 12.8659(17.0971) | Steps 65(56.97) | Grad Norm 11.5304(20.2063) | Total Time 1.00(1.00)
Iter 0180 | Time 0.2220(0.1924) | Bit/dim 12.4080(15.9211) | Steps 71(59.77) | Grad Norm 9.6506(17.6139) | Total Time 1.00(1.00)
Iter 0190 | Time 0.2679(0.2058) | Bit/dim 10.9605(14.7864) | Steps 83(64.09) | Grad Norm 8.8094(15.3840) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2721(0.2228) | Bit/dim 10.7167(13.8055) | Steps 83(69.06) | Grad Norm 8.0526(13.5162) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2710(0.2356) | Bit/dim 9.7427(12.8631) | Steps 83(72.72) | Grad Norm 7.6958(12.0331) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2697(0.2458) | Bit/dim 9.1394(11.9625) | Steps 89(76.27) | Grad Norm 7.5262(10.8729) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2735(0.2535) | Bit/dim 8.2315(11.1020) | Steps 89(79.61) | Grad Norm 7.2882(9.9476) | Total Time 1.00(1.00)
Iter 0240 | Time 0.2591(0.2565) | Bit/dim 7.9348(10.3108) | Steps 83(80.92) | Grad Norm 6.9238(9.1995) | Total Time 1.00(1.00)
Iter 0250 | Time 0.2455(0.2569) | Bit/dim 7.0793(9.5555) | Steps 83(81.47) | Grad Norm 6.6249(8.5861) | Total Time 1.00(1.00)
Iter 0260 | Time 0.2538(0.2553) | Bit/dim 6.8590(8.8630) | Steps 83(81.87) | Grad Norm 6.5290(8.0671) | Total Time 1.00(1.00)
Iter 0270 | Time 0.2465(0.2564) | Bit/dim 6.0602(8.2290) | Steps 83(82.17) | Grad Norm 5.7838(7.5717) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3059(0.2687) | Bit/dim 6.0507(7.6562) | Steps 101(86.84) | Grad Norm 4.6730(6.9608) | Total Time 1.00(1.00)
Iter 0290 | Time 0.3058(0.2788) | Bit/dim 5.6906(7.1529) | Steps 101(90.56) | Grad Norm 4.1087(6.2833) | Total Time 1.00(1.00)
Iter 0300 | Time 0.3050(0.2860) | Bit/dim 5.3949(6.7125) | Steps 101(93.30) | Grad Norm 3.5995(5.6293) | Total Time 1.00(1.00)
Iter 0310 | Time 0.3050(0.2913) | Bit/dim 5.2016(6.3219) | Steps 101(95.32) | Grad Norm 3.6289(5.0737) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Epoch 0
Iter 0000 | Time 0.3019(0.3019) | Bit/dim 29.7938(29.7938) | Steps 41(41.00) | Grad Norm 17.3070(17.3070) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1233(0.2549) | Bit/dim 29.8372(29.7760) | Steps 41(41.00) | Grad Norm 17.5029(17.3165) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1267(0.2210) | Bit/dim 29.7278(29.7488) | Steps 41(41.00) | Grad Norm 17.3938(17.3017) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1255(0.1960) | Bit/dim 29.4756(29.6972) | Steps 41(41.00) | Grad Norm 16.9428(17.2130) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1256(0.1776) | Bit/dim 29.1899(29.6007) | Steps 41(41.00) | Grad Norm 16.2461(17.0624) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1259(0.1641) | Bit/dim 28.9700(29.4855) | Steps 41(41.00) | Grad Norm 16.0093(16.8670) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1215(0.1533) | Bit/dim 28.7252(29.3141) | Steps 41(41.00) | Grad Norm 15.8287(16.6126) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1214(0.1451) | Bit/dim 28.3146(29.0955) | Steps 41(41.00) | Grad Norm 15.4601(16.3346) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1291(0.1396) | Bit/dim 27.8617(28.8271) | Steps 41(41.00) | Grad Norm 15.1998(16.0762) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1270(0.1364) | Bit/dim 27.1921(28.4982) | Steps 41(41.00) | Grad Norm 15.6767(15.9243) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1271(0.1340) | Bit/dim 26.4183(28.0412) | Steps 41(41.00) | Grad Norm 17.0811(15.9745) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1232(0.1319) | Bit/dim 25.0435(27.4149) | Steps 41(41.00) | Grad Norm 18.9588(16.5720) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1233(0.1298) | Bit/dim 23.2806(26.5450) | Steps 41(41.00) | Grad Norm 23.3017(18.0570) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1263(0.1283) | Bit/dim 21.6407(25.4306) | Steps 41(41.00) | Grad Norm 29.0121(20.4448) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1476(0.1279) | Bit/dim 20.1431(24.2251) | Steps 47(41.18) | Grad Norm 35.2843(23.5498) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1476(0.1331) | Bit/dim 19.7200(23.1230) | Steps 47(42.71) | Grad Norm 34.6332(26.5549) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1479(0.1371) | Bit/dim 18.5472(22.0435) | Steps 47(43.84) | Grad Norm 35.0591(28.6819) | Total Time 1.00(1.00)
Iter 0170 | Time 0.1257(0.1396) | Bit/dim 17.5955(21.0172) | Steps 41(44.49) | Grad Norm 35.8116(30.3836) | Total Time 1.00(1.00)
Iter 0180 | Time 0.1318(0.1372) | Bit/dim 16.7601(20.0336) | Steps 47(44.71) | Grad Norm 37.1848(31.9665) | Total Time 1.00(1.00)
Iter 0190 | Time 0.1845(0.1484) | Bit/dim 15.9055(19.0013) | Steps 59(48.19) | Grad Norm 37.2091(33.5538) | Total Time 1.00(1.00)
Iter 0200 | Time 0.1851(0.1578) | Bit/dim 14.7951(17.9836) | Steps 59(51.03) | Grad Norm 37.1701(34.7224) | Total Time 1.00(1.00)
Iter 0210 | Time 0.1853(0.1651) | Bit/dim 13.2840(16.9104) | Steps 59(53.12) | Grad Norm 38.3234(35.6606) | Total Time 1.00(1.00)
Iter 0220 | Time 0.1961(0.1716) | Bit/dim 11.9911(15.7650) | Steps 65(55.35) | Grad Norm 37.0401(36.2812) | Total Time 1.00(1.00)
Iter 0230 | Time 0.1943(0.1773) | Bit/dim 10.8383(14.5804) | Steps 65(57.89) | Grad Norm 33.5087(36.1232) | Total Time 1.00(1.00)
Iter 0240 | Time 0.2242(0.1844) | Bit/dim 9.6516(13.4027) | Steps 71(60.28) | Grad Norm 28.8005(34.8415) | Total Time 1.00(1.00)
Iter 0250 | Time 0.2144(0.1959) | Bit/dim 8.4579(12.2328) | Steps 71(63.70) | Grad Norm 23.2415(32.4664) | Total Time 1.00(1.00)
Iter 0260 | Time 0.2260(0.2027) | Bit/dim 7.7875(11.1403) | Steps 77(67.06) | Grad Norm 17.5623(29.1690) | Total Time 1.00(1.00)
Iter 0270 | Time 0.2467(0.2093) | Bit/dim 7.3288(10.1997) | Steps 83(70.02) | Grad Norm 13.6594(25.4746) | Total Time 1.00(1.00)
Iter 0280 | Time 0.2462(0.2190) | Bit/dim 6.9802(9.3905) | Steps 83(73.43) | Grad Norm 11.8636(22.1230) | Total Time 1.00(1.00)
Iter 0290 | Time 0.2459(0.2262) | Bit/dim 6.5301(8.6637) | Steps 83(75.94) | Grad Norm 10.0210(19.0875) | Total Time 1.00(1.00)
Iter 0300 | Time 0.2804(0.2403) | Bit/dim 6.1225(8.0275) | Steps 95(80.81) | Grad Norm 8.6886(16.5199) | Total Time 1.00(1.00)
Iter 0310 | Time 0.2804(0.2509) | Bit/dim 5.8088(7.4915) | Steps 95(84.54) | Grad Norm 8.0158(14.3416) | Total Time 1.00(1.00)
Iter 0320 | Time 0.2802(0.2588) | Bit/dim 5.5561(7.0391) | Steps 95(87.28) | Grad Norm 7.0350(12.4909) | Total Time 1.00(1.00)
Iter 0330 | Time 0.2804(0.2645) | Bit/dim 5.4882(6.6488) | Steps 95(89.31) | Grad Norm 6.6811(10.9985) | Total Time 1.00(1.00)
Iter 0340 | Time 0.2902(0.2703) | Bit/dim 5.2937(6.3019) | Steps 95(90.80) | Grad Norm 7.6222(9.8528) | Total Time 1.00(1.00)
Iter 0350 | Time 0.3142(0.2811) | Bit/dim 5.2688(6.0172) | Steps 101(93.34) | Grad Norm 7.1523(8.9926) | Total Time 1.00(1.00)
Iter 0360 | Time 0.3223(0.2915) | Bit/dim 4.9815(5.7450) | Steps 107(96.50) | Grad Norm 8.9292(8.3641) | Total Time 1.00(1.00)
Iter 0370 | Time 0.3260(0.2990) | Bit/dim 4.9498(5.5100) | Steps 107(99.26) | Grad Norm 6.7141(7.8519) | Total Time 1.00(1.00)
Iter 0380 | Time 0.3227(0.3055) | Bit/dim 4.6844(5.2986) | Steps 107(101.29) | Grad Norm 5.3547(7.3033) | Total Time 1.00(1.00)
Iter 0390 | Time 0.3241(0.3103) | Bit/dim 4.6256(5.0997) | Steps 107(102.79) | Grad Norm 5.2312(6.7278) | Total Time 1.00(1.00)
Iter 0400 | Time 0.3203(0.3138) | Bit/dim 4.4689(4.9248) | Steps 107(103.89) | Grad Norm 5.5981(6.3418) | Total Time 1.00(1.00)
Iter 0410 | Time 0.3738(0.3187) | Bit/dim 4.2589(4.7686) | Steps 119(105.24) | Grad Norm 6.1527(6.1431) | Total Time 1.00(1.00)
Iter 0420 | Time 0.3631(0.3255) | Bit/dim 4.0657(4.6121) | Steps 119(107.31) | Grad Norm 6.6289(6.0735) | Total Time 1.00(1.00)
Iter 0430 | Time 0.3388(0.3301) | Bit/dim 4.0090(4.4622) | Steps 113(108.80) | Grad Norm 4.8320(5.9685) | Total Time 1.00(1.00)
Iter 0440 | Time 0.3634(0.3364) | Bit/dim 3.8203(4.3289) | Steps 119(110.24) | Grad Norm 5.1914(5.7824) | Total Time 1.00(1.00)
Iter 0450 | Time 0.3392(0.3384) | Bit/dim 3.9038(4.2106) | Steps 113(111.28) | Grad Norm 7.3732(5.7675) | Total Time 1.00(1.00)
Iter 0460 | Time 0.3391(0.3403) | Bit/dim 3.6400(4.0831) | Steps 113(112.08) | Grad Norm 4.8391(5.7702) | Total Time 1.00(1.00)
Iter 0470 | Time 0.3394(0.3412) | Bit/dim 3.7318(3.9739) | Steps 113(112.61) | Grad Norm 4.6540(5.6628) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3391(0.3415) | Bit/dim 3.5676(3.8701) | Steps 113(112.85) | Grad Norm 4.1524(5.4270) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3392(0.3411) | Bit/dim 3.4829(3.7712) | Steps 113(112.89) | Grad Norm 4.1527(5.2926) | Total Time 1.00(1.00)
Iter 0500 | Time 0.3391(0.3412) | Bit/dim 3.4233(3.7017) | Steps 113(113.06) | Grad Norm 6.8891(5.3661) | Total Time 1.00(1.00)
Iter 0510 | Time 0.3519(0.3421) | Bit/dim 3.3452(3.6200) | Steps 113(113.04) | Grad Norm 6.3144(5.2184) | Total Time 1.00(1.00)
Iter 0520 | Time 0.3558(0.3450) | Bit/dim 3.3014(3.5452) | Steps 113(113.03) | Grad Norm 3.6342(5.0400) | Total Time 1.00(1.00)
Iter 0530 | Time 0.3425(0.3446) | Bit/dim 3.2048(3.4786) | Steps 113(113.02) | Grad Norm 4.3767(4.7551) | Total Time 1.00(1.00)
Iter 0540 | Time 0.3426(0.3443) | Bit/dim 3.2189(3.4205) | Steps 113(113.02) | Grad Norm 4.7170(4.7282) | Total Time 1.00(1.00)
Iter 0550 | Time 0.3547(0.3447) | Bit/dim 3.2262(3.3630) | Steps 113(113.01) | Grad Norm 6.1768(5.0613) | Total Time 1.00(1.00)
Iter 0560 | Time 0.3439(0.3448) | Bit/dim 3.1240(3.3082) | Steps 113(113.01) | Grad Norm 4.2296(4.8332) | Total Time 1.00(1.00)
Iter 0570 | Time 0.3492(0.3451) | Bit/dim 3.1141(3.2605) | Steps 113(113.01) | Grad Norm 4.0545(4.4384) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3449(0.3478) | Bit/dim 3.0172(3.2183) | Steps 113(113.01) | Grad Norm 2.1951(4.0154) | Total Time 1.00(1.00)
Iter 0590 | Time 0.3534(0.3473) | Bit/dim 3.1266(3.1748) | Steps 113(112.70) | Grad Norm 2.7456(3.8398) | Total Time 1.00(1.00)
validating...
Epoch 0000 | Time 4.5922, Bit/dim 2.9802
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Epoch 0
Iter 0000 | Time 0.3027(0.3027) | Bit/dim 30.4177(30.4177) | Steps 41(41.00) | Grad Norm 16.9325(16.9325) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1207(0.2555) | Bit/dim 30.4005(30.4270) | Steps 41(41.00) | Grad Norm 16.7206(16.9236) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1206(0.2201) | Bit/dim 30.6013(30.4309) | Steps 41(41.00) | Grad Norm 17.0390(16.9070) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1207(0.1941) | Bit/dim 30.1511(30.4004) | Steps 41(41.00) | Grad Norm 16.5890(16.8700) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1217(0.1749) | Bit/dim 30.1350(30.3365) | Steps 41(41.00) | Grad Norm 16.6059(16.7957) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1207(0.1607) | Bit/dim 30.0520(30.2412) | Steps 41(41.00) | Grad Norm 16.7070(16.7159) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1207(0.1503) | Bit/dim 29.7667(30.1252) | Steps 41(41.00) | Grad Norm 16.5646(16.6724) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1206(0.1426) | Bit/dim 29.7108(29.9707) | Steps 41(41.00) | Grad Norm 16.8685(16.6474) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1207(0.1370) | Bit/dim 29.0867(29.7665) | Steps 41(41.00) | Grad Norm 17.2685(16.7366) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1217(0.1328) | Bit/dim 28.5123(29.5002) | Steps 41(41.00) | Grad Norm 18.6326(17.0656) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1205(0.1297) | Bit/dim 27.7540(29.1276) | Steps 41(41.00) | Grad Norm 21.5943(17.8567) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1206(0.1274) | Bit/dim 26.6451(28.5982) | Steps 41(41.00) | Grad Norm 27.9799(19.7736) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1207(0.1256) | Bit/dim 24.9070(27.8115) | Steps 41(41.00) | Grad Norm 40.6743(23.7049) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1206(0.1243) | Bit/dim 22.9126(26.7264) | Steps 41(41.00) | Grad Norm 49.7148(29.7331) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1206(0.1234) | Bit/dim 20.6967(25.3630) | Steps 41(41.00) | Grad Norm 56.8477(36.5042) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1446(0.1267) | Bit/dim 18.7657(23.8642) | Steps 47(42.00) | Grad Norm 57.3985(42.0332) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1451(0.1314) | Bit/dim 16.6824(22.2691) | Steps 47(43.31) | Grad Norm 58.5399(45.9249) | Total Time 1.00(1.00)
Iter 0170 | Time 0.2040(0.1368) | Bit/dim 15.1050(20.6690) | Steps 65(44.82) | Grad Norm 56.1209(48.3508) | Total Time 1.00(1.00)
Iter 0180 | Time 0.2053(0.1545) | Bit/dim 13.9638(19.1103) | Steps 65(50.12) | Grad Norm 50.2776(49.2889) | Total Time 1.00(1.00)
Iter 0190 | Time 0.2050(0.1675) | Bit/dim 12.7472(17.6176) | Steps 65(54.03) | Grad Norm 45.3713(48.7098) | Total Time 1.00(1.00)
Iter 0200 | Time 0.2041(0.1765) | Bit/dim 11.8755(16.2257) | Steps 65(56.73) | Grad Norm 39.8983(46.9571) | Total Time 1.00(1.00)
Iter 0210 | Time 0.2144(0.1843) | Bit/dim 11.1192(14.9242) | Steps 71(59.26) | Grad Norm 34.9369(44.4890) | Total Time 1.00(1.00)
Iter 0220 | Time 0.2134(0.1919) | Bit/dim 9.8481(13.7173) | Steps 71(62.34) | Grad Norm 31.8490(41.5091) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2396(0.2038) | Bit/dim 9.2697(12.6160) | Steps 77(66.05) | Grad Norm 27.2913(38.1970) | Total Time 1.00(1.00)
Iter 0240 | Time 0.2398(0.2132) | Bit/dim 8.4005(11.6026) | Steps 77(68.93) | Grad Norm 23.5295(34.7277) | Total Time 1.00(1.00)
Iter 0250 | Time 0.2479(0.2209) | Bit/dim 7.6728(10.6707) | Steps 83(71.23) | Grad Norm 20.7459(31.2950) | Total Time 1.00(1.00)
Iter 0260 | Time 0.2477(0.2280) | Bit/dim 7.4041(9.8281) | Steps 83(74.32) | Grad Norm 16.9919(27.9408) | Total Time 1.00(1.00)
Iter 0270 | Time 0.2476(0.2332) | Bit/dim 6.6734(9.0717) | Steps 83(76.60) | Grad Norm 14.6574(24.7204) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3074(0.2404) | Bit/dim 6.2068(8.3879) | Steps 95(78.99) | Grad Norm 12.5703(21.7471) | Total Time 1.00(1.00)
Iter 0290 | Time 0.3041(0.2582) | Bit/dim 5.6778(7.7403) | Steps 101(84.34) | Grad Norm 10.3945(18.9738) | Total Time 1.00(1.00)
Iter 0300 | Time 0.3042(0.2704) | Bit/dim 5.3641(7.1685) | Steps 101(88.72) | Grad Norm 8.6581(16.4127) | Total Time 1.00(1.00)
Iter 0310 | Time 0.3039(0.2796) | Bit/dim 5.0021(6.6639) | Steps 101(91.94) | Grad Norm 7.4213(14.1106) | Total Time 1.00(1.00)
Iter 0320 | Time 0.3040(0.2861) | Bit/dim 4.9487(6.2351) | Steps 101(94.32) | Grad Norm 5.9530(12.1111) | Total Time 1.00(1.00)
Iter 0330 | Time 0.3055(0.2911) | Bit/dim 4.8392(5.8506) | Steps 101(96.07) | Grad Norm 5.0389(10.4058) | Total Time 1.00(1.00)
Iter 0340 | Time 0.3143(0.2968) | Bit/dim 4.4017(5.5000) | Steps 107(98.52) | Grad Norm 4.7508(8.9730) | Total Time 1.00(1.00)
Iter 0350 | Time 0.3142(0.3014) | Bit/dim 4.2727(5.1862) | Steps 107(100.75) | Grad Norm 4.2268(7.7342) | Total Time 1.00(1.00)
Iter 0360 | Time 0.3652(0.3107) | Bit/dim 4.1473(4.9237) | Steps 119(103.77) | Grad Norm 3.8579(6.7115) | Total Time 1.00(1.00)
Iter 0370 | Time 0.3654(0.3250) | Bit/dim 4.0462(4.7030) | Steps 119(107.77) | Grad Norm 3.3404(5.8848) | Total Time 1.00(1.00)
Iter 0380 | Time 0.3650(0.3355) | Bit/dim 3.7984(4.5034) | Steps 119(110.72) | Grad Norm 3.4078(5.2331) | Total Time 1.00(1.00)
Iter 0390 | Time 0.3652(0.3433) | Bit/dim 3.8796(4.3073) | Steps 119(112.89) | Grad Norm 3.4303(4.7102) | Total Time 1.00(1.00)
Iter 0400 | Time 0.3654(0.3491) | Bit/dim 3.6135(4.1251) | Steps 119(114.49) | Grad Norm 3.6707(4.3840) | Total Time 1.00(1.00)
Iter 0410 | Time 0.3654(0.3533) | Bit/dim 3.4213(3.9603) | Steps 119(115.68) | Grad Norm 4.7955(4.3022) | Total Time 1.00(1.00)
Iter 0420 | Time 0.3653(0.3565) | Bit/dim 3.3112(3.8101) | Steps 119(116.55) | Grad Norm 3.8369(4.1774) | Total Time 1.00(1.00)
Iter 0430 | Time 0.3652(0.3593) | Bit/dim 3.1867(3.6617) | Steps 119(117.53) | Grad Norm 2.5496(3.9471) | Total Time 1.00(1.00)
Iter 0440 | Time 0.3699(0.3627) | Bit/dim 3.0762(3.5428) | Steps 125(119.49) | Grad Norm 4.0062(3.7666) | Total Time 1.00(1.00)
Iter 0450 | Time 0.3701(0.3647) | Bit/dim 3.1009(3.4369) | Steps 125(120.94) | Grad Norm 3.0962(3.5858) | Total Time 1.00(1.00)
Iter 0460 | Time 0.3702(0.3662) | Bit/dim 2.9397(3.3305) | Steps 125(122.01) | Grad Norm 2.3284(3.3515) | Total Time 1.00(1.00)
Iter 0470 | Time 0.3702(0.3673) | Bit/dim 2.9783(3.2458) | Steps 125(122.79) | Grad Norm 2.1094(3.1742) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3701(0.3681) | Bit/dim 2.8626(3.1684) | Steps 125(123.37) | Grad Norm 3.0160(3.0741) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3830(0.3725) | Bit/dim 2.9404(3.0995) | Steps 125(123.80) | Grad Norm 3.1130(2.9708) | Total Time 1.00(1.00)
Iter 0500 | Time 0.3829(0.3752) | Bit/dim 2.8877(3.0323) | Steps 125(124.11) | Grad Norm 3.1791(2.9993) | Total Time 1.00(1.00)
Iter 0510 | Time 0.3727(0.3747) | Bit/dim 2.7605(2.9799) | Steps 125(124.35) | Grad Norm 2.9284(2.9874) | Total Time 1.00(1.00)
Iter 0520 | Time 0.3723(0.3741) | Bit/dim 2.7997(2.9269) | Steps 125(124.52) | Grad Norm 2.6728(2.7985) | Total Time 1.00(1.00)
Iter 0530 | Time 0.3747(0.3742) | Bit/dim 2.7875(2.8849) | Steps 125(124.64) | Grad Norm 3.4330(2.7110) | Total Time 1.00(1.00)
Iter 0540 | Time 0.3755(0.3743) | Bit/dim 2.7395(2.8486) | Steps 125(124.74) | Grad Norm 2.4729(2.6938) | Total Time 1.00(1.00)
Iter 0550 | Time 0.3741(0.3745) | Bit/dim 2.7039(2.8001) | Steps 125(124.81) | Grad Norm 1.7382(2.6326) | Total Time 1.00(1.00)
Iter 0560 | Time 0.3742(0.3745) | Bit/dim 2.7343(2.7663) | Steps 125(124.86) | Grad Norm 2.3569(2.5561) | Total Time 1.00(1.00)
Iter 0570 | Time 0.3746(0.3745) | Bit/dim 2.6907(2.7313) | Steps 125(124.89) | Grad Norm 2.2826(2.5052) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3743(0.3745) | Bit/dim 2.5528(2.7005) | Steps 125(124.92) | Grad Norm 3.4861(2.5763) | Total Time 1.00(1.00)
Iter 0590 | Time 0.3748(0.3745) | Bit/dim 2.5499(2.6742) | Steps 125(124.94) | Grad Norm 2.9776(2.7511) | Total Time 1.00(1.00)
validating...
Epoch 0000 | Time 4.7673, Bit/dim 2.5562
Saving model at epoch 0.
Epoch 1
Iter 0600 | Time 0.3899(0.3750) | Bit/dim 2.6231(2.6495) | Steps 125(124.96) | Grad Norm 2.3732(2.7286) | Total Time 1.00(1.00)
Iter 0610 | Time 0.3777(0.3763) | Bit/dim 2.6025(2.6309) | Steps 125(124.97) | Grad Norm 3.3182(2.9567) | Total Time 1.00(1.00)
Iter 0620 | Time 0.3775(0.3762) | Bit/dim 2.5481(2.6094) | Steps 125(124.98) | Grad Norm 1.8586(2.8771) | Total Time 1.00(1.00)
Iter 0630 | Time 0.3732(0.3771) | Bit/dim 2.4941(2.5839) | Steps 125(124.98) | Grad Norm 3.2783(2.8527) | Total Time 1.00(1.00)
Iter 0640 | Time 0.3731(0.3759) | Bit/dim 2.4968(2.5595) | Steps 125(124.82) | Grad Norm 4.8875(3.1749) | Total Time 1.00(1.00)
Iter 0650 | Time 0.3656(0.3738) | Bit/dim 2.5202(2.5426) | Steps 119(123.75) | Grad Norm 2.2055(3.1122) | Total Time 1.00(1.00)
Iter 0660 | Time 0.3653(0.3717) | Bit/dim 2.4688(2.5293) | Steps 119(122.50) | Grad Norm 1.6428(2.9190) | Total Time 1.00(1.00)
Iter 0670 | Time 0.3657(0.3701) | Bit/dim 2.4327(2.5205) | Steps 119(121.58) | Grad Norm 2.8006(2.9433) | Total Time 1.00(1.00)
Iter 0680 | Time 0.3644(0.3683) | Bit/dim 2.4947(2.5096) | Steps 119(120.76) | Grad Norm 2.2796(2.9018) | Total Time 1.00(1.00)
Iter 0690 | Time 0.3656(0.3676) | Bit/dim 2.4472(2.4974) | Steps 119(120.30) | Grad Norm 2.5256(2.8288) | Total Time 1.00(1.00)
Iter 0700 | Time 0.3762(0.3691) | Bit/dim 2.4229(2.4813) | Steps 119(119.79) | Grad Norm 4.6759(2.8419) | Total Time 1.00(1.00)
Iter 0710 | Time 0.3764(0.3703) | Bit/dim 2.3991(2.4680) | Steps 119(119.41) | Grad Norm 4.8070(3.0745) | Total Time 1.00(1.00)
Iter 0720 | Time 0.3774(0.3713) | Bit/dim 2.4258(2.4583) | Steps 119(119.15) | Grad Norm 3.5009(3.0479) | Total Time 1.00(1.00)
Iter 0730 | Time 0.3775(0.3714) | Bit/dim 2.4389(2.4442) | Steps 119(118.78) | Grad Norm 4.1055(3.2734) | Total Time 1.00(1.00)
Iter 0740 | Time 0.3757(0.3707) | Bit/dim 2.3630(2.4352) | Steps 119(118.35) | Grad Norm 4.2344(3.4080) | Total Time 1.00(1.00)
Iter 0750 | Time 0.3549(0.3680) | Bit/dim 2.3843(2.4222) | Steps 113(117.43) | Grad Norm 3.1133(3.5518) | Total Time 1.00(1.00)
Iter 0760 | Time 0.3522(0.3655) | Bit/dim 2.3703(2.4121) | Steps 113(116.57) | Grad Norm 2.9552(3.3411) | Total Time 1.00(1.00)
Iter 0770 | Time 0.3528(0.3629) | Bit/dim 2.4019(2.4111) | Steps 113(115.78) | Grad Norm 2.0516(3.1610) | Total Time 1.00(1.00)
Iter 0780 | Time 0.3541(0.3619) | Bit/dim 2.3352(2.4036) | Steps 113(115.36) | Grad Norm 3.7255(3.1572) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Epoch 0
Iter 0000 | Time 0.3006(0.3006) | Bit/dim 32.4596(32.4596) | Steps 41(41.00) | Grad Norm 19.6926(19.6926) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1209(0.2541) | Bit/dim 32.4033(32.4424) | Steps 41(41.00) | Grad Norm 19.5520(19.6441) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1208(0.2192) | Bit/dim 32.0647(32.4064) | Steps 41(41.00) | Grad Norm 19.0848(19.5855) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1208(0.1934) | Bit/dim 32.2678(32.3437) | Steps 41(41.00) | Grad Norm 19.4871(19.5167) | Total Time 1.00(1.00)
Iter 0040 | Time 0.1223(0.1745) | Bit/dim 31.6851(32.2119) | Steps 41(41.00) | Grad Norm 18.9050(19.3844) | Total Time 1.00(1.00)
Iter 0050 | Time 0.1208(0.1605) | Bit/dim 31.2150(32.0288) | Steps 41(41.00) | Grad Norm 18.4458(19.2540) | Total Time 1.00(1.00)
Iter 0060 | Time 0.1209(0.1502) | Bit/dim 30.8902(31.7828) | Steps 41(41.00) | Grad Norm 18.5739(19.1022) | Total Time 1.00(1.00)
Iter 0070 | Time 0.1208(0.1425) | Bit/dim 30.4803(31.4904) | Steps 41(41.00) | Grad Norm 18.9131(19.0086) | Total Time 1.00(1.00)
Iter 0080 | Time 0.1208(0.1369) | Bit/dim 29.6809(31.1002) | Steps 41(41.00) | Grad Norm 19.0699(18.9983) | Total Time 1.00(1.00)
Iter 0090 | Time 0.1226(0.1328) | Bit/dim 28.8588(30.5984) | Steps 41(41.00) | Grad Norm 20.0100(19.1369) | Total Time 1.00(1.00)
Iter 0100 | Time 0.1209(0.1298) | Bit/dim 27.4559(29.9357) | Steps 41(41.00) | Grad Norm 21.1555(19.5303) | Total Time 1.00(1.00)
Iter 0110 | Time 0.1208(0.1275) | Bit/dim 26.0502(29.1039) | Steps 41(41.00) | Grad Norm 24.3060(20.3855) | Total Time 1.00(1.00)
Iter 0120 | Time 0.1208(0.1257) | Bit/dim 24.7327(28.1176) | Steps 41(41.00) | Grad Norm 26.6890(21.7366) | Total Time 1.00(1.00)
Iter 0130 | Time 0.1209(0.1245) | Bit/dim 23.5679(27.0601) | Steps 41(41.00) | Grad Norm 30.3857(23.5784) | Total Time 1.00(1.00)
Iter 0140 | Time 0.1209(0.1236) | Bit/dim 22.5319(25.9693) | Steps 41(41.00) | Grad Norm 33.3190(25.8351) | Total Time 1.00(1.00)
Iter 0150 | Time 0.1208(0.1229) | Bit/dim 21.2599(24.8781) | Steps 41(41.00) | Grad Norm 35.6594(28.0771) | Total Time 1.00(1.00)
Iter 0160 | Time 0.1210(0.1224) | Bit/dim 20.1071(23.7462) | Steps 41(41.00) | Grad Norm 36.6553(30.2438) | Total Time 1.00(1.00)
Iter 0170 | Time 0.1209(0.1220) | Bit/dim 18.5597(22.5569) | Steps 41(41.00) | Grad Norm 40.4186(32.4796) | Total Time 1.00(1.00)
Iter 0180 | Time 0.1285(0.1219) | Bit/dim 16.8587(21.2874) | Steps 47(41.18) | Grad Norm 43.3222(34.8066) | Total Time 1.00(1.00)
Iter 0190 | Time 0.1208(0.1222) | Bit/dim 15.3652(19.9347) | Steps 41(41.58) | Grad Norm 42.9222(36.8202) | Total Time 1.00(1.00)
Iter 0200 | Time 0.1798(0.1322) | Bit/dim 13.7590(18.5148) | Steps 59(44.71) | Grad Norm 41.9705(38.1713) | Total Time 1.00(1.00)
Iter 0210 | Time 0.1805(0.1448) | Bit/dim 12.0654(17.0370) | Steps 59(48.46) | Grad Norm 39.7010(38.8391) | Total Time 1.00(1.00)
Iter 0220 | Time 0.1890(0.1549) | Bit/dim 10.8884(15.5879) | Steps 65(51.75) | Grad Norm 34.3370(38.1531) | Total Time 1.00(1.00)
Iter 0230 | Time 0.2479(0.1739) | Bit/dim 9.9165(14.1649) | Steps 77(57.54) | Grad Norm 28.3007(36.3867) | Total Time 1.00(1.00)
Iter 0240 | Time 0.2510(0.1935) | Bit/dim 9.0373(12.8571) | Steps 83(63.17) | Grad Norm 22.2961(33.4597) | Total Time 1.00(1.00)
Iter 0250 | Time 0.2230(0.2080) | Bit/dim 8.0277(11.6891) | Steps 77(68.02) | Grad Norm 16.5848(29.6955) | Total Time 1.00(1.00)
Iter 0260 | Time 0.2347(0.2214) | Bit/dim 7.6769(10.6780) | Steps 83(73.01) | Grad Norm 12.2827(25.6057) | Total Time 1.00(1.00)
Iter 0270 | Time 0.3046(0.2435) | Bit/dim 7.1432(9.7989) | Steps 101(80.36) | Grad Norm 9.4208(21.6524) | Total Time 1.00(1.00)
Iter 0280 | Time 0.3045(0.2596) | Bit/dim 6.9820(9.0780) | Steps 101(85.78) | Grad Norm 7.9922(18.2269) | Total Time 1.00(1.00)
Iter 0290 | Time 0.3045(0.2715) | Bit/dim 6.5964(8.4390) | Steps 101(89.78) | Grad Norm 7.5802(15.4523) | Total Time 1.00(1.00)
Iter 0300 | Time 0.3047(0.2797) | Bit/dim 6.3839(7.9087) | Steps 101(92.57) | Grad Norm 7.1522(13.2746) | Total Time 1.00(1.00)
Iter 0310 | Time 0.3048(0.2849) | Bit/dim 5.9124(7.4438) | Steps 101(94.45) | Grad Norm 6.6975(11.5905) | Total Time 1.00(1.00)
Iter 0320 | Time 0.2804(0.2894) | Bit/dim 5.8257(7.0572) | Steps 95(95.99) | Grad Norm 6.6204(10.3172) | Total Time 1.00(1.00)
Iter 0330 | Time 0.2805(0.2884) | Bit/dim 5.6199(6.6874) | Steps 95(96.03) | Grad Norm 6.3525(9.2631) | Total Time 1.00(1.00)
Iter 0340 | Time 0.2805(0.2863) | Bit/dim 5.3556(6.3654) | Steps 95(95.76) | Grad Norm 5.7069(8.3853) | Total Time 1.00(1.00)
Iter 0350 | Time 0.2805(0.2848) | Bit/dim 5.2101(6.0725) | Steps 95(95.56) | Grad Norm 5.4928(7.6634) | Total Time 1.00(1.00)
Iter 0360 | Time 0.2803(0.2837) | Bit/dim 5.0387(5.8116) | Steps 95(95.41) | Grad Norm 5.4432(7.0822) | Total Time 1.00(1.00)
Iter 0370 | Time 0.2806(0.2829) | Bit/dim 4.9636(5.5892) | Steps 95(95.30) | Grad Norm 5.1759(6.6068) | Total Time 1.00(1.00)
Iter 0380 | Time 0.2808(0.2823) | Bit/dim 4.6697(5.3698) | Steps 95(95.22) | Grad Norm 4.9904(6.2149) | Total Time 1.00(1.00)
Iter 0390 | Time 0.2807(0.2821) | Bit/dim 4.5499(5.1642) | Steps 95(95.17) | Grad Norm 5.2171(5.9036) | Total Time 1.00(1.00)
Iter 0400 | Time 0.2662(0.2791) | Bit/dim 4.3237(4.9696) | Steps 95(94.99) | Grad Norm 5.4530(5.7751) | Total Time 1.00(1.00)
Iter 0410 | Time 0.3156(0.2825) | Bit/dim 4.1365(4.7725) | Steps 107(96.74) | Grad Norm 4.9494(5.6294) | Total Time 1.00(1.00)
Iter 0420 | Time 0.3174(0.2910) | Bit/dim 3.9681(4.5688) | Steps 107(99.30) | Grad Norm 5.0672(5.4455) | Total Time 1.00(1.00)
Iter 0430 | Time 0.3172(0.2979) | Bit/dim 3.9552(4.4005) | Steps 107(101.32) | Grad Norm 4.8246(5.2643) | Total Time 1.00(1.00)
Iter 0440 | Time 0.3171(0.3030) | Bit/dim 3.6901(4.2325) | Steps 107(102.81) | Grad Norm 4.2082(5.0466) | Total Time 1.00(1.00)
Iter 0450 | Time 0.3174(0.3069) | Bit/dim 3.6618(4.0878) | Steps 107(103.91) | Grad Norm 4.2427(4.8624) | Total Time 1.00(1.00)
Iter 0460 | Time 0.2664(0.3011) | Bit/dim 3.4812(3.9452) | Steps 95(102.72) | Grad Norm 4.5250(4.7044) | Total Time 1.00(1.00)
Iter 0470 | Time 0.3171(0.2990) | Bit/dim 3.4207(3.8261) | Steps 107(102.34) | Grad Norm 4.1293(4.6021) | Total Time 1.00(1.00)
Iter 0480 | Time 0.3342(0.3048) | Bit/dim 3.3043(3.7147) | Steps 107(103.56) | Grad Norm 3.6678(4.4104) | Total Time 1.00(1.00)
Iter 0490 | Time 0.3212(0.3122) | Bit/dim 3.1989(3.6059) | Steps 107(104.46) | Grad Norm 3.2774(4.2409) | Total Time 1.00(1.00)
Iter 0500 | Time 0.3420(0.3168) | Bit/dim 3.1033(3.5145) | Steps 107(105.13) | Grad Norm 3.4778(4.1268) | Total Time 1.00(1.00)
Iter 0510 | Time 0.3172(0.3189) | Bit/dim 3.1314(3.4254) | Steps 107(105.62) | Grad Norm 3.6603(4.0096) | Total Time 1.00(1.00)
Iter 0520 | Time 0.3369(0.3277) | Bit/dim 3.0729(3.3498) | Steps 107(105.98) | Grad Norm 3.8560(3.8834) | Total Time 1.00(1.00)
Iter 0530 | Time 0.3342(0.3290) | Bit/dim 3.0614(3.2763) | Steps 107(106.25) | Grad Norm 3.2298(3.7369) | Total Time 1.00(1.00)
Iter 0540 | Time 0.3469(0.3284) | Bit/dim 3.0334(3.2094) | Steps 107(106.45) | Grad Norm 3.3010(3.5892) | Total Time 1.00(1.00)
Iter 0550 | Time 0.3346(0.3266) | Bit/dim 2.9348(3.1352) | Steps 107(106.11) | Grad Norm 3.2757(3.5058) | Total Time 1.00(1.00)
Iter 0560 | Time 0.3307(0.3258) | Bit/dim 2.8966(3.0814) | Steps 107(106.17) | Grad Norm 3.0016(3.3565) | Total Time 1.00(1.00)
Iter 0570 | Time 0.2933(0.3236) | Bit/dim 2.8946(3.0314) | Steps 101(106.04) | Grad Norm 2.7870(3.2895) | Total Time 1.00(1.00)
Iter 0580 | Time 0.3221(0.3208) | Bit/dim 2.8382(2.9825) | Steps 107(105.81) | Grad Norm 2.9408(3.1659) | Total Time 1.00(1.00)
Iter 0590 | Time 0.2953(0.3158) | Bit/dim 2.7604(2.9339) | Steps 101(105.01) | Grad Norm 2.5365(2.9798) | Total Time 1.00(1.00)
validating...
Epoch 0000 | Time 4.6300, Bit/dim 2.7239
Saving model at epoch 0.
Epoch 1
Iter 0600 | Time 0.3088(0.3124) | Bit/dim 2.7521(2.8914) | Steps 101(104.10) | Grad Norm 3.2033(2.8698) | Total Time 1.00(1.00)
Iter 0610 | Time 0.3091(0.3116) | Bit/dim 2.7409(2.8535) | Steps 101(103.43) | Grad Norm 2.8471(2.8595) | Total Time 1.00(1.00)
Iter 0620 | Time 0.3053(0.3106) | Bit/dim 2.6704(2.8141) | Steps 101(102.79) | Grad Norm 2.1151(2.7740) | Total Time 1.00(1.00)
Iter 0630 | Time 0.2967(0.3095) | Bit/dim 2.6220(2.7750) | Steps 101(102.32) | Grad Norm 3.6313(2.8114) | Total Time 1.00(1.00)
Iter 0640 | Time 0.3039(0.3087) | Bit/dim 2.6317(2.7335) | Steps 101(101.97) | Grad Norm 3.5048(2.9808) | Total Time 1.00(1.00)
Iter 0650 | Time 0.2931(0.3058) | Bit/dim 2.5812(2.6967) | Steps 101(101.72) | Grad Norm 2.2152(3.2158) | Total Time 1.00(1.00)
Iter 0660 | Time 0.2934(0.3062) | Bit/dim 2.6017(2.6695) | Steps 101(101.53) | Grad Norm 5.0823(3.2290) | Total Time 1.00(1.00)
Iter 0670 | Time 0.2979(0.3069) | Bit/dim 2.5958(2.6407) | Steps 101(101.39) | Grad Norm 2.5929(3.2132) | Total Time 1.00(1.00)
Iter 0680 | Time 0.2914(0.3038) | Bit/dim 2.5522(2.6140) | Steps 101(101.29) | Grad Norm 2.5960(2.9791) | Total Time 1.00(1.00)
Iter 0690 | Time 0.3025(0.3036) | Bit/dim 2.5218(2.5937) | Steps 101(101.21) | Grad Norm 2.5741(2.7664) | Total Time 1.00(1.00)
Iter 0700 | Time 0.3145(0.3038) | Bit/dim 2.5237(2.5670) | Steps 101(101.16) | Grad Norm 3.4784(2.7984) | Total Time 1.00(1.00)
Iter 0710 | Time 0.3079(0.3046) | Bit/dim 2.5241(2.5551) | Steps 101(101.12) | Grad Norm 3.3468(3.1499) | Total Time 1.00(1.00)
Iter 0720 | Time 0.3030(0.3042) | Bit/dim 2.4234(2.5373) | Steps 101(101.09) | Grad Norm 3.6074(3.1496) | Total Time 1.00(1.00)
Iter 0730 | Time 0.3034(0.3036) | Bit/dim 2.4728(2.5205) | Steps 101(101.06) | Grad Norm 2.0204(3.1376) | Total Time 1.00(1.00)
Iter 0740 | Time 0.2923(0.3010) | Bit/dim 2.4442(2.5019) | Steps 101(101.05) | Grad Norm 3.1921(3.3023) | Total Time 1.00(1.00)
Iter 0750 | Time 0.2917(0.2988) | Bit/dim 2.3884(2.4792) | Steps 101(101.03) | Grad Norm 4.3164(3.3954) | Total Time 1.00(1.00)
Iter 0760 | Time 0.2916(0.2971) | Bit/dim 2.3627(2.4564) | Steps 101(101.03) | Grad Norm 2.4440(3.4135) | Total Time 1.00(1.00)
Iter 0770 | Time 0.3074(0.2964) | Bit/dim 2.3843(2.4414) | Steps 101(101.02) | Grad Norm 2.5357(3.4547) | Total Time 1.00(1.00)
Iter 0780 | Time 0.3049(0.3012) | Bit/dim 2.3957(2.4274) | Steps 101(101.01) | Grad Norm 1.6135(3.2788) | Total Time 1.00(1.00)
Iter 0790 | Time 0.2952(0.3025) | Bit/dim 2.3158(2.4124) | Steps 101(101.01) | Grad Norm 2.3051(3.0092) | Total Time 1.00(1.00)
Iter 0800 | Time 0.3060(0.3017) | Bit/dim 2.3695(2.4030) | Steps 101(101.01) | Grad Norm 5.1021(3.2403) | Total Time 1.00(1.00)
Iter 0810 | Time 0.2920(0.2997) | Bit/dim 2.3247(2.3861) | Steps 101(101.01) | Grad Norm 4.5609(3.1726) | Total Time 1.00(1.00)
Iter 0820 | Time 0.2922(0.2984) | Bit/dim 2.3376(2.3754) | Steps 101(101.00) | Grad Norm 5.1928(3.3745) | Total Time 1.00(1.00)
Iter 0830 | Time 0.2917(0.2969) | Bit/dim 2.3449(2.3643) | Steps 101(101.00) | Grad Norm 2.3984(3.3170) | Total Time 1.00(1.00)
Iter 0840 | Time 0.2913(0.2957) | Bit/dim 2.3069(2.3559) | Steps 101(101.00) | Grad Norm 5.5515(3.9331) | Total Time 1.00(1.00)
Iter 0850 | Time 0.2913(0.2948) | Bit/dim 2.3296(2.3482) | Steps 101(101.00) | Grad Norm 6.8302(4.2882) | Total Time 1.00(1.00)
Iter 0860 | Time 0.2915(0.2946) | Bit/dim 2.3273(2.3325) | Steps 101(101.00) | Grad Norm 3.2820(4.2755) | Total Time 1.00(1.00)
Iter 0870 | Time 0.3035(0.2949) | Bit/dim 2.3120(2.3193) | Steps 101(101.00) | Grad Norm 2.1788(4.2331) | Total Time 1.00(1.00)
Iter 0880 | Time 0.3029(0.2970) | Bit/dim 2.2819(2.3061) | Steps 101(101.00) | Grad Norm 1.4295(3.8973) | Total Time 1.00(1.00)
Iter 0890 | Time 0.2919(0.2980) | Bit/dim 2.2121(2.2950) | Steps 101(101.00) | Grad Norm 4.9128(4.1284) | Total Time 1.00(1.00)
Iter 0900 | Time 0.2927(0.2966) | Bit/dim 2.2616(2.2878) | Steps 101(101.00) | Grad Norm 4.4203(4.3480) | Total Time 1.00(1.00)
Iter 0910 | Time 0.2930(0.2957) | Bit/dim 2.2404(2.2722) | Steps 101(101.00) | Grad Norm 2.8853(4.3306) | Total Time 1.00(1.00)
Iter 0920 | Time 0.2928(0.2950) | Bit/dim 2.2859(2.2653) | Steps 101(101.00) | Grad Norm 4.4672(3.9074) | Total Time 1.00(1.00)
Iter 0930 | Time 0.2933(0.2957) | Bit/dim 2.2297(2.2597) | Steps 101(101.30) | Grad Norm 4.2331(3.6874) | Total Time 1.00(1.00)
Iter 0940 | Time 0.2928(0.2958) | Bit/dim 2.1972(2.2515) | Steps 101(101.40) | Grad Norm 3.7638(3.6155) | Total Time 1.00(1.00)
Iter 0950 | Time 0.2927(0.2950) | Bit/dim 2.2183(2.2449) | Steps 101(101.29) | Grad Norm 4.9701(3.6799) | Total Time 1.00(1.00)
Iter 0960 | Time 0.2931(0.2945) | Bit/dim 2.2175(2.2378) | Steps 101(101.22) | Grad Norm 6.5202(3.7533) | Total Time 1.00(1.00)
Iter 0970 | Time 0.2929(0.2941) | Bit/dim 2.2387(2.2324) | Steps 101(101.16) | Grad Norm 2.4939(4.2254) | Total Time 1.00(1.00)
Iter 0980 | Time 0.2926(0.2938) | Bit/dim 2.1878(2.2259) | Steps 101(101.12) | Grad Norm 2.8489(3.8135) | Total Time 1.00(1.00)
Iter 0990 | Time 0.2928(0.2936) | Bit/dim 2.1922(2.2216) | Steps 101(101.09) | Grad Norm 2.3428(3.4477) | Total Time 1.00(1.00)
Iter 1000 | Time 0.2926(0.2934) | Bit/dim 2.1829(2.2218) | Steps 101(101.06) | Grad Norm 4.1327(3.2853) | Total Time 1.00(1.00)
Iter 1010 | Time 0.2930(0.2935) | Bit/dim 2.1923(2.2179) | Steps 101(101.05) | Grad Norm 2.8263(3.4984) | Total Time 1.00(1.00)
Iter 1020 | Time 0.2927(0.2935) | Bit/dim 2.1619(2.2082) | Steps 101(101.03) | Grad Norm 1.9224(3.2263) | Total Time 1.00(1.00)
Iter 1030 | Time 0.2926(0.2935) | Bit/dim 2.2281(2.1986) | Steps 101(101.18) | Grad Norm 1.5672(2.9817) | Total Time 1.00(1.00)
Iter 1040 | Time 0.2927(0.2934) | Bit/dim 2.1529(2.1914) | Steps 101(101.13) | Grad Norm 5.2384(3.1613) | Total Time 1.00(1.00)
Iter 1050 | Time 0.2929(0.2933) | Bit/dim 2.2320(2.1917) | Steps 101(101.10) | Grad Norm 6.6431(3.7680) | Total Time 1.00(1.00)
Iter 1060 | Time 0.2928(0.2934) | Bit/dim 2.1914(2.1856) | Steps 101(101.23) | Grad Norm 3.3536(3.9091) | Total Time 1.00(1.00)
Iter 1070 | Time 0.2998(0.2940) | Bit/dim 2.1971(2.1787) | Steps 107(101.84) | Grad Norm 2.3649(3.6453) | Total Time 1.00(1.00)
Iter 1080 | Time 0.3051(0.2952) | Bit/dim 2.1167(2.1710) | Steps 107(102.58) | Grad Norm 3.7539(3.4494) | Total Time 1.00(1.00)
Iter 1090 | Time 0.3019(0.2971) | Bit/dim 2.2027(2.1663) | Steps 107(103.74) | Grad Norm 4.4230(3.5343) | Total Time 1.00(1.00)
Iter 1100 | Time 0.3021(0.2984) | Bit/dim 2.1352(2.1610) | Steps 107(104.60) | Grad Norm 2.8773(3.2712) | Total Time 1.00(1.00)
Iter 1110 | Time 0.3171(0.3006) | Bit/dim 2.1816(2.1582) | Steps 107(105.23) | Grad Norm 2.7761(3.1533) | Total Time 1.00(1.00)
Iter 1120 | Time 0.2996(0.3015) | Bit/dim 2.1284(2.1491) | Steps 107(105.69) | Grad Norm 1.4697(2.9542) | Total Time 1.00(1.00)
Iter 1130 | Time 0.2994(0.3010) | Bit/dim 2.1309(2.1432) | Steps 107(106.04) | Grad Norm 3.4934(2.8566) | Total Time 1.00(1.00)
Iter 1140 | Time 0.2997(0.3007) | Bit/dim 2.1081(2.1346) | Steps 107(106.29) | Grad Norm 3.5789(2.8280) | Total Time 1.00(1.00)
Iter 1150 | Time 0.2995(0.3005) | Bit/dim 2.1819(2.1376) | Steps 107(106.48) | Grad Norm 6.1886(2.9532) | Total Time 1.00(1.00)
Iter 1160 | Time 0.2996(0.3003) | Bit/dim 2.1151(2.1309) | Steps 107(106.61) | Grad Norm 2.5776(3.2572) | Total Time 1.00(1.00)
Iter 1170 | Time 0.3139(0.3012) | Bit/dim 2.0927(2.1318) | Steps 107(106.71) | Grad Norm 2.6953(3.1748) | Total Time 1.00(1.00)
Iter 1180 | Time 0.2996(0.3036) | Bit/dim 2.1649(2.1332) | Steps 107(106.79) | Grad Norm 2.9121(3.3087) | Total Time 1.00(1.00)
Iter 1190 | Time 0.2996(0.3027) | Bit/dim 2.1230(2.1288) | Steps 107(106.84) | Grad Norm 5.4391(3.6547) | Total Time 1.00(1.00)
validating...
Epoch 0001 | Time 4.6032, Bit/dim 2.1089
Saving model at epoch 1.
Epoch 2
Iter 1200 | Time 0.3061(0.3021) | Bit/dim 2.1386(2.1286) | Steps 107(106.89) | Grad Norm 5.2583(3.9566) | Total Time 1.00(1.00)
Iter 1210 | Time 0.3035(0.3032) | Bit/dim 2.1320(2.1257) | Steps 107(106.92) | Grad Norm 2.2972(3.8291) | Total Time 1.00(1.00)
Iter 1220 | Time 0.3035(0.3033) | Bit/dim 2.1413(2.1256) | Steps 107(106.94) | Grad Norm 1.7191(3.4912) | Total Time 1.00(1.00)
Iter 1230 | Time 0.3039(0.3035) | Bit/dim 2.0810(2.1197) | Steps 107(106.95) | Grad Norm 4.3374(3.3654) | Total Time 1.00(1.00)
Iter 1240 | Time 0.3036(0.3035) | Bit/dim 2.1691(2.1185) | Steps 107(106.97) | Grad Norm 2.6272(3.3222) | Total Time 1.00(1.00)
Iter 1250 | Time 0.3035(0.3042) | Bit/dim 2.1023(2.1113) | Steps 107(107.12) | Grad Norm 2.4526(3.2249) | Total Time 1.00(1.00)
Iter 1260 | Time 0.3033(0.3041) | Bit/dim 2.0709(2.1064) | Steps 107(107.09) | Grad Norm 1.3705(2.8856) | Total Time 1.00(1.00)
Iter 1270 | Time 0.3085(0.3041) | Bit/dim 2.1302(2.1071) | Steps 107(107.06) | Grad Norm 1.1514(2.6438) | Total Time 1.00(1.00)
Iter 1280 | Time 0.3036(0.3053) | Bit/dim 2.0535(2.1007) | Steps 107(107.05) | Grad Norm 2.9049(2.6047) | Total Time 1.00(1.00)
Iter 1290 | Time 0.3033(0.3048) | Bit/dim 2.1070(2.0993) | Steps 107(107.03) | Grad Norm 2.5606(2.5859) | Total Time 1.00(1.00)
Iter 1300 | Time 0.3277(0.3055) | Bit/dim 2.1051(2.1040) | Steps 113(107.21) | Grad Norm 1.9895(2.6533) | Total Time 1.00(1.00)
Iter 1310 | Time 0.3278(0.3063) | Bit/dim 2.1248(2.1051) | Steps 113(107.47) | Grad Norm 3.6562(2.7930) | Total Time 1.00(1.00)
Iter 1320 | Time 0.3107(0.3077) | Bit/dim 2.1377(2.0993) | Steps 107(107.81) | Grad Norm 3.1821(2.7142) | Total Time 1.00(1.00)
Iter 1330 | Time 0.3032(0.3086) | Bit/dim 2.1066(2.0946) | Steps 107(108.05) | Grad Norm 1.9698(2.4935) | Total Time 1.00(1.00)
Iter 1340 | Time 0.3311(0.3118) | Bit/dim 2.1205(2.0936) | Steps 113(108.72) | Grad Norm 1.9966(2.3114) | Total Time 1.00(1.00)
Iter 1350 | Time 0.3032(0.3122) | Bit/dim 2.1369(2.0928) | Steps 107(108.89) | Grad Norm 1.2718(2.0990) | Total Time 1.00(1.00)
Iter 1360 | Time 0.3088(0.3138) | Bit/dim 2.0696(2.0858) | Steps 107(109.22) | Grad Norm 3.2760(2.1839) | Total Time 1.00(1.00)
Iter 1370 | Time 0.3033(0.3155) | Bit/dim 2.0639(2.0819) | Steps 107(109.71) | Grad Norm 2.5498(2.2146) | Total Time 1.00(1.00)
Iter 1380 | Time 0.3277(0.3155) | Bit/dim 2.0545(2.0832) | Steps 113(109.76) | Grad Norm 2.7857(2.1542) | Total Time 1.00(1.00)
Iter 1390 | Time 0.3276(0.3163) | Bit/dim 2.1096(2.0835) | Steps 113(109.99) | Grad Norm 5.5365(2.5156) | Total Time 1.00(1.00)
Iter 1400 | Time 0.3276(0.3188) | Bit/dim 2.0462(2.0754) | Steps 113(110.64) | Grad Norm 2.8511(2.6584) | Total Time 1.00(1.00)
Iter 1410 | Time 0.3033(0.3186) | Bit/dim 2.0123(2.0707) | Steps 107(110.60) | Grad Norm 2.4497(2.6374) | Total Time 1.00(1.00)
Iter 1420 | Time 0.3278(0.3211) | Bit/dim 2.0532(2.0659) | Steps 113(111.23) | Grad Norm 3.2729(2.6792) | Total Time 1.00(1.00)
Iter 1430 | Time 0.3276(0.3223) | Bit/dim 1.9728(2.0627) | Steps 113(111.55) | Grad Norm 2.2546(2.6685) | Total Time 1.00(1.00)
Iter 1440 | Time 0.3279(0.3249) | Bit/dim 2.0831(2.0639) | Steps 113(111.93) | Grad Norm 1.3265(2.6156) | Total Time 1.00(1.00)
Iter 1450 | Time 0.3364(0.3278) | Bit/dim 2.0719(2.0610) | Steps 113(112.21) | Grad Norm 1.7022(2.4640) | Total Time 1.00(1.00)
Iter 1460 | Time 0.3239(0.3289) | Bit/dim 2.0597(2.0571) | Steps 113(112.42) | Grad Norm 1.7525(2.3129) | Total Time 1.00(1.00)
Iter 1470 | Time 0.3235(0.3270) | Bit/dim 2.0142(2.0569) | Steps 113(112.42) | Grad Norm 2.9052(2.4307) | Total Time 1.00(1.00)
Iter 1480 | Time 0.3238(0.3256) | Bit/dim 2.0870(2.0575) | Steps 113(112.42) | Grad Norm 3.3506(2.5954) | Total Time 1.00(1.00)
Iter 1490 | Time 0.3238(0.3252) | Bit/dim 2.0779(2.0574) | Steps 113(112.58) | Grad Norm 2.8252(2.3072) | Total Time 1.00(1.00)
Iter 1500 | Time 0.3239(0.3249) | Bit/dim 2.0413(2.0539) | Steps 113(112.69) | Grad Norm 2.7660(2.3678) | Total Time 1.00(1.00)
Iter 1510 | Time 0.3238(0.3247) | Bit/dim 2.0489(2.0534) | Steps 113(112.77) | Grad Norm 2.2936(2.4744) | Total Time 1.00(1.00)
Iter 1520 | Time 0.3238(0.3248) | Bit/dim 2.0090(2.0507) | Steps 113(112.83) | Grad Norm 2.3580(2.4481) | Total Time 1.00(1.00)
Iter 1530 | Time 0.3237(0.3246) | Bit/dim 2.0018(2.0448) | Steps 113(112.87) | Grad Norm 2.4698(2.4105) | Total Time 1.00(1.00)
Iter 1540 | Time 0.3259(0.3248) | Bit/dim 1.9960(2.0410) | Steps 113(112.91) | Grad Norm 1.6697(2.3235) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Resuming at epoch 2 with args Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64).
Epoch 2
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume='/home/bahr/cdm/experiments/ffjord/checkpt.pth', rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Resuming at epoch 2 with args Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', begin_epoch=1, conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64).
Epoch 2
Iter 0000 | Time 0.4939(0.4939) | Bit/dim 2.1044(2.1044) | Steps 107(107.00) | Grad Norm 4.9434(4.9434) | Total Time 1.00(1.00)
Iter 0010 | Time 0.3025(0.4444) | Bit/dim 2.1550(2.1052) | Steps 107(107.15) | Grad Norm 2.0063(4.6724) | Total Time 1.00(1.00)
Iter 0020 | Time 0.3012(0.4071) | Bit/dim 2.0754(2.1083) | Steps 107(107.11) | Grad Norm 2.8363(4.0316) | Total Time 1.00(1.00)
Iter 0030 | Time 0.3016(0.3795) | Bit/dim 2.1143(2.1088) | Steps 107(107.08) | Grad Norm 1.7597(3.3743) | Total Time 1.00(1.00)
Iter 0040 | Time 0.3169(0.3604) | Bit/dim 2.1047(2.1089) | Steps 107(107.06) | Grad Norm 1.0936(2.7549) | Total Time 1.00(1.00)
Iter 0050 | Time 0.3095(0.3473) | Bit/dim 2.1188(2.1064) | Steps 107(107.04) | Grad Norm 0.9149(2.2556) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=100)
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluation_numerical", type=bool, default=False)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=100, batch_size_schedule='', conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluation_numerical=False, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 100. Total 600 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Epoch 0
Iter 0000 | Time 0.3041(0.3041) | Bit/dim 27.7752(27.7752) | Steps 41(41.00) | Grad Norm 15.8644(15.8644) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1258(0.2577) | Bit/dim 27.6335(27.7603) | Steps 41(41.00) | Grad Norm 15.8847(15.8697) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1261(0.2232) | Bit/dim 27.5649(27.7266) | Steps 41(41.00) | Grad Norm 15.7338(15.8790) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1265(0.1978) | Bit/dim 27.4615(27.6715) | Steps 41(41.00) | Grad Norm 16.0049(15.8746) | Total Time 1.00(1.00)
/home/bahr/cdm/main.py
#!/usr/bin/env python3
import argparse
import os
import time
import numpy as np

import torch
import torchvision.datasets as dset
import torchvision.transforms as tforms

import lib.utils as utils
import lib.dataset as dataset

import models.train_cnf as train_cnf
import models.train_snf as train_snf


# ============================================================================
# Arguments general
# ============================================================================
torch.backends.cudnn.benchmark = True
parser = argparse.ArgumentParser("Continuous Depth Models")

parser.add_argument("--model", choices=["ffjord", "snf"], type=str)
parser.add_argument("--data", choices=["piv","mnist", "cifar10"], type=str, default="mnist")
parser.add_argument("--resume", type=str, default=None)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=128) # Try 32, 64, 128, 256
parser.add_argument("--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated.")
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)

# ============================================================================
# Arguments evaluation
# ============================================================================
parser.add_argument("--evaluate", type=bool, default=True)
parser.add_argument("--resnet_checkpoint", type=str, default=None)
parser.add_argument("--save_recon_images_size", type=int, default=8)
parser.add_argument("--experiment_name", type=str, default=None)
parser.add_argument("--save", type=str, default=None)

parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

# ============================================================================
# Arguments for ffjord
# ============================================================================
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument("--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)")

# ============================================================================
# Arguments for snf
# ============================================================================
parser.add_argument('--z_size', type=int, default=64, metavar='ZSIZE', help='how many stochastic hidden units')
parser.add_argument('--num_flows', type=int, default=4,metavar='NUM_FLOWS', help='Number of flow layers, ignored in absence of flows')
parser.add_argument('--num_householder', type=int, default=8, metavar='NUM_HOUSEHOLDERS',help=""" For Householder Sylvester flow: Number of Householder matrices per flow. Ignored for other flow types.""")


args = parser.parse_args()

if args.model == "ffjord" and args.save == None:
    args.save = "experiments/ffjord"
elif args.model == "snf" and args.save == None:
    args.save = "experiments/snf"

if args.data == "piv" and args.experiment_name == None:
    args.experiment_name = "piv"
elif args.data == "mnist" and args.experiment_name == None:
    args.experiment_name = "mnist"
elif args.data == "cifar10" and args.experiment_name == None:
    args.experiment_name = "cifar10"

if torch.cuda.is_available:
    device_name = torch.cuda.get_device_name(0)
    device = torch.device("cuda:0")
else:
    device_name = None
    device = torch.device("cpu")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info('Running on:' + str(device) + ' ' + str(device_name))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


# ============================================================================
# Data handling
# ============================================================================
def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "piv":
        im_dim = 2
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Training-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
        test_set = dataset.H5Dataset("/home/bahr/cdm/data/ISPIV_dataset/Batch_Validation-Dataset_2Labels_S12_SynthImg_Alex.hdf5")
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)

    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":

    train_set, test_loader, data_shape = get_dataset(args)
    train_loader = get_train_loader(train_set, args.num_epochs)

    if args.model == "ffjord":
        train_cnf.run(args, logger, train_loader, test_loader, data_shape)
    elif args.model == "snf":
        train_snf.run(args, logger, train_loader, test_loader, data_shape)

Running on:cuda:0 TITAN RTX
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=128, batch_size_schedule='', conv=True, data='mnist', dims='8,32,32,8', divergence_fn='approximate', dl2int=None, evaluate=True, experiment_name='mnist', imagesize=None, l1int=None, l2int=None, layer_type='ignore', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, model='ffjord', multiscale=False, nonlinearity='softplus', num_blocks=1, num_epochs=1000, num_flows=4, num_householder=8, parallel=False, rademacher=True, residual=False, resnet_checkpoint=None, resume=None, rtol=1e-05, save='experiments/ffjord', save_recon_images_size=16, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='2,2,1,-2,-2', test_atol=None, test_batch_size=200, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, z_size=64)
===> Using batch size 128. Total 468 iterations/epoch.
SequentialFlow(
  (chain): ModuleList(
    (0): LogitTransform()
    (1): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): IgnoreConv2d(
                (_layer): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (1): IgnoreConv2d(
                (_layer): Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (2): IgnoreConv2d(
                (_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (3): IgnoreConv2d(
                (_layer): ConvTranspose2d(32, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
              (4): IgnoreConv2d(
                (_layer): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
              )
            )
            (activation_fns): ModuleList(
              (0): Softplus(beta=1, threshold=20)
              (1): Softplus(beta=1, threshold=20)
              (2): Softplus(beta=1, threshold=20)
              (3): Softplus(beta=1, threshold=20)
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 17746
Epoch 0
Iter 0000 | Time 0.3330(0.3330) | Bit/dim 27.0518(27.0518) | Steps 41(41.00) | Grad Norm 16.7476(16.7476) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1388(0.2831) | Bit/dim 26.9808(27.0620) | Steps 41(41.00) | Grad Norm 16.4939(16.7052) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1427(0.2461) | Bit/dim 27.0578(27.0367) | Steps 41(41.00) | Grad Norm 16.5797(16.6334) | Total Time 1.00(1.00)
Iter 0030 | Time 0.1456(0.2196) | Bit/dim 27.0021(26.9983) | Steps 41(41.00) | Grad Norm 16.7139(16.5500) | Total Time 1.00(1.00)
